
\section{The Richardson iteration}

\begin{intro}
  As a first example and prototype for all other iterative methods we
  consider Richardson's method, which for matrices and vectors in
  $\R^n$ reads
  \begin{gather}
    \label{eq:richardson:1}
    \vec x^{(k+1)}
    = \vec x^{(k)}
    - \omega_k \bigl(\mat A \vec x^{(k)} - \vec b \bigr).
  \end{gather}
  $\omega_k$ is a relaxation parameter, which can be chosen a priori
  or can be changed in every step. We will for simplicity assume
  $\omega_k = \omega$.
\end{intro}  

\begin{Lemma}{richardson-error-step}
  \label{lemma:richardson:1}
  The error after one step of the Richardson method is given by
  \begin{gather}
    \label{eq:richardson:14}
    \vec x^{(k+1)} - x = \mat E \left(
      \vec x^{(k)} - x \right),
  \end{gather}
  where the \putindex{error propagation operator} is
  \begin{gather}
    \label{eq:richardson:15}
    \mat E = \mat I - \omega \mat A.
  \end{gather}
\end{Lemma}

\begin{proof}
  Using the fact that $\vec x = \mat A^{-1} \vec b$, we write
  \begin{gather*}
    \vec x^{(k+1)} - \vec x 
    = \vec x^{(k)}
    - \omega \bigl(\mat A \vec x^{(k)} - \vec b \bigr) -  \vec x^{(k)}
    =  \vec x^{(k)} - \vec x - \omega \mat A \bigl(
     \vec x^{(k)} -  \vec x \bigr).
  \end{gather*}
\end{proof}

\begin{Theorem}{richardson-convergence1}
  \label{theorem:richardson:1}
  If $\mat A$ is symmetric, positive definite, with extremal
  eigenvalues $\lambda>0$ and $\Lambda>0$, then Richardson's method
  converges if and only if $0 < \omega < 2/\Lambda$. The optimal
  relaxation parameter is 
  \begin{gather}
    \label{eq:richardson:2}
    \omega_{\text{opt}} = \frac{2}{\lambda+\Lambda},
  \end{gather}
  which yields an optimal contraction rate of
  \begin{gather}
    \label{eq:richardson:4}
    \rho_{\text{opt}}
    = 1-\frac{2\lambda}{\lambda+\Lambda}
    = \frac{\Lambda-\lambda}{\Lambda+\lambda}
    = \frac{\kappa-1}{\kappa+1}
    = 1 -\frac2\kappa + \mathcal
    O\left(\kappa^{-2}\right),
  \end{gather}
  where $\kappa = \Lambda/\lambda$ is the so called \define{spectral
    condition number}.
\end{Theorem}

\begin{proof}
  Convergence of this method is analyzed through the \putindex{Banach
    fixed-point theorem}, which requires contraction
  property of the matrix $\mat M = \mat I - \omega \mat A$.
  Alternatively, we studied a theorem that
  states, that a matrix iteration converges if and only if the
  spectral radius
  \begin{gather*}
    \rho(\mat M) = \max \left|\lambda(\mat M)\right| < 1,
  \end{gather*}
  the maximum absolute value of the eigenvalues of $\mat M$ is
  strictly less than one.
  
  If $\mat A$ is symmetric, positive definite, with eigenvalues
  $\lambda_i > 0$, we have that
  \begin{gather}
    \label{eq:richardson:13}
    \rho(\mat M) = \max_i \left|1-\omega \lambda_i\right|.
  \end{gather}
  Let the extremal eigenvalues be determined by the minimum and
  maximum of the Rayleigh quotient,
  \begin{gather}
    \label{eq:richardson:3}
    \lambda
    = \min_{x\in \R^n} \frac{\vec x^T\mat A\vec x}{\vec x^T\vec x},
    \qquad\text{and}\qquad
    \Lambda = \max_{x\in \R^n} \frac{\vec x^T\mat A\vec x}{\vec x^T\vec x}.
  \end{gather}
  Then, equation~\eqref{eq:richardson:13} yields that the method
  converges for $0 < \omega < 2/\Lambda$. Furthermore, for 
  $1/\Lambda \le \omega \le 2/\Lambda$ we have
  \begin{gather*}
    \rho(\mat M) = \max \bigl\{ -1+\omega \Lambda,  1-\omega \lambda \bigr\}.
  \end{gather*}
  The optimal parameter $\omega$ is the one where both values are
  equal and thus~\eqref{eq:richardson:2} and~\eqref{eq:richardson:4} hold.
\end{proof}

\begin{intro}
  The analysis of finite element methods shows that it is beneficial
  to give up the focus on finite dimensional spaces and rather use
  theory that applies to separable Hilbert spaces. If results can
  obtained in this context, they can easily be restricted to finite
  dimensional subspaces and thus become uniform with respect to the
  mesh parameter. Thus, we will first reformulate Richardson's method
  for this case and then derive convergence estimates.
\end{intro}

\begin{intro}
  Elements of an abstract Hilbert space $X$ will be denoted by
  $u,v,w$, etc. On the other hand, coefficient vectors in $\R^n$ are
  denoted by letters $\vec x,\vec y,\vec z$, etc.
\end{intro}

\begin{Definition}{richardson-method}
  Let $X$ be a Hilbert space with inner product $\scal(.,.)$. Let
  $a(.,.)$ be a second bilinear form on $X$ and the domain of its
  operator is $V$. Then, for any right hand
  side $f\in V$ and any start vector $u^{(0)}\in V$,
  \define{Richardson's method} is defined by the iteration
  \begin{gather}
    \label{eq:richardson:5}
    \scal(u^{(k+1)},v) = \scal(u^{(k)},v)
    - \omega_k \bigl(a(u^{(k)},v) - \scal(f,v)\bigr), \qquad \forall v\in X.
  \end{gather}
  $\omega_k$ is a suitable \putindex{relaxation parameter}, chosen
  such that the method converges.
\end{Definition}

\begin{note}
  The scalar products in~\eqref{eq:richardson:5} become necessary,
  since different from the case in $\R^n$, the result of applying the
  bilinear form $a(.,.)$ to $u^{(k)}$ in the first argument yields a
  linear form on $X$. In order to convert this to a vector in $X$, we
  have to apply the isomorphism induced by the \putindex{Riesz
    representation theorem}.
\end{note}

\begin{theorem}{richardson-convergence2}
  \label{theorem:richardson:2}
  Let the bilinear form $a(.,.)$ be bounded and elliptic on $X\times
  X$, namely, let there exist positive constants $\Lambda$ and $\lambda$ such
  that for all $u,v\in X$ there holds
  \begin{gather}
    \label{eq:richardson:6}
    a(u,v) \le \Lambda \norm{u} \norm{v},
    \qquad
    a(u,u) \ge \lambda \norm{u}^2.
  \end{gather}
  Then, Richardson's iteration converges for
  $\omega_k = \omega$ for any $\omega \in (0, 2\lambda/\Lambda^2)$.
\end{theorem}

\begin{proof}
  We define the iteration operator $T$ as the solution operator of
  equation~\eqref{eq:richardson:5}, namely $T u^{(k)} := u^{(k+1)}$. We
  have to prove that $T$ is a contraction on $X$ under the assumptions
  of the theorem.

  For two arbitrary vectors $u^1, u^2 \in X$, let $w = u^1-u^2$ be
  their difference. Due to linearity, we have $T w = T u^1-T u^2$ and
  \begin{gather*}
    \scal(T w,v) = \scal(w,v) - \omega a(w,v) = \scal(w-\omega A w,v).
  \end{gather*}
  Using $v=Tw$ as a test function, we obtain
  \begin{align*}
    \norm{Tw}^2
    & = \scal(w-\omega A w,w-\omega A w) \\
    &= \norm{w}^2 - 2\omega a(w,w) + \omega^2 \norm{Aw}^2\\
    & \le \norm{w}^2 - 2\lambda\omega \norm{w}^2
    +  \Lambda^2 \omega^2\norm{w}^2\\
    & = \underbrace{\bigl(1-2\lambda\omega
      + \Lambda^2\omega^2\bigr)}_{=:\rho(\omega)} \norm{w}^2.
  \end{align*}
  The function $\rho(\omega)$ is a parabola open to the top, which
  at zero equals one and has a negative derivative. Thus, it is less
  than one for small positive valuers of $\omega$. The other
  point where $\rho(\omega) = 1$ is $\omega = 2\lambda/\Lambda^2$.
\end{proof}

\begin{note}
  The condition on $\omega$ in Theorem~\ref{theorem:richardson:2} is
  more restrictive than in Theorem~\ref{theorem:richardson:1}, since
  $\lambda/\Lambda \le 1$. This is
  due to the fact, that in Theorem~\ref{theorem:richardson:1} we
  assume symmetry, and thus orthogonal diagonalizability of the matrix
  $\mat A$. With similar assumptions,
  Theorem~\ref{theorem:richardson:2} could be made sharper.
\end{note}

\begin{note}
  It is clear that the boundedness and ellipticity
  estimates~\eqref{eq:richardson:6} hold for any finite dimensional
  subspace $X_n\subset X$, and thus the convergence
  estimate~\eqref{eq:richardson:4} becomes independent of $n$.
  
  More interesting and also more common is the case where the bilinear form
  $a(.,.)$ is unbounded on $X$. While it is still bounded on each
  finite subspace $X_n$, this bound cannot be independent of $n$ if
  the sequence $\{X_n\}$ approximates $X$.
\end{note}  

\begin{note}
  We define an operator $B:X\to X^*$ such that $Bu = b(u,.) :=
  \scal(u,.)$. By the Riesz representation theorem, there is a
  continuous inverse operator $B^{-1}: X^*\to X$, which is often
  called \define{Riesz isomorphism}.
\end{note}

\begin{Definition}{p-richardson-method}
  \label{definition:richardson:2}
  When we apply Richardson's method as in~\eqref{eq:richardson:5} on a
  computer, each step involves a multiplication with the matrix $\mat A$,
  but an inversion of the matrix $\mat B$, corresponding to the iteration
  \begin{gather*}
    \mat B \vec x^{(k+1)}
    = \mat B \vec x^{(k)}
    - \omega_k \bigl(\mat A \vec x^{(k)} - \vec b \bigr),
  \end{gather*}
  or equivalently,
  \begin{gather}
    \label{eq:richardson:7}
    \vec x^{(k+1)}
    = \vec x^{(k)}
    - \omega_k \mat B^{-1}\bigl(\mat A \vec x^{(k)} - \vec b \bigr).
  \end{gather}
  The iteration in~\eqref{eq:richardson:7} is commonly referred to as
  \define{preconditioned Richardson iteration} and $\mat B^{-1}$ as the
  \define{preconditioner}. Note that by introducing the iteration in
  its weak form~\eqref{eq:richardson:5}, the preconditioner arrives
  naturally and with necessity.
  
  The goal of this chapter is finding preconditioners $\mat B^{-1}$, or
  equivalently inner products $\scal(.,.)$, such that the bilinear
  form $a(.,.)$ is bounded and the condition number
  $\kappa = \Lambda/\lambda$ is small.
  
  In order to reduce (or increase) confusion, we will refer to the
  inner product that we search in order to bound the condition number
  as $b(.,.)$ instead of $\scal(.,.)$, this way separating the
  Hilbert space $X$ more clearly from the task of
  preconditioning. Thus, the operator $B$ and the matrix $\mat B$ will
  be associated with a bilinear form $b(.,.)$ and the final version of
  the preconditioned Richardson iteration is
  \begin{gather}
    \label{eq:richardson:10}
    b(u^{(k+1)},v) = b(u^{(k)},v)
    - \omega_k \bigl(a(u^{(k)},v) - f(v)\bigr), \qquad \forall v\in X,
  \end{gather}
  or in operator form
  \begin{gather}
    \label{eq:richardson:11}
    u^{(k+1)} = u^{(k)} - \omega_k B^{-1} (A u^{(k)} - f).
  \end{gather}
\end{Definition}

\begin{remark}
  The space $X$ and is inner product does not appear anymore in this
  formulation, since the bilinear form $b(.,.)$ has replaced
  it. Thus, finding a preconditioner also amounds to changing the
  space in which we iterate. This is reflected by the following:
\end{remark}

\begin{corollary}
  Let the symmetric bilinear forms $a(.,.)$ and $b(.,.)$ in the
  Richardson iteration~\eqref{eq:richardson:10} be both bounded and
  positive definite on the
  same space $V$ and fulfill the
  \define{spectral equivalence} relation
  \begin{gather}
    \label{eq:richardson:12}
    \lambda b(u,u) \le a(u,u) \le \Lambda b(u,u), \quad \forall u\in V.
  \end{gather}
  Then, if $\omega_k \equiv \omega \in (0,2\Lambda)$, the iteration is
  a contraction on $V$. The optimal contraction number is $\rho$
  according to equation~\eqref{eq:richardson:4} for $\omega$ chosen as
  in~\eqref{eq:richardson:2}.
\end{corollary}

\begin{proof}
  This corollary is equivalent to Theorem~\ref{theorem:richardson:2}
  if the inner product $\scal(.,.)$ is replaced by the bilinear form
  $b(.,.)$.
\end{proof}

\begin{remark}
  Originally, the space $V$ was chosen as the domain of $A$ which
  essentially meant $V \subset H^2(\Omega)$, since we required $Av \in
  X$. An additional benefit of the preconditioned version is, that now
  $V \subset H^1(\Omega)$ is sufficient and at least here no
  regularity assumption is required.
\end{remark}

\begin{notation}
  \index{lambdaBA@$\lambda(B,A)$}
  \index{Lambdaba@$\Lambda(B,A)$}
  In order to distinguish different preconditioners, we will also us
  the notation $\lambda(B,A)$ and $\Lambda(B,A)$ to refer to the
  constants in the norm equivalence~\eqref{eq:richardson:12}.
\end{notation}


\begin{example}
  Let us take the example~\eqref{eq:itintro:1}.
  By the PoincarÃ©-Friedrichs inequality, $a(.,.)$ is an inner product
  on $X$ and thus we can choose $\scal(.,.) = a(.,.)$. In
  particular, $\lambda = \Lambda = 1$ and the optimal choice is
  $\omega = 1$. Then, Richardson's iteration becomes
  \begin{gather*}
    a(u^{(k+1)},v) = a(u^{(k)},v)
    - \bigl(a(u^{(k)},v) - f(v)\bigr) =  f(v), \qquad \forall v\in X,
  \end{gather*}
  which converges in a single step, but we have to solve the original
  equation for $u$. Thus, either the inversion of the matrix $A_n$ is
  trivial on each finite dimensional subspace $X_n$, or the method is
  useless. With usual finite element bases, the latter is true.
\end{example}

\begin{example}
  In the other extreme, we would like to use the $\R^n$ or $L^2$
  inner product on $X_n$ or $X$, such that the Riesz isomorphism is
  easily computable. But then, the bilinear form $a(.,.)$ is unbounded
  on $X$. Thus, while for each finite $n$, the condition number
  $\kappa_n = \Lambda_n/\lambda_n$ exists, it converges to infinity if
  $n\to\infty$.
\end{example}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
