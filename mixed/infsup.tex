
\section{Finite-dimensional problems}
\begin{intro}
  So far, our power horse for well-posedness was the Lax-Milgram
  lemma, which can be applied under the conditions
  \begin{xalignat}2
    a(u,v) &\le M \norm{u}\norm{v} & \forall u,v&\in V\\
    \label{eq:infsup:elliptic}
    a(u,u) &\ge \gamma \norm{u}^2 & \forall u&\in V.
  \end{xalignat}
  The second condition can also be rewritten in terms of the
  \putindex{Rayleigh quotient} as
  \begin{gather*}
    0 < \gamma = \inf_{u\in V}\frac{a(u,u)}{\norm{u}^2}.
  \end{gather*}
  Restricting this to a finite dimensional space, the notation usually
  changes from
  \begin{gather}
    a(u,v) = f(v)
    \qquad\text{to}\qquad
    v^TA u = v^T f,
  \end{gather}
  where $A\in \R^{n\times n}$ is the matrix associated with the
  bilinear form. The bound for the Rayleigh quotient means nothing but
  that the real parts of all eigenvalues of $A$ are bounded from below
  by $\gamma$. Thus, a matrix $A$ for which we can apply the
  Lax-Milgram lemma is positive definite. And the statement of the
  lemma in finite dimension is, that a positive definite matrix is
  invertible. We know from linear algebra that this is true, but we
  also know that the condition is all but necessary.
\end{intro}

\begin{intro}
  Why did we replace this clear theorem by the weaker Lax-Milgram
  lemma, when we studied elliptic partial differential equations?  For
  the first condition, it should be noted that spectral properties of
  operators between spaces of infinite dimension are much harder to
  obtain. Further, we do not need information on the whole spectrum,
  but only on the eigenvalue closest to zero. Therefore, we used a
  simple estimate in order to avoid discussing the spectrum at
  all. But, there is an important difference between
  Theorem~\ref{Theorem:la-invertible} and the
  estimate~\eqref{eq:infsup:elliptic}: the assumption of the theorem
  is qualitative, $\lambda \neq 0$, while the assumption of Lax-Milgram
  is quantitative,
  \begin{gather*}
    \Re\lambda \ge \gamma> 0.
  \end{gather*}
  The following problem shows why such a change is necessary.
\end{intro}

\begin{Problem}{unbounded-inverse}
  On the space $\ell_2(\R)$ define the operator $A$ by its eigenvalue
  decomposition
  \begin{align*}
    A: \ell_2(\R) &\to \ell_2(\R)\\
    e_k & \mapsto \tfrac1k e_k.
  \end{align*}
  Here, $\{e_k\}$ is the orthogonal basis of unit vectors of the form
  \begin{gather*}
    \arraycolsep0.1em
    \begin{array}{cccccccc}
      e_k =(0&,\ldots,&0&,&1&,&0&,\ldots)^T.\\
      &&&&\uparrow\\
      &&&&k
    \end{array}
  \end{gather*}
  \begin{enumerate}
  \item Show that this operator does not have a bounded inverse, albeit
    its eigenvalues are positive.
  \item Show that the range of $A$ is not closed in $\ell_2(\R)$
  \end{enumerate}
\begin{solution}  
  \begin{enumerate}
  \item For each $e_k$, the inverse is $A^{-1} e_k = k e_k$, which for
    $k\to\infty$ is clearly unbounded.
  \item We have to construct a convergent sequence in the range of $A$
    such that the pre-image of the sequence does not converge.
    \begin{enumerate}
    \item Choose
      \begin{gather*}
        v_n = \sum_{k=1}^n \frac1k e_k.
      \end{gather*}
      \item $v_n$ is a Cauchy-sequence, since
        \begin{gather*}
          \norm{v_m-v_n}^2 = \norm*{\sum_{k=m}^n \frac1k e_k}^2
          = \sum_{k=m}^n \frac1{k^2}\norm*{e_k}^2
          \le \frac1{m^2} \sum_{k=1}^\infty \frac1{k^2}
          = \frac{\pi^2}{6} \frac1{m^2}.
        \end{gather*}
      \item We conclude that $v=\lim_{n\to\infty}v_n$ exists in the
        closure of the range of $A$.
      \item There holds
        \begin{gather*}
          v_n = A \sum_{k=1}^n e_k =: A u_n.
        \end{gather*}
      \item The sequence $u_n$ is not a Cauchy sequence, since
        \begin{gather*}
          \norm{v_m-v_n}^2 = \norm*{\sum_{k=m}^n e_k}^2
          = \sum_{k=m}^n \norm*{e_k}^2
          = n-m.
        \end{gather*}
    \end{enumerate}
  \end{enumerate}
\end{solution}
\end{Problem}

\begin{Problem}{lax-milgram-not-applicable}
  Find an invertible, symmetric matrix $A\in \R^{2\times 2}$ and a
  vector $v\in \R^2$ such that $v^T A v=0$ and thus the Lax-Milgram
  lemma is inconclusive.
\begin{solution}
  \begin{gather*}
    A =
    \begin{pmatrix}
      1 & 0 \\ 0 & -1
    \end{pmatrix}
  \end{gather*}
\end{solution}
\end{Problem}

The question of well-posedness in finite dimensions can be answered by:

\begin{Theorem}{la-invertible}
  A matrix $A\in\R^{n\times n}$ is invertible if and only if one of
  the following equivalent conditions holds:
  \begin{enumerate}
  \item all its (possibly complex) eigenvalues are nonzero,
  \item all its singular values are nonzero,
  \item for each $v\in\R^n$ holds $Av\neq 0$.
  \end{enumerate}
\end{Theorem}

\begin{intro}
  We focus on the second and third conditions, respectively, in
  Theorem~\ref{Theorem:la-invertible}.
  But, the problem above tells us that we
  will run into trouble, if we do not quantify this. Therefore, we
  start our attempt by requiring:
  \begin{gather*}
    \norm{Au}^2 \ge \gamma \norm{u}^2 \qquad\forall u\in V.
  \end{gather*}
  But while this is a condition we can easily write down for matrices
  and operators, it does not work that well for bilinear forms. For
  those, we have the condition which was introduced in this form by
  NeÄas. It implies that the linear form on $V$, defined by $a(u,\cdot)$
  is bounded from below.
  % It turns out, if we go this far, we can even consider bilinear forms
  % $a(\cdot,\cdot)$ on two different spaces $V$ and $W$ with an associated
  % operator $A: V\to W^*$.
\end{intro}

\begin{Definition}{infsup1}
  A bilinear form $a(\cdot,\cdot)$ on $V\times W$ is said to admit the
  \define{inf-sup condition} or is called \define{inf-sup stable}, if
  there holds
  \begin{gather}
    \label{eq:infsup:1}
    \inf_{u\in V} \sup_{w\in W} \frac{a(u,w)}{\norm{u}_V\norm{w}_W}
    \ge \gamma > 0.
  \end{gather}
\end{Definition}

\begin{remark}
  In this finite dimensional exposition, is clear that $V$ and $W$
  must have the same dimension, and thus $V=W=\R^n$. This will be
  different, when we consider infinite dimensional spaces and indeed
  consider different spaces $V$ and $W$.
\end{remark}

\begin{Lemma}{infsup2}
  The following statements are equivalent to the inf-sup
  condition~\eqref{eq:infsup:1}:
  \begin{gather}
    \label{eq:infsup:2}
    \forall u\in V \;\exists w\in W \;:\; a(u,w) \ge \gamma \norm{u}_V\norm{w}_W
  \end{gather}
  \begin{gather}
    \label{eq:infsup:3}
    \forall u\in V
    \;\exists w\in W \;:\;
    \left\{
    \arraycolsep0.3ex
    \begin{array}{rcl}
      \norm{w}_W &\le&\norm{u}_V\\
      a(u,w) &\ge& \gamma \norm{u}_V^2
    \end{array}
    \right.
  \end{gather}
\end{Lemma}

\begin{Problem}{inf-sup-equivalence}
  Prove \blockref{Lemma}{infsup2}.
\end{Problem}

\input{svd}

\begin{Definition}{ker-range-rn}
  \index{ker}
  Let $A: V\to W$ be a linear operator. Then, we define
  the \define{kernel} and the \define{range} of $A$ as
  \begin{align*}
    \ker A &= \bigl\{ v\in V \big| Av = 0\bigr\} \\
    \range A &= \bigl\{ w\in W \big| \;\exists\,v\in V: Av=w\bigr\}.
  \end{align*}
\end{Definition}

\begin{Definition}{orthogonal1}
  Let $V\subset\R^n$ be a subspace. We define the \define{orthogonal
    complement} of $V$ as
  \begin{gather}
    \label{eq:infsup:4}
    V^\perp = \bigl\{w\in \R^n \big| \;\forall\,v\in V \scal(w,v) = 0 \bigr\}.
  \end{gather}
\end{Definition}

\begin{Lemma}{ker-coker-rn}
  Let $A\in \R^{m\times n}$ and $A^T$ its transpose. Then, there holds
  \begin{gather}
    \label{eq:infsup:5}
    \begin{split}
      \ker A &= \range{A^T}^\perp\\
      \range A &= \ker{A^T}^\perp\\
      \ker{A^T} &= \range A^\perp\\
      \range{A^T} &= \ker{A}^\perp
    \end{split}
  \end{gather}
\end{Lemma}

\begin{proof}
  % First, we note that
  % \begin{gather*}
  %   \R^n = \ker A \oplus \ker A^\perp,
  %   \qquad
  %   \R^m = \ker{A^T} \oplus \ker{A^T}^\perp.
  % \end{gather*}
  Let $A=U\Sigma V^T$ be the singular value decomposition of $A$ and
  $r$ be the number of nonzero singular values. Then, the first $r$
  vectors of $U$ span the range of $A$ and the last $n-r$ vectors of
  $V$ span its kernel. Furthermore,
  \begin{gather}
    A^T = \bigl(U\Sigma V^T\bigr)^T = V \Sigma U^T.
  \end{gather}
  Therefore, the first $r$ vectors of V span the range of $A^T$ and
  the last $n-r$ vectors of $U$ span its kernel.
\end{proof}

\begin{Corollary}{ker-coker-iso}
  Let $A\in \R^{m\times n}$ and $A^T$ its transpose. Then, the
  restrictions $A\colon \ker A^\perp \to \range A$ and $A^T\colon
  \ker{A^T}^\perp \to \range{A^T}$ are isomorphisms.
\end{Corollary}

\begin{proof}
  We note that $\dim \range A = \dim \range{A^T}$. Thus, by
  \blockref{Lemma}{ker-coker-rn} the dimensions of domain and range of
  each of the restricted operators are equal, say $\dim \range A =
  r$. The singular value decomposition of the operators is
  \begin{gather*}
    A = U\Sigma V^T \qquad A^T = V\Sigma U^T,
  \end{gather*}
  where all matrices are in $\R^{r\times r}$ and
  \begin{gather*}
    \Sigma = \diag(\sigma_1,\dots,\sigma_r),
  \end{gather*}
  and all singular values are positive. Thus, $A$ and $A^T$ are invertible.
\end{proof}

\begin{Corollary}{svd-infsup}
  Let $r=\dim \ker A^\perp$. Then, for the smallest nonzero singular
  value there holds
  \begin{gather}
    \label{eq:infsup:6}
    \sigma_r
    = \inf_{v\in \ker{A}^\perp} \sup_{w\in \R^m} \frac{w^T A v}{\abs{v}\abs{w}}
    = \inf_{w\in \ker{A^T}^\perp} \sup_{v\in \R^n} \frac{w^T A v}{\abs{v}\abs{w}}.
  \end{gather}
\end{Corollary}

\begin{proof}
  Since the Cauchy-Schwarz inequality turns into an equation if and
  only if the two vectors are coaligned, there holds for any $v\in \R^n$:
  \begin{gather*}
    \sup_{w\in\R^m}\frac{w^T A v}{\abs{w}} = \frac{v^T A^T A v}{\abs{Av}}.
  \end{gather*}
  Therefore,
  \begin{gather*}
    \inf_{v\in \ker{A}^\perp} \sup_{w\in \R^m}
    \frac{w^T A v}{\abs{v}\abs{w}}
    = \inf_{v\in \ker{A}^\perp} \frac{\abs{Av}}{\abs{v}}.
  \end{gather*}
  Now, let $v = \sum \alpha_i v_i$ where $v_i$ are the columns of $V$
  in the SVD of $A$. Then,
  \begin{gather*}
    \abs{Av}^2 = \abs*{A\sum_{i=1}^r \alpha_i v_i}^2
    = \abs*{\sum_{i=1}^r \sigma_i \alpha_i u_i}^2
    = \sum_{i=1}^r \sigma_i^2 \alpha_i^2.
  \end{gather*}
  The quotient
  \begin{gather*}
    \frac{\sum_{i=1}^r \sigma_i^2 \alpha_i^2}{\sum_{i=1}^r \alpha_i^2}
  \end{gather*}
  clearly has its minimum if $\alpha_1 = \dots=\alpha_{r-1} = 0$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Infinite dimensional Hilbert spaces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  In the previous section, we derived quantitative conditions to
  ensure the invertibility of a matrix $A$ or its restriction to its
  cokernel $\ker A^\perp$. The arguments there have a natural
  extension to infinite dimensional Hilbert spaces, which we will
  derive in this section. We already saw in
  \blockref{Problem}{unbounded-inverse} that we may run into trouble
  if the range of $A$ is not closed. On the other hand, it turns out
  that most notions of linear algebra related to orthogonality can be
  maintained in Hilbert spaces if closed subspaces are considered.
  We begin by citing the most important results.
\end{intro}

\begin{Definition}{polar-orthogonal}
  Let $W\subset V$ be a subspace of a Hilbert space $V$. We define its
  \define{polar space} $W^0\subset V^*$ and its
  \define{orthogonal complement} $W^\perp\subset V$ by
  \begin{gather}
    \label{eq:infsup:7}
    \begin{split}
    W^0 &= \bigl\{f\in V^* \big| \scal(f,w)_{V^*\times V} = 0
    \;\forall\,w\in W\bigr\},
    \\
    W^\perp &= \bigl\{v\in V \big| \scal(v,w)_{V} = 0
    \;\forall\,w\in W\bigr\}.
    \end{split}
  \end{gather}
\end{Definition}

\begin{Lemma}{orthogonal-closed}
  The polar space $W^0$ and the orthogonal complement $W^\perp$ of a
  subspace $W\subset V$ are both closed.
\end{Lemma}

\begin{proof}
  Consider the mapping
  \begin{align*}
    \Phi_w\colon V^* &\to \R,\\
    v&\mapsto \scal(v,w)_{V^*\times V}.
  \end{align*}
  For any $w$, the kernel of $\phi$ is closed as
  the pre-image of a closed set. $W^0$ is closed since it is the
  intersection of these kernels for all $w\in W$.

  The inner product is continuous on $V\times V$. Therefore, the
  mapping
  \begin{align*}
    \phi_w\colon V &\to \R,\\
    v&\mapsto \scal(v,w),
  \end{align*}
  is continuous. The argument continues as above.
\end{proof}

\begin{Theorem}{orthogonal-complement}
  Let $W$ be a subspace of a Hilbert space $V$ and $W^\perp$ its
  orthogonal complement. Then, $W^\perp = \overline{W}^\perp$. Further,
  $V = W \oplus W^\perp$ if and only if $W$ is closed.
\end{Theorem}

\begin{proof}
  Clearly, $\overline{W}^\perp \subset W^\perp$ since $W\subset\overline{W}$.
  

  Now, the ``only if'' follows by the fact, that if $W$ is not
  closed, there is an element $w\in \overline{W}$ but not in $W$ such that
  $\scal(w,u)=0$ for all $u\in W^\perp$. Thus, $w\not\in W^\perp$ and
  consequently $w\not\in W^\perp \oplus W$.


\end{proof}

\begin{Theorem}{infsup-well-posedness}
  Let $a(\cdot,\cdot)$ on $V\times W$ be a bounded bilinear form such
  that the inf-sup condition~\eqref{eq:infsup:1} holds.
  % \begin{gather}
  %   a(u,w) \le M \norm{u}_V \norm{w}_W.
  % \end{gather}
  Then, the problem finding $u\in V$ such that
  \begin{gather}
    a(u,w) = f(w) \qquad\forall w\in W,
  \end{gather}
  has a unique solution and
  \begin{gather}
    \norm{u}_V \le \frac1\gamma \norm{f}_{W^*}
  \end{gather}
\end{Theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The inf-sup condition for mixed problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
