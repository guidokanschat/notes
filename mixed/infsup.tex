
Let us begin with a simple example, displaying some of the
difficulties we may have.

\begin{Problem}{unbounded-inverse}
  On the space $\ell_2(\R)$ define the operator $A$ by its eigenvalue
  decomposition
  \begin{align}
    A: \ell_2(\R) &\to \ell_2(\R)\\
    e_k & \mapsto \tfrac1k e_k.
  \end{align}
  Here, $\{e_k\}$ is the orthogonal basis of unit vectors of the form
  \begin{gather}
    \arraycolsep0.1em
    \begin{array}{cccccccc}
      e_k =(0&,\ldots,&0&,&1&,&0&,\ldots)^\transpose.\\
      &&&&\uparrow\\
      &&&&k
    \end{array}
  \end{gather}
  \begin{enumerate}
  \item Show that this operator does not have a bounded inverse, albeit
    its eigenvalues are positive.
  \item Show that the range of $A$ is not closed in $\ell_2(\R)$
  \end{enumerate}
\begin{solution}
  \begin{enumerate}
  \item For each $e_k$, the inverse is $A^{-1} e_k = k e_k$. In particular, $A$ is injective.
    On the other hand, it holds
    \begin{gather}
      \lim_{k\to\infty}\frac{\norm{A^{-1}e_k}}{\norm{e_k}}=\lim_{k\to\infty}k=\infty
    \end{gather}
      and the inverse cannot be bounded.
  \item We have to construct a convergent sequence in the range of $A$
    such that the pre-image of the sequence does not converge.
    \begin{enumerate}
    \item Choose
      \begin{gather}
        v_n = \sum_{k=1}^n \frac1k e_k.
      \end{gather}
      \item $v_n$ is a Cauchy-sequence, since
        \begin{gather}
          \norm{v_m-v_n}^2 = \norm*{\sum_{k=m}^n \frac1k e_k}^2
          = \sum_{k=m}^n \frac1{k^2}\norm*{e_k}^2
          \le \frac1{m^2} \sum_{k=1}^\infty \frac1{k^2}
          = \frac{\pi^2}{6} \frac1{m^2}.
        \end{gather}
      \item We conclude that $v=\lim_{n\to\infty}v_n$ exists in the
        closure of the range of $A$.
      \item There holds
        \begin{gather}
          v_n = A \sum_{k=1}^n e_k =: A u_n.
        \end{gather}
      \item Due to the injectivity of $A$ for $v$ to be in the range of $A$,
            $u_n$ has to converge in $\ell_2(\R)$.
      \item The sequence $u_n$ is not a Cauchy sequence, since
        \begin{gather}
          \norm{v_m-v_n}^2 = \norm*{\sum_{k=m}^n e_k}^2
          = \sum_{k=m}^n \norm*{e_k}^2
          = n-m.
        \end{gather}
    \end{enumerate}
  \end{enumerate}
\end{solution}
\end{Problem}

\section{Finite-dimensional problems}
\begin{intro}
  So far, our power horse for well-posedness was the Lax-Milgram
  lemma, which can be applied under the conditions
  \begin{xalignat}2
    a(u,v) &\le M \norm{u}\norm{v} & \forall u,v&\in V\\
    a(u,u) &\ge \ellipa \norm{u}^2 & \forall u&\in V.
  \end{xalignat}
  The second condition can also be rewritten in terms of the
  \putindex{Rayleigh quotient} as
  \begin{gather}
    0 < \ellipa = \inf_{u\in V}\frac{a(u,u)}{\norm{u}^2}.
  \end{gather}
  Restricting this to a finite dimensional space, the notation usually
  changes from
  \begin{gather}
    a(u,v) = f(v)
    \qquad\text{to}\qquad
    v^\transpose A u = v^\transpose f,
  \end{gather}
  where $A\in \R^{n\times n}$ is the matrix associated with the
  bilinear form. The bound for the Rayleigh quotient means nothing but
  that the real parts of all eigenvalues of $A$ are bounded from below
  by $\ellipa$. Thus, a matrix $A$ for which we can apply the
  Lax-Milgram lemma is positive definite. And the statement of the
  lemma in finite dimension is, that a positive definite matrix is
  invertible. We know from linear algebra that this is true, but we
  also know that the condition is all but necessary.
\end{intro}

\begin{intro}
  Why did we replace this clear theorem by the weaker Lax-Milgram
  lemma, when we studied elliptic partial differential equations?  For
  the first condition, it should be noted that spectral properties of
  operators between spaces of infinite dimension are much harder to
  obtain. Further, we do not need information on the whole spectrum,
  but only on the eigenvalue closest to zero. Therefore, we used a
  simple estimate in order to avoid discussing the spectrum at
  all. But, there is an important difference between
  Theorem~\ref{Theorem:la-invertible} and the
  estimate~\eqref{eq:infsup:elliptic}: the assumption of the theorem
  is qualitative, $\lambda \neq 0$, while the assumption of Lax-Milgram
  is quantitative,
  \begin{gather}
    \Re\lambda \ge \ellipa> 0.
  \end{gather}
  The following problem shows why such a change is necessary.
\end{intro}


\begin{Problem}{lax-milgram-not-applicable}
  Find an invertible, symmetric matrix $A\in \R^{2\times 2}$ and a
  vector $v\in \R^2$ such that $v^\transpose A v=0$ and thus the Lax-Milgram
  lemma is inconclusive.
\begin{solution}
  \begin{gather}
    A =
    \begin{pmatrix}
      1 & 0 \\ 0 & -1
    \end{pmatrix}
  \end{gather}
\end{solution}
\end{Problem}

The question of well-posedness in finite dimensions can be answered by:

\begin{Theorem}{la-invertible}
  A matrix $A\in\R^{n\times n}$ is invertible if and only if one of
  the following equivalent conditions holds:
  \begin{enumerate}
  \item all its (possibly complex) eigenvalues are nonzero,
  \item all its singular values are nonzero,
  \item for each nonzero $v\in\R^n$ holds $Av\neq 0$.
  \end{enumerate}
\end{Theorem}

\begin{intro}
  We focus on the second and third conditions, respectively, in
  Theorem~\ref{Theorem:la-invertible}.
  But, the problem above tells us that we
  will run into trouble, if we do not quantify this. Therefore, we
  start our attempt by requiring:
  \begin{gather}
    \norm{Au}^2 \ge \ellipa \norm{u}^2 \qquad\forall u\in V.
  \end{gather}
  But while this is a condition we can easily write down for matrices
  and operators, it does not work that well for bilinear forms. Thus,
  we first look at the singular value decomposition.
\end{intro}

\input{svd}

\begin{Definition}{ker-range-rn}
  \index{ker}
  Let $A: V\to W$ be a linear operator. Then, we define
  the \define{kernel} and the \define{range} of $A$ as
  \begin{align}
    \ker A &= \bigl\{ v\in V \big| Av = 0\bigr\} \\
    \range A &= \bigl\{ w\in W \big| \;\exists\,v\in V: Av=w\bigr\}.
  \end{align}
\end{Definition}

\begin{Definition}{orthogonal1}
  Let $V\subset\R^n$ be a subspace. We define the \define{orthogonal
    complement} of $V$ as
  \begin{gather}
    \label{eq:infsup:4}
    \ortho{V} = \bigl\{w\in \R^n \big| \;\forall\,v\in V \scal(w,v) = 0 \bigr\}.
  \end{gather}
\end{Definition}

\begin{Lemma}{ker-coker-rn}
  Let $A\in \R^{m\times n}$ and $A^\transpose$ its transpose. Then, there holds
  \begin{gather}
    \label{eq:infsup:5}
    \begin{split}
      \ker A &= \ortho{\range{A^\transpose}}\\
      \range A &= \ortho{\ker{A^\transpose}}\\
      \ker{A^\transpose} &= \ortho{\range A}\\
      \range{A^\transpose} &= \ortho{\ker{A}}
    \end{split}
  \end{gather}
\end{Lemma}

\begin{proof}
  % First, we note that
  % \begin{gather}
  %   \R^n = \ker A \oplus \ortho{\ker A},
  %   \qquad
  %   \R^m = \ker{A^\transpose} \oplus \ortho{\ker{A^\transpose}}.
  % \end{gather}
  Let $A=U\Sigma V^\transpose$ be the singular value decomposition of $A$ and
  $r$ be the number of nonzero singular values. Then, the first $r$
  vectors of $U$ span the range of $A$ and the last $n-r$ vectors of
  $V$ span its kernel. Furthermore,
  \begin{gather}
    A^\transpose = \bigl(U\Sigma V^\transpose\bigr)^\transpose = V \Sigma U^\transpose.
  \end{gather}
  Therefore, the first $r$ vectors of V span the range of
  $A^\transpose$ and the last $n-r$ vectors of $U$ span its
  kernel. The lemma follows since $U$ and $V$ are orthogonal.
\end{proof}

\begin{Corollary}{ker-coker-iso}
  Let $A\in \R^{m\times n}$ and $A^\transpose$ its transpose. Then, the
  restrictions $A\colon \ortho{\ker A} \to \range A$ and $A^\transpose\colon
  \ortho{\ker{A^\transpose}} \to \range{A^\transpose}$ are isomorphisms.
  
  The linear system $Ax=f\in\R^m$ has at least one solution if and
  only if $f\in \range A$. If $x\in\R^n$ is such a solution, then
  every $y\in\R^n$ with $y-x\in\ker A$ is a solution as well.
\end{Corollary}

\begin{proof}
  We note that $\dim \range A = \dim \range{A^\transpose}$. Thus, by
  \slideref{Lemma}{ker-coker-rn} the dimensions of domain and range of
  each of the restricted operators are equal, say $\dim \range A =
  r$. The singular value decomposition of the operators is
  \begin{gather}
    A = U\Sigma V^\transpose \qquad A^\transpose = V\Sigma U^\transpose,
  \end{gather}
  where all matrices are in $\R^{r\times r}$ and
  \begin{gather}
    \Sigma = \diag(\sigma_1,\dots,\sigma_r),
  \end{gather}
  and all singular values are positive. Thus, $A$ and $A^\transpose$ are invertible.
\end{proof}

\begin{Corollary}{svd-infsup}
  Let $r=\dim \ortho{\ker A}$. Then, for the smallest nonzero singular
  value there holds
  \begin{gather}
    \label{eq:infsup:6}
    \sigma_r
    = \inf_{v\in \ortho{\ker A}} \sup_{w\in \R^m} \frac{w^\transpose A v}{\norm{v}\norm{w}}
    = \inf_{w\in \ortho{\ker{A^\transpose}}} \sup_{v\in \R^n} \frac{w^\transpose A v}{\norm{v}\norm{w}}.
  \end{gather}
\end{Corollary}

\begin{proof}
  Since the Cauchy-Schwarz inequality turns into an equation if and
  only if the two vectors are coaligned, there holds for any $v\in \R^n$:
  \begin{gather}
    \sup_{w\in\R^m}\frac{w^\transpose A v}{\norm{w}} = \frac{v^\transpose A^\transpose A v}{\norm{Av}} = \frac{\norm{Av}^2}{\norm{Av}}.
  \end{gather}
  Therefore,
  \begin{gather}
    \inf_{v\in \ortho{\ker A}} \sup_{w\in \R^m}
    \frac{w^\transpose A v}{\norm{v}\norm{w}}
    = \inf_{v\in \ortho{\ker A}} \frac{\norm{Av}}{\norm{v}}.
  \end{gather}
  Now, let $v = \sum \alpha_i v_i$ where $v_i$ are the columns of $V$
  in the SVD of $A$. Then,
  \begin{gather}
    \norm{Av}^2 = \norm*{A\sum_{i=1}^r \alpha_i v_i}^2
    = \norm*{\sum_{i=1}^r \sigma_i \alpha_i u_i}^2
    = \sum_{i=1}^r \sigma_i^2 \alpha_i^2.
  \end{gather}
  The quotient
  \begin{gather}
    \frac{\sum_{i=1}^r \sigma_i^2 \alpha_i^2}{\sum_{i=1}^r \alpha_i^2}
  \end{gather}
  clearly has its minimum if $\alpha_1 = \dots=\alpha_{r-1} = 0$.
\end{proof}

\begin{Definition}{infsup1}
  A bilinear form $a(\cdot,\cdot)$ on $V\times W$ is said to admit the
  \define{inf-sup condition} or is called \define{inf-sup stable}, if
  there holds
  \begin{gather}
    \label{eq:infsup:1}
    \inf_{u\in V} \sup_{w\in W} \frac{a(u,w)}{\norm{u}_V\norm{w}_W}
    \ge \ellipa > 0.
  \end{gather}
\end{Definition}

\begin{remark}
  In this finite dimensional exposition, is clear that $V$ and $W$
  must have the same dimension, and thus $V=W=\R^n$. This will be
  different, when we consider infinite dimensional spaces and indeed
  consider different spaces $V$ and $W$.
\end{remark}

\begin{Lemma}{infsup2}
  The following statements are equivalent to the inf-sup
  condition~\eqref{eq:infsup:1}:
  \begin{gather}
    \label{eq:infsup:2}
    \forall u\in V \;\exists w\in W \;:\; a(u,w) \ge \ellipa \norm{u}_V\norm{w}_W
  \end{gather}
  \begin{gather}
    \label{eq:infsup:3}
    \forall u\in V
    \;\exists w\in W \;:\;
    \left\{
    \arraycolsep0.3ex
    \begin{array}{rcl}
      \norm{w}_W &\le&\norm{u}_V\\
      a(u,w) &\ge& \ellipa \norm{u}_V^2
    \end{array}
    \right.
  \end{gather}
  \begin{gather}
    \label{eq:infsup:3a}
    \forall u\in V
    \;\exists w\in W \;:\;
    \left\{
    \arraycolsep0.3ex
    \begin{array}{rcl}
      \ellipa \norm{w}_W &\le&\norm{u}_V\\
      Aw &=& u
    \end{array}
    \right.
  \end{gather}
\end{Lemma}

\begin{Problem}{inf-sup-equivalence}
  Prove Lemma 2.1.16.
\begin{solution}
  We have to prove the following statements are equivalent to the inf-sup condition:
  \begin{align}
    \forall u\in V \;\exists w\in W \;:\; a(u,w) \ge \ellipa \norm{u}_V\norm{w}_W     \tag{1}
  \end{align}
  \begin{align}
   \begin{aligned}
    \forall u\in V \;\exists w\in W \;:\;
    \begin{cases}
      \norm{w}_W \le\norm{u}_V\\
      a(u,w) \ge \ellipa \norm{u}_V^2
    \end{cases}
   \end{aligned}
   \tag{2}
  \end{align}
  \begin{align}
   \begin{aligned}
    \forall u\in V \;\exists w\in W \;:\;
    \begin{cases}
      \ellipa \norm{w}_W \le \norm{u}_V\\
      Aw = u
      \end{cases}
  \end{aligned}
      \tag{3}
  \end{align}
  The inf-sup condition reads
  \begin{align}
   \inf_{u\in V} \sup_{w\in W} \frac{a(u,w)}{\norm{u}_V\norm{w}_W} \ge \ellipa \tag{IS}
  \end{align}

  $(IS)\Rightarrow(3)$\\
  The inf-sup condition is equivalent to $A: ker(A)^\perp\to V^*$ being an isomorphism.
  By the Riesz representation theorem there exists for a given $u\in V$ a $w\in W$ such that
  $Aw=J u$ where $J$ is the Riesz map. Hence, it holds
  \begin{align}
a(u,w)=\langle A w, u\rangle = \langle J u, u\rangle = \norm{u}_V^2.
  \end{align}
 Due to $w\in ker(A)^\perp$, $A^{-1}$ is bounded and
 \begin{align}
 \norm{w}_W=\norm{A^{-1}u}_V\leq \frac{1}{\ellipa}\norm{u}_V.
  \end{align}

  $(3)\Rightarrow(2)$ \\
  Define $\tilde{w}=\ellipa w$. Then,
  \begin{align}
a(u,\tilde{w})=\ellipa a(u,w) = \ellipa \norm{u}_V^2
  \end{align}
  and
  \begin{align}
\norm{\tilde{w}}_W=\ellipa \norm{w}_W\leq\norm{u}_V
  \end{align}

  $(2)\Rightarrow(1)$ \\
  \begin{align}
  a(u,w) \ge \ellipa \norm{u}_V^2 \ge \ellipa \norm{u}_V \norm{w}_W
  \end{align}

  $(1)\Rightarrow(IS)$ \\
  \begin{align}
  &\forall u\in V \exists w\in W: a(u,w)\ge \ellipa \norm{u}_V\norm{w}_W\\
  &\Rightarrow \forall u\in V: \sup_{w\in W} \frac{a(u,w)}{\norm{w}_W}\ge \ellipa \norm{u}_V\\
  &\Rightarrow \inf_{u\in V}\sup_{w\in W} \frac{a(u,w)}{\norm{w}_W\norm{u}_V}\ge \ellipa
  \end{align}
\end{solution}
\end{Problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Infinite dimensional Hilbert spaces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  In the previous section, we derived quantitative conditions to
  ensure the invertibility of a matrix $A$ or its restriction to its
  cokernel $\ortho{\ker A}$. The arguments there have a natural
  extension to infinite dimensional Hilbert spaces, which we will
  derive in this section. We already saw in
  \slideref{Problem}{unbounded-inverse} that we may run into trouble
  if the range of $A$ is not closed. On the other hand, it turns out
  that most notions of linear algebra related to orthogonality can be
  maintained in Hilbert spaces if closed subspaces are considered.
  We begin by citing the most important results.

  The presentation here is neither complete nor self-contained. It
  just highlights some of the important facts. In particular, we
  assume the validity of the \putindex{Riesz representation theorem}
  and the existence of a \putindex{Schauder basis} a priori.
\end{intro}

\begin{Notation}{musical-isomorphisms}
  The \putindex{Riesz representation theorem} induces isomorphisms
  between a Hilbert space $V$ and its dual $V^*$. We call them
  \define{Riesz isomorphism}s. Following custom in differential
  geometry, we denote them as \define{musical isomorphism}s
  \begin{xalignat}2
    I_\flat\colon V&\to V^*
    &I_\sharp\colon V^*&\to V\\
    v&\mapsto v^\flat
    &\phi&\mapsto \phi^\sharp.
  \end{xalignat}
  \index{sharp@$\sharp$}\index{flat@$\flat$}\index{b@$\flat$}
\end{Notation}

\begin{Problem*}{riesz-h10}{Riesz Isomorphisms}
  Let $V=H^1_0(\domain)$ on the domain $\domain=(0,1)$ be equipped with the
  inner product
  \begin{gather}
    \scal(u,v)_V = \int_0^1 u'v'\dt.
  \end{gather}
  \begin{enumerate}
  \item Show that for any function $f\in L^2(\domain)$ the functional
    $\phi_f$ defined by
    \begin{gather}
      \phi_f(v) = \int_0^1 f v \dt
    \end{gather}
    is in $V^*$, even if $f\not\in V$.
  \item Discuss this for the function $f\equiv 1$ and compute $f^\sharp$.
  \end{enumerate}
\end{Problem*}

\begin{Example}{solution-pressure}
  When we look at computing the pressure in Stokes' equations, we have
  to solve a weak formulation of the form: find $p\in Q$ such that
  \begin{gather}
    \form(\div \vv,p) = g(v) \qquad \forall \vv\in \vV.
  \end{gather}
  Here $\vg\in \vV^*$ consists of the parts of the momentum equation not
  containing the pressure. If we define $B^\transpose$ by
  \begin{gather}
    \scal(B^\transpose q,\vv)_{V^*\times V} = \form(\div \vv,q)
    \qquad \forall \vv\in \vV, q \in Q,
  \end{gather}
  then the operator form of the problem posed in $\vV^*$ is
  \begin{gather}
    B^\transpose p = g
  \end{gather}
  Since this equation involves two different spaces, we require an
  extension of \slideref{Lemma}{ker-coker-rn}.
\end{Example}

\begin{Definition}{polar-orthogonal}
  Let $W\subset V$ be a subspace of a Hilbert space $V$. We define its
  \define{orthogonal complement} $\ortho{W}\subset V$ and its
  \define{polar space} $\polar{W}\subset V^*$ by
  \begin{gather}
    \label{eq:infsup:7}
    \begin{aligned}
    \ortho{W} &= \bigl\{v\in V &\big|&& \scal(v,w)_{V} &= 0
    &\forall\,w&\in W\bigr\},
    \\
    \polar{W} &= \bigl\{f\in V^* &\big|&& \scal(f,w)_{V^*\times V} &= 0
    &\forall\,w&\in W\bigr\}.
    \end{aligned}
  \end{gather}
  For a subspace $U\subset V^*$, we define its polar space
  \begin{gather}
    \dualpolar{U} = \bigl\{v\in V \quad\big|\quad \scal(u,v)_{V^*\times V} = 0
    \quad\forall\,u\in U\bigr\}
  \end{gather}
\end{Definition}

\begin{Problem}{polar-orthogonal}
  Show that $\polar{W} = I_\flat \ortho{W}$ and $\dualpolar{U} = I_\sharp \ortho{U}$.
\end{Problem}

\begin{Lemma}{orthogonal-closed}
  The polar space $\polar{W}$ and the orthogonal complement $\ortho{W}$ of a
  subspace $W\subset V$ are both closed. So is the polar space $\dualpolar{U}$
  of a subspace $U\subset V^*$.
\end{Lemma}

\begin{proof}
  Consider the mapping
  \begin{align}
    \Phi_w\colon V^* &\to \R,\\
    \phi&\mapsto \scal(\phi,w)_{V^*\times V}.
  \end{align}
  For any $w$, the kernel of $\Phi_w$ is closed as
  the pre-image of a closed set. $\polar{W}$ is closed since it is the
  intersection of these kernels for all $w\in W$.

  The inner product is continuous on $V\times V$. Therefore, the
  mapping
  \begin{align}
    \Psi_w\colon V &\to \R,\\
    v&\mapsto \scal(v,w),
  \end{align}
  is continuous. The argument continues as above. Similar for $\dualpolar{U}$.
\end{proof}

\begin{Theorem}{orthogonal-complement}
  Let $W$ be a subspace of a Hilbert space $V$ and $\ortho{W}$ its
  orthogonal complement. Then, $\ortho{W} =
  \ortho{\overline{W}}$. Furthermore, there holds
  \begin{gather}
    V = W \oplus \ortho{W}
    \qquad\Longleftrightarrow\qquad
    \text{$W$ is closed.}
  \end{gather}
\end{Theorem}

\begin{proof}
  Clearly, $\ortho{\overline{W}} \subset \ortho{W}$ since
  $W\subset\overline{W}$. Let now $u\in \ortho{W}$. Then, $\phi =
  \scal(u,\cdot)$ is a continuous linear functional on $V$. Therefore,
  if a sequence $w_n \subset W$ converges to $w\in \overline{W}$, we
  have
  \begin{gather}
    \scal(u,w) = \lim_{n\to\infty} \scal(u,w_n) = 0.
  \end{gather}
  Hence, $u\in \ortho{\overline{W}}$ and $\ortho{W} = \ortho{\overline{W}}$.

  Now, the ``only if'' follows by the fact, that if $W$ is not
  closed, there is an element $w\in \overline{W}$ but not in $W$ such that
  $\scal(w,u)=0$ for all $u\in \ortho{W}$. Thus, $w\not\in \ortho{W}$ and
  consequently $w\not\in \ortho{W} \oplus W$.

  Let now $W$ be closed. We show that there is a unique decomposition
  \begin{gather}
    \label{eq:infsup:8}
    v = w + u,\qquad w\in W, \;u\in \ortho{W},
  \end{gather}
  which is equivalent to $V = W \oplus \ortho{W}$. Uniqueness follows,
  since
  \begin{gather}
    v = w_1+u_1 = w_2+u_2
  \end{gather}
  implies that for any $y\in V$
  \begin{gather}
    0 = \scal(w_1-w_2+u_1-u_2,y) = \scal(w_1-w_2,y) + \scal(u_1-u_2,y).
  \end{gather}
  Choosing $y=u_1-u_2$ and $w_1-w_2$ in turns, we see that one of the
  inner products vanishes for orthogonality and the other implies that
  the difference is zero.

  If $v\in W$, we choose $w=v$ and $u=0$. For $v\not\in W$, we prove
  existence by considering that due to the closedness of $W$ there holds
  \begin{gather}
    d=\inf_{w\in W} \norm{v-w} >0.
  \end{gather}
  Let $w_n$ be a minimizing sequence. Using the parallelogram identity
  \begin{gather}
    \norm{a+b}^2+\norm{a-b}^2 = 2\norm{a}^2+2\norm{b}^2,
  \end{gather}
  we prove that $\{w_n\}$ is a Cauchy sequence by
  \begin{align}
    \norm{w_m-w_n}^2 &= \norm{(v-w_n)-(v-w_m)}^2\\
    &= 2\norm{v-w_n}^2+2\norm{v-w_m}^2-\norm{2v-w_m-w_n}^2\\
    &= 2\norm{v-w_n}^2+2\norm{v-w_m}^2-4\norm*{v-\frac{w_m+w_n}2}^2\\
    &\le 2\norm{v-w_n}^2+2\norm{v-w_m}^2-4d^2,
  \end{align}
  since $(w_m+w_n)/2\in W$ and $d$ is the infimum. Now we use the
  minimizing property to obtain
  \begin{gather}
    \lim_{m,n\to\infty}\norm{w_m-w_n}^2 = 2d^2-2d^2 -4d^2=0.
  \end{gather}
  By completeness of $V$, $w=\lim w_n$ exists and by the closedness of
  $W$, we have $w\in W$. Let $u=v-w$. By continuity of the norm, we
  have $\norm{u}=d$. It remains to show that $u\in \ortho{W}$. To this
  end, we introduce the variation $w+\epsilon \tilde w$ with $\tilde
  w\in W$ to obtain
  \begin{align}
    d^2 &\le \norm{v-w-\epsilon \tilde w}^2\\
    &= \norm{u}^2-2\epsilon\scal(u,\tilde w)+\epsilon^2 \norm{\tilde w},
  \end{align}
  implying for any $\epsilon>0$
  \begin{gather}
    0\le-2\epsilon\scal(u,\tilde w)+\epsilon^2 \norm{\tilde w},
  \end{gather}
  which requires $\scal(u,\tilde w) = 0$.
\end{proof}

\begin{Definition}{orthogonal-projection}
  Let $V$ be a Hilbert space and $W\subset V$ be a closed
  subspace. For a vector $v\in V$, let $v=w+u$ be the unique
  decomposition with $w\in W$ and $u\in \ortho{W}$. Then we call $w$ and
  $u$ the \define{orthogonal projection}s of $v$ into $W$ and $\ortho{W}$,
  respectively. We write
  \begin{gather}
    \Pi_W v = w, \qquad \Pi_{\ortho{W}} v = u.
  \end{gather}
\end{Definition}

% \begin{Lemma}{polar-orthogonal-hilbert}
%   Let $V$ be a Hilbert space and $W\subset V$ be a closed
%   subspace. Then, the polar space $\polar{W}\subset V^*$ and the orthogonal
%   space $\ortho{W}$ can be isometrically identified by \putindex{Riesz
%     representation}.
% \end{Lemma}

% \begin{proof}
%   For every $f$ in the
%   dual of $\ortho{W}$, define $g\in V^*$ by
%   \begin{gather}
%     \scal(g,v)_{V^*\times V} =
%     \scal(f,\Pi_{\ortho{V}}v)_{(\ortho{V})^*\times \ortho{V}}.
%   \end{gather}
%   Clearly, $g(v)=0$ for $v\in W$, therefore $g\in \polar{W}$.
% \end{proof}

% \begin{Corollary}

% \end{Corollary}


\begin{Theorem*}{closed-range}{Closed Range Theorem}
  Let $V,W$ be Hilbert spaces and $A\colon V\to W$ a continuous linear
  operator. Then, the following statements are equivalent:
  \begin{gather}
    \label{eq:infsup:9}
    \begin{split}
      \range A &\text{ is closed in } W,\\
      \range{A^\transpose} &\text{ is closed in } V^*,\\
      \range A &= \dualpolar{\ker{A^\transpose}},\\
      \range{A^\transpose} &= \polar{\ker A}.
    \end{split}
  \end{gather}
\end{Theorem*}

\begin{remark}
  This is the famous \emph{\putindex{closed range theorem}} by Banach.
  It actually holds under weaker assumptions, for instance $V,W$ only
  Banach spaces. The proof can be found for instance
  in~\cite[p.~205--209]{Yosida80}.
\end{remark}

\begin{Theorem*}{open-mapping}{Open Mapping Theorem}
  Let $A\colon V\to W$ be continuous and surjective. Then, the image
  $A(U)\subset W$ of any open set $U\subset V$ is open.
\end{Theorem*}

\begin{remark}
  This is the \emph{\putindex{open mapping theorem}} by Banach. The
  proof can be found for instance in~\cite[p.75--76]{Yosida80}.
\end{remark}

\begin{Lemma}{closed-infsup}
  Let $A\colon V\to W$ be continuous. Then, $\range A$ is closed in
  $W$ if and only if there exists $\ellipa>0$ such that
  \begin{gather}
    \label{eq:infsup:10}
    \forall w\in \range A\;
    \exists v\in V\quad
    Av = w
    \;\wedge\;
    \ellipa \norm{v}_V \le \norm{w}_W.
  \end{gather}
\end{Lemma}

\begin{proof}
  We first show that the inf-sup condition~\eqref{eq:infsup:10}
  implies $\range A$ closed. To this end, let $\{w_n\}$ be a Cauchy
  sequence in $\range A$ converging to a point $w\in W$. By the assumption,
  there
  is a sequence $\{v_n\}$ in $V$ such that $Av_n = w_n$ and
  $\ellipa \norm{v_n} \le \norm{w_n}$. Hence, using
  \begin{gather}
    \norm{v_m-v_n}_V \le \frac1\ellipa \norm{w_m-w_n}_W,
  \end{gather}
  we realize that $\{v_n\}$ is a Cauchy sequence in $V$. Therefore, $v_n\to v\in
  V$ and due to continuity of $A$ we obtain $Av=w$ and thus $w\in
  \range A$.

  Conversely, let $\range A$ be closed in $W$. Thus, it is a Banach
  space and the \putindex{open mapping theorem} applies to $A\colon
  V\to\range A$. We map the open unit ball $B_1(0)\subset V$ and
  obtain that $A(B_1(0))$ is open in $\range A$, implying that there
  is an open ball $B_\delta(0) \subset A(B_1(0))$. This is sufficient
  to construct $v$:

  Let $w\in\range A$. Then,
  \begin{gather}
    \tilde w = \frac\delta2 \frac{w}{\norm{w}}
    \in B_\delta(0) \subset A(B_1(0)).
  \end{gather}
  Hence, there is $v\in V$ with $\norm{v}<1$ such that $Av=\tilde w$,
  which proves the lemma.
\end{proof}

\begin{Theorem}{infsup-well-equivalence}
  Let $a(\cdot,\cdot)$ on $V\times W$ be a bounded bilinear form % such that
%  \begin{gather}
%    a(v,w) \le M \norm{v}_V \norm{w}_W,
%  \end{gather}
  and $A\colon V\to W^*$ its associated operator.
  Then, the following statements are equivalent:
  \begin{enumerate}
  \item There exists $\ellipa>0$ such that
    \begin{gather}
      \label{eq:infsup:11}
      \inf_{w\in W}\sup_{v\in V}
      \frac{a(v,w)}{\norm{v}_V\norm{w}_W}
      \ge \ellipa.
    \end{gather}
  \item The operator $A^\transpose\colon W\to \polar{\ker A}$ is an isomorphism and
    \begin{gather}
      \label{eq:infsup:12}
      \norm{A^\transpose w}_{V^*} \ge \ellipa \norm{w}_{W} \qquad\forall w\in W.
    \end{gather}
  \item The operator $A\colon \ortho{\ker A}\to W^*$ is an isomorphism
    and
    \begin{gather}
      \label{eq:infsup:13}
      \norm{Av}_{W^*} \ge \ellipa\norm{v}_V\qquad \forall v\in \ortho{\ker A}.
    \end{gather}
  \end{enumerate}
\end{Theorem}

\begin{proof}
  First, we show the equivalence of the first two statements. Let us
  use equivalently to the inf-sup condition~\eqref{eq:infsup:11}
  \begin{gather}
    \norm{A^\transpose w}_{V^*}
    = \sup_{v\in V}\frac{\scal(A^\transpose w,v)}{\norm{v}_V}
    = \sup_{v\in V}\frac{a(v,w)}{\norm{v}_V}
    \ge \ellipa\norm{w} \qquad
    \forall w\in W.
  \end{gather}
  Thus, equations~\eqref{eq:infsup:11} and~\eqref{eq:infsup:12} are
  equivalent and we have already proven that the second statement
  implies the first. It remains to show the $A^\transpose$ is an isomorphism
  from $W$ onto $\polar{\ker A}$. Equation~\eqref{eq:infsup:12} implies that
  $A^\transpose\colon W \to \range{A^\transpose}$ is an isomorphism and its inverse is
  bounded by $1/\ellipa$ (multiply both sides by $A^{-1}$). Using
  \slideref{Lemma}{closed-infsup}, we obtain that $\range{A^\transpose}$ is
  closed in $V^*$ and the \putindex{closed range theorem} settles the
  issue.

  In order to prove equivalence of the second and third statement, we
  use the result of \slideref{Problem}{polar-orthogonal} to isometrically
  identify $(\ortho{\ker A})^*$ with $\polar{\ker A}$. Thus, $A$ is an
  isomorphism from $\ortho{\ker A}$ onto $W^*$ if and only if $A^\transpose$ is an
  isomorphism from $W$ onto $(\ortho{\ker A})^* = \polar{\ker A}$. and
  \begin{gather}
    \norm{A}_{W^*\to \ortho{\ker A}} = \norm{A^\transpose}_{\polar{\ker A}\to W}.
  \end{gather}
\end{proof}

\begin{Corollary}{infsup-well-posedness1}
  Let $a(\cdot,\cdot)$ on $V\times W$ be a bounded bilinear form. %such that
  %\begin{gather}
  %  a(v,w) \le M \norm{v}_V \norm{w}_W.
  %\end{gather}
  Let the inf-sup-condition
  \begin{gather}
    \inf_{w\in W}\sup_{v\in V}
    \frac{a(v,w)}{\norm{v}_V\norm{w}_W}
    \ge \ellipa > 0
  \end{gather}
  hold.  Then, the problem finding $w\in W$ such that
  \begin{gather}
    a(v,w) = f(v) \qquad\forall v\in V,
  \end{gather}
  has a unique solution for $f\in \polar{\ker A}$ and
  \begin{gather}
    \norm{w}_W \le \frac1\ellipa \norm{f}_{V^*}.
  \end{gather}
  The opposite implication holds true.
\end{Corollary}

\begin{remark}
  \slideref{Corollary}{infsup-well-posedness1} exhibits an asymmetry
  between the left and right argument. In particular, we obtain a
  unique solution only for the adjoint operator $A^\transpose$, which is
  exactly what we need, when we compute say a pressure from the
  divergence of a velocity field. In general, we consider the
  restriction of $f$ to the polar set of the kernel in the above
  well-posedness result detrimental and would prefer a result that
  holds for all $f\in V^*$. This on the other hand requires
  $\ker A=\{0\}$, or $\overline{\range{A^\transpose}} = W^*$. Then, on the
  other hand, we see that $\range{A^\transpose}$ is closed since $\range{A}$ is
  closed and the closed range theorem holds. Therefore, we obtain the
  following theorem for the case that we require a unique solution for
  all right hand sides.
\end{remark}

\begin{Theorem}{infsup-well-posedness2}
  Let $a(\cdot,\cdot)$ on $V\times W$ be a bounded bilinear form. % such that
%  \begin{gather}
%    a(v,w) \le M \norm{v}_V \norm{w}_W.
%  \end{gather}
  Let for some $\ellipa>0$ the inf-sup-conditions
  \begin{align}
    \inf_{w\in W}\sup_{v\in V}
    \frac{a(v,w)}{\norm{v}_V\norm{w}_W}
    &\ge \ellipa,\\
    \inf_{v\in V}\sup_{w\in W}
    \frac{a(v,w)}{\norm{v}_V\norm{w}_W}
    &\ge \ellipa
  \end{align}
  hold.  Then, the problem finding $v\in V$ such that
  \begin{gather}
    a(v,w) = f(w) \qquad\forall w\in W,
  \end{gather}
  has a unique solution for $f\in W^*$ and
  \begin{gather}
    \norm{v}_V \le \frac1\ellipa \norm{f}_{W^*}.
  \end{gather}
\end{Theorem}

\begin{Problem}{closed-range}
  Assume for $A\colon V\to W^*$ that $\range A$ is closed. Show
  \begin{enumerate}
  \item $A:V\to W^*$ is surjective if and only if $A^\transpose$ is
    injective.
  \item Show that with all other assumptions unchanged, the second
    inf-sup condition in \slideref{Theorem}{infsup-well-posedness2} is
    not only sufficient, but equivalent to $A$ injective.
  \item \slideref{Theorem}{infsup-well-posedness2} seems excessive if
    we only want to establish one of the statements
    \begin{xalignat}3
      \forall f&\in W^*& \exists u&\in V& a(u,w) &= f(w)\\
      \forall g&\in V^*& \exists u&\in W& a(v,u) &= g(v)
    \end{xalignat}
    Is this true or is there equivalence of both inf-sup conditions
    with only one of these statements?
  \end{enumerate}
\end{Problem}

\begin{remark}
  If we compare \slideref{Theorem}{infsup-well-posedness2} with
  \slideref{Corollary}{infsup-well-posedness1}, we see that the only
  difference lies in the fact that the second inf-sup condition
  ensures surjectivity of $A$ by injectivity of $A^\transpose$. In some cases
  it may be impossible difficult to prove both inf-sup conditions. Then, it is
  sufficient to prove one inf-sup condition, say the first, and then
  only injectivity of $A^\transpose$. Although we verify less than the
  assumptions of \slideref{Theorem}{infsup-well-posedness2}, the
  closed range theorem saves us from the additional work. We further
  note that this notion is symmetric between $A$ and $A^\transpose$, that is,
  it is sufficient to prove inf-sup for either operator and
  injectivity for the other.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The inf-sup condition for mixed problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  In the previous section, we have developed a framework for
  well-posedness of problems which are not $V$-elliptic. In principle,
  this theory can be applied to the bilinear form
  $\mathcal A((u,p),(v,q))$ as a whole. On the other hand, we can
  formally split the solution of a constrained minimization problem
  into the reduced problem and then computing the Lagrange multiplier,
  which more clearly exhibits the relation of the two spaces $V$ and
  $Q$ involved in the mixed formulation. Here are the resulting
  theorems.
\end{intro}

\begin{Definition}{mixed-weak}
  The abstract saddle-point problem in weak form reads:
  find $(u,p)\in V\times Q$ such that
  \begin{gather}
    \label{eq:saddle-point-weak}
    \arraycolsep.1em
    \begin{matrix}
      a(u,v) &+& b(v,p) &=& f(v) &\quad&\forall v\in V, \\
      b(u,q) && &=& g(q) &&\forall q\in Q.
    \end{matrix}
  \end{gather}
  Here, $V$ and $Q$ are Hilbert spaces chosen, such that $a(.,.)$ and
  $b(.,.)$ are bounded bilinear forms on $V\times V$ and $V\times Q$,
  respectively. The bounds $\bounda$ and $\boundb$ are such that
  \begin{gather}
    \label{eq:saddle-point-bounds}
    \sup_{u,v\in V}\frac{a(u,v)}{\norm{u}_V\norm{v}_V} = \bounda,
    \qquad
    \sup_{\substack{v\in V\\q\in Q}}\frac{b(v,q)}{\norm{v}_V\norm{q}_Q} = \boundb.
  \end{gather}
\end{Definition}


\begin{Theorem}{infsup-mixed1}
  Let $V$ and $Q$ be Hilbert spaces. Let $a(\cdot,\cdot)$ and
  $b(\cdot,\cdot)$ be bounded bilinear forms. Then, the weak
  formulation find $u\in V$ and $p\in Q$ such that
  \begin{gather}
    a(u,v) + b(v,p) + b(u,q)
    = f(v)+g(q)
    \qquad\forall v\in V, q\in Q,
  \end{gather}
  has a unique solution for any $f\in V^*$ and any $g\in Q^*$ if and
  only if there exists $\ellipa>0$ such that
  \begin{gather}
    \label{eq:infsup:system-infsup}
    \inf_{\substack{u\in V\\p\in Q}}
    \sup_{\substack{v\in V\\q\in Q}}
    \quad \frac{a(u,v) + b(v,p) + b(u,q)}{\norm{(u,p)}_{V\times
        Q}\norm{(v,q)}_{V\times Q}} \ge \ellipa.
  \end{gather}
\end{Theorem}

\begin{proof}
  Straight application of \slideref{Theorem}{infsup-well-posedness2}.
\end{proof}

Often, like in the case of Stokes' equations, the properties of the
form $a(\cdot,\cdot)$ or the form $b(\cdot,\cdot)$ are already known
and you want to combine them to a mixed formulation. Thus, the theorem
above is unwieldy, since it requires to do the analysis from
scratch. We thus provide an equivalence theorem, which allows us to
separate the properties of the bilinear forms.

\begin{Theorem}{infsup-mixed2}
  Let $V$ and $Q$ be Hilbert spaces and let
  \begin{gather}
    \begin{split}
      \ker B &= \bigl\{v\in V \big| b(v,q) = 0 \;\forall q\in Q\bigr\}.
    \end{split}
  \end{gather}
  Then, the mixed problem finding $(u,p)\in V\times Q$ such that
  \begin{gather}
    a(u,v) + b(v,p) + b(u,q) = f(v) \quad\forall v\in V, q\in Q,
  \end{gather}
  is well-posed if and only if the reduced problem
  finding $u\in \ker B$ such that
    \begin{gather}
      a(u,v) = f(v) \quad\forall v\in \ker B
    \end{gather}
    is well-posed for any $f\in V^*$ and there is a positive constant
    $\infsupc$ such that
    \begin{gather}
      \inf_{q\in Q}\sup_{v\in V} \frac{b(v,q)}{\norm{v}_V\norm{q}_Q} \ge \infsupc.
    \end{gather}
\end{Theorem}

\begin{proof}
  In order to show the ``if'', we note that by well-posedness of the
  reduced problem, $u\in V$ is well-determined and bounded by the data
  $f\in V^*$ without knowledge of the Lagrange
  multiplier. Furthermore, there holds $b(u,q) = 0$.
  
  Entering this into the mixed formulation, the Lagrange multiplier
  $p$ is determined by
  \begin{gather}
    b(v,p) = f(v) - a(u,v), \qquad\forall v\in V.
  \end{gather}
  Applying \slideref{Corollary}{infsup-well-posedness1} to the
  bilinear form $b(.,.)$, we deduce that this equation has a unique
  solution $p\in Q$ if and only if $f-a(u,\cdot) \in \polar{\ker B}$, which
  is true due to the statement of the reduced problem.

  For the only if, we note that choosing $v=0$ in the mixed problem,
  we see that there holds
  \begin{gather}
    b(u,q) = 0 \qquad \forall q\in Q,
  \end{gather}
  and thus $u\in \ker B$. For such $u$ holds in the mixed formulation
  \begin{gather}
    f(v) = a(u,v) + b(v,p) = a(u,v) \qquad \forall v \in \ker B.
  \end{gather}
  In order to deduce the inf-sup condition, we note that
  well-posedness of the mixed problem implies the inf-sup
  condition~\eqref{eq:infsup:system-infsup}. Since the infimum will
  not decrease by taking a subset, we confine the set to $u=0$ and
  obtain
  \begin{align}
    &\inf_{p\in Q}\sup_{v\in V} \frac{b(v,p)}{\norm{v}_V\norm{q}_Q}
    \\ = &
           \inf_{p\in Q}
           \sup_{\substack{v\in V\\q\in Q}}
    \frac{a(0,v) + b(v,p) + b(0,q)}{\norm{(0,p)}_{V\times Q} \norm{(v,q)}_{V\times Q}}
    \\ \ge&
            \inf_{\substack{u\in U\\p\in Q}}
    \sup_{\substack{v\in V\\q\in Q}}
    \frac{a(u,v) + b(v,p) + b(u,q)}{\norm{(u,p)}_{V\times Q} \norm{(v,q)}_{V\times Q}} \\\ge& \ellipa > 0
  \end{align}
\end{proof}

\begin{Problem}{inhomogeneous-continuity}
  Show that \slideref{Theorem}{infsup-mixed2} can be extended to the
  case with right hand side $f(v)+g(q)$ with $g\in Q^*$.

\begin{solution}
  We want to solve the problem
  \begin{align}
    a(u,v) + b(v,p) + b(u,q) = f(v)+g(q) \quad\forall v\in V, q\in Q,
  \end{align}
  where $b(v,p)$ fulfills a inf-sup condition.

  \begin{enumerate}
  \item Due to \slideref{Theorem}{infsup-well-equivalence}, the
    operator $B: V\to Q^*$ is surjective and thus, there exists
    $u_g\in V$ such that
    \begin{gather}
      b(u_g,q) = q(q) \quad\forall q\in Q.
    \end{gather}
  \item Now consider the function $u_0 = u-u_g$. For $u$ to solve the
    original problem $u_0$ has to solve
    \begin{align}
      a(u_0+u_g,v) + b(v,p) + b(u_0+u_g,q) = f(v)+g(q) \quad\forall v\in V, q\in Q\\
      \Leftrightarrow a(u_0,v) + b(v,p) + b(u_0,q) = f(v)-a(u_g,v) \quad\forall v\in V, q\in Q
    \end{align}
  \item Due to $a(u_g,v) \leq \bounda \norm{u_g}_V\norm{v}_V$,
    the right-hand side $f(\cdot)-a(u_g,\cdot)$ is in $V^*$
    and we are in the setting of
    \slideref{Theorem}{infsup-mixed2}.
  \end{enumerate}
\end{solution}
\end{Problem}

\begin{remark}
  Since $V$ is a Hilbert space, the decomposition
  $V = \ker B \oplus \ker B^\perp$ is uniquely determined and there is
  a corresponding decomposition $V^* = \polar{(\ker B^\perp)} \oplus \polar{\ker B} = I_\flat\ker B \oplus I_\flat \ortho{\ker B} $,
  such that $f = f^0+f^\perp$ above. The way we solve the reduced
  problem first and then compute the Lagrange multiplier implies that
  the solution $u$ only depends on $f^\perp$ only.
\end{remark}

\begin{remark}
  We have imposed well-posedness of the reduced problem only in an
  abstract way. Depending on $a(.,.)$ we can formulate two conditions:
  ellipticity on $\ker B$ or inf-sup stability on $\ker B$. Indeed,
  most problems considered in this class will have symmetric bilinear
  forms $a(.,.)$, such that ellipticity serves as our usual
  assumption.  In these cases, note that $V$-ellipticity already
  implies the well-posedness on $\ker B$.
\end{remark}

\begin{Notation}{v0-kernel}
  Since the kernels of the bilinear form $b(\cdot,\cdot)$ play an
  important role, we abbreviate
  \begin{gather}
    \begin{split}
      V^0 = \ker B &= \bigl\{v\in V \;\big|\; b(v,q) = 0 \quad\forall q\in Q\bigr\},\\
      Q^0 = \ker{B^\transpose} &= \bigl\{q\in Q \;\big|\; b(v,q) = 0 \quad\forall v\in V\bigr\}.
    \end{split}
  \end{gather}
  We als define for $g\in Q^*$ and for $f\in V^*$ the affine spaces
  \begin{gather}
    \begin{split}
      V^g &= \bigl\{v\in V \;\big|\; b(v,q) = g(q) \quad\forall q\in Q\bigr\},\\
      Q^f &= \bigl\{q\in Q \;\big|\; b(v,q) = f(v) \quad\forall v\in V\bigr\}.
    \end{split}
  \end{gather}
\end{Notation}

\begin{intro}
  We summarize the results of this section in a theorem for
  well-posedness of the mixed formulation in
  \slideref{Definition}{mixed-weak} with simplified assumptions.  It
  will be the basis for further results in this course. We know from
  the discussion above that this assumption is only sufficient and
  weaker conditions may be imposed on $a(.,.)$. But indeed, it helps
  us through a lot of problems and is a good compromise between
  generality and ease of use.
\end{intro}

\begin{Theorem}{mixed-elliptic}
  Let the bilinear form $a(.,.)$ be positive semi-definite on $V$ and
  elliptic on $V^0 = \ker B$. Let the bilinear form $b(\cdot,\cdot)$
  be inf-sup stable. Thus, there are constants $\ellipa>0$ and
  $\infsupc>0$
  \begin{gather}
    \inf_{v\in V^0} \frac{a(v,v)}{\norm{u}_V^2} = \ellipa,  \qquad
    \inf_{q\in Q}\sup_{v\in V} \frac{b(v,q)}{\norm{v}_V\norm{q}_Q} = \infsupc.
  \end{gather}
  Then, the abstract mixed problem in \slideref{Definition}{mixed-weak}
  has a unique solution.
\end{Theorem}

\begin{Problem}{mixed-elliptic}
  Derive bounds for the solutions $u\in V$ and $p\in Q$ using the
  assumptions of the previous theorem.
\end{Problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Galerkin approximation of mixed problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{../mixed/galerkin}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End:
