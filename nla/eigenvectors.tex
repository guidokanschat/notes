\begin{intro}
  After having developed a reliable algorithm for eigenvalues, we now
  turn back to the problem of computing eigenvectors. As pointed out
  earlier, we do not use the transformations of the QR iteration for
  several reasons.

  We do use the transformation to Hessenberg or tridiagonal form
  though, since we will require a QR factorization of the matrix,
  which is cheaper in this form. Hence, we will compute eigenvectors
  of the Hessenberg matrix
  \begin{gather}
    \matH = \matq^* \mata \matq.
  \end{gather}
  Multiplying this equation from the left by $\matq^*$, it is clear
  that $(\lambda,\vv)$ is an eigenpair of $\matH$, if and only if
  $(\lambda,\matq\vv)$ is an eigenpair of $\mata$.

  In the following, we will use shifted matrices $\matH-\lambda\id$,
  where we assume that $\lambda$ is an approximation to an actual
  eigenvalue of $\matH$ of accuracy $\epsilon\ge \eps$.
\end{intro}

\begin{Algorithm*}{inverse-iteration}{Inverse iteration}
    \begin{algorithmic}[1]
    \Require $\matH\in\Cnn$, $\lambda\in\C$, initial vector $\vx^{(0)}\in\C^n$, $\norm{\vx^{(0)}}_2 = 1$
    \For{$k=1,\dots$ until convergence}
    \State Solve $(\matH-\lambda\id) \vy = \vx^{(k-1)}$
    \State $\vx^{(k)} = \frac{\vy}{\norm{\vy}}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{todo}
  Move to first chapter
\end{todo}

\begin{Lemma}{qr-roundoff}
  Let $\matq\matr=\matH$ be an exact QR factorization and let
  $\tilde \matq\tilde\matr$ be computed with floating point accuracy
  $\eps$. Then, there holds with a constant $c$ independent of $\matH$
  \begin{gather}
    \norm{\matH-\tilde\matq\tilde\matr}_2 \le c \eps \norm{\matH}_2.
  \end{gather}
\end{Lemma}

\begin{remark}
  In the ideal case where $\lambda$ is an exact eigenvalue of $\matH$,
  the linear system determining $\vy$ in the inverse iteration is not
  solvable. Using the QR factorization of $\matH$, we know from
  \slideref{Lemma}{perfect-shift}, that one of the diagonal entries of
  the upper triangular factor $\matr$ is zero. If $\matH$ is
  unreduced, this is the entry in the last row.

  A simple remedy for the non-invertibility of $\matH = \matq\matr$ is
  now to replace this zero entry on the diagonal of $\matr$ by a
  zonzero value of size $\eps \norm{\matH}_2$, hence not affecting the
  roundoff error of the QR factorization, but making backward
  substitution possible.

  Note that if this is done in the last row of the matrix, we can put
  any number there, since it corresponds to rescaling the result of
  backward substitution, which in turn will be overwritten by the
  normalization step of the inverse iteration.
\end{remark}

\begin{Lemma}{inverse-epsilon}
  Let $\lambda$ be a simple approximate eigenvalue of $\matH$ with
  error $\epsilon$. Then, the residual after one step admits the estimate
  \begin{gather}
    \norm{\vr^{(1)}} = \norm{\matH\vx^{(0)} - \lambda\vx^{(1)}} \le \frac{\alpha_n}{\epsilon},
  \end{gather}
  where $\alpha_n$ is the coefficient of $\vx^{(0)}$ with respect to the
least dominant eigenvector of $\matH-\lambda\id$.
\end{Lemma}

\begin{proof}
  See~\cite{PetersWilkinson79} for details. Like in the convergence
  proof for the vector iteration, we use a basis $\{\vv_1,\dots,\vv_n$
  of eigenvectors associated to the eigenvalues
  $\sigma_1,\dots,\sigma-n$ of $\matH-\lambda\id$ and write
  \begin{gather}
    \vx^{(0)} = \sum \alpha_i\vv_i.
  \end{gather}
  Hence,
  \begin{gather}
    \vy^{(1)} = \sum \frac{\alpha_i}{\sigma_i} \vv_i.
  \end{gather}
  From the algorithm, we get
  \begin{gather}
    \norm{\matH\vx^{(1)}-\lambda\vx^{(1)}}
    = \frac{\norm{\matH\vy^{(1)}-\lambda\vy^{(1)}}}{\norm{\vy^{(1)}}}
    = \frac{\norm{\vx^{(0)}}}{\norm{\vy^{(1)}}} \le \frac{\epsilon}{\alpha_n}.
  \end{gather}
\end{proof}

\begin{Corollary}{inverse-speed}
  In most cases, the inverse iteration yields an eigenvector
  approximation as good as the approximation of the eigenvalue in one
  to two steps.
\end{Corollary}

\begin{remark}
  The previous results show that the inverse iteration allows us to
  approximate real eigenvectors to real, simple eigenvalues very
  efficiently. If $\matH$ accidentally has a multiple eigenvalue, its
  multiplicity is known because the QR iteration computed a tight
  cluster around this eigenvalue. A combination of varying the initial
  vector and varying the shift between different approximations of the
  eigenvalue typically computes a basis for the invariant subspace of
  the cluster.
\end{remark}

\begin{todo}
  Parlett, Saad: Complex shift and invert strategies for real matrices
\end{todo}

\begin{intro}
  What is missing is the computation of eigenvectors to complex
  eigenvalues of real matrices in real arithmetic. To this end, let
  $\lambda = \mu+i\nu$. We will now describe an algorithm, originally
  from~\cite{ParlettSaad87}, which computes the vector
  $\vy = \vy_r + i \vy_i$ in the algorithm from the vector
  $\vx = \vx_r+i\vx_i$. Since we are only considering a single step,
  we omit the index of the sequence.
\end{intro}

\begin{Lemma}{inverse-complex}
  The complex linear system
  \begin{gather}
    \label{eq:inverse:1}
    (\matH - \lambda\id) \vy = \vx
  \end{gather}
  with real matrix $\matH$ and complex shift $\lambda = \mu+i\nu$, can
  be solved by consecutively solving for $\vy_r$ and $\vy_i$
  \begin{align}
    \bigl((\matH-\mu\id)^2+\nu^2\id\bigr) \vy_r
    &= (\matH-\mu\id) \vx_r - \nu \vx_i\\
    (\matH-\mu\id) \vy_i &= \vx_i + \nu\vy_r.
  \end{align}
\end{Lemma}

\begin{proof}
  Splitting the system~\eqref{eq:inverse:1} into its real and
  imaginary parts, we obtain the real block system
  \begin{align}
    (\matH - \mu\id) \vy_r + \nu \vy_i &= \vx_r\\
    -\nu \vy_r + (\matH - \mu\id) \vy_i &= \vx_i.
  \end{align}
  % For the inverse of its block matrix, we easily verify the formula
  % \begin{gather}
  %   \begin{pmatrix}
  %     \matH - \mu\id & \nu\\
  %     -\nu & \matH - \mu\id
  %   \end{pmatrix}^{-1}
  %   =
  %   \begin{pmatrix}
  %     \mats^{-1}&-\nu\matt^{-1}\\\nu\matt^{-1}&\mats^{-1}
  %   \end{pmatrix},
  % \end{gather}
  % where
  % \begin{align}
  %   \mats &= (\matH-\mu\id) + \nu^2(\matH-\mu\id)^{-1}\\
  %   \matt &= (\matH-\mu\id)^2 + \nu^2\id.
  % \end{align}
  % Hence,
  % \begin{gather}
  %   \begin{pmatrix}
  %     \vy_r\\\vy_i
  %   \end{pmatrix}
  %   =
  %   \begin{pmatrix}
  %     \mats^{-1}&-\nu\matt^{-1}\\\nu\matt^{-1}&\mats^{-1}
  %   \end{pmatrix}
  %   \begin{pmatrix}
  %     \vx_r\\\vx_i
  %   \end{pmatrix}
  %   .
  % \end{gather}
  Now, we formally solve for $\vy_i$ in the second row, obtaining
  \begin{gather}
    \vy_i = (\matH-\mu\id)^{-1}(\nu\vy_r+\vx_i).
  \end{gather}
  Entering this into the first equation yields
  \begin{gather}
    \bigl((\matH-\mu\id) + \nu^2(\matH-\mu\id)^{-1}\bigr)\vy_r
    = \vx_r - \nu (\matH-\mu\id)^{-1}\vx_i.
  \end{gather}
  The result of the lemma is obtained multiplying by $\matH-\mu\id$.
\end{proof}

\begin{Algorithm*}{inverse-iteration-complex}{Inverse iteration for complex eigenvectors}
    \begin{algorithmic}[1]
    \Require $\matH\in\Cnn$, $\lambda\in\C$, initial vector $\vx_r^{(0)},\vx_i^{(0)}\in\R^n$, $\norm{\vx_r^{(0)}}_2 = \norm{\vx_i^{(0)}}_2 = 1$
    \For{$k=1,\dots$ until convergence}
    \State Solve $\bigl((\matH-\mu\id)^2+\nu^2\id\bigr) \vy_r^{(k)}
    = (\matH-\mu\id) \vx_r^{(k-1)} - \nu \vx_i^{(k-1)}$
      \State Solve $(\matH-\mu\id) \vy_i^{(k)} = \vx_i^{(k-1)} + \nu\vy_r^{(k)}$
    \State $\vx_r^{(k)} = \frac{\vy_r^{(k)}}{\norm{\vy_r^{(k)}}}$
    \State $\vx_i^{(k)} = \frac{\vy_i^{(k)}}{\norm{\vy_i^{(k)}}}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{todo}
\begin{remark}
  For this algorithm, we typically need two QR factorizations, one for
  $\matH-\mu\id$ and one for $\bigl((\matH-\mu\id)^2+\nu^2\id\bigr)$.
\end{remark}
\end{todo}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
