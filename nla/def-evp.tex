\begin{todo}
  GM: Fixed a typo
\end{todo}
\begin{intro}
  The following results can be found in any book on linear
  algebra. Thus, we will just keep the arguments short. There will be a
  focus on normal matrices justified by results on conditioning of
  eigenvalue problems later on.

  Thus, spectral theory based on module theory will not be needed in
  this class. The spectral theorem for normal matrices on the other
  hand is fairly simple and can be proved without too much overhead.
\end{intro}

\begin{Definition}{eigenvalue}
  An \define{eigenvalue} of a matrix $\mata\in \C^{n\times n}$ is a
  complex number $\lambda$ such that the matrix
  \begin{gather}
   \mata-\lambda\id
  \end{gather}
  is singular.

  The set of all eigenvalues of $\mata$ is called the
  \define{spectrum} $\sigma(\mata)$.

  The \define{eigenspace} for $\lambda$ is the kernel of
  $A-\lambda\id$, that is, the set
\begin{gather}
    \esp{\lambda} = \bigl\{
    \vv \in \C^n \;\big\vert\;
    \mata\vv = \lambda\vv \bigr\}.
\end{gather}
The \define{geometric multiplicity} of $\lambda$ is the dimension of
$\esp{\lambda}$.


An \define{eigenvector} for $\lambda$ is a (normed) vector in
$\esp\lambda$. We refer to an eigenvector $\lambda$ and a
corresponding eigenvector $\vv$ as \define{eigenpair}.
\end{Definition}

\begin{Definition}{eigenvalue-algebraic}
  An \define{eigenvalue} of a matrix $\mata\in \C^{n\times n}$ is a root of the characteristic polynomial $\chi(\lambda) = \det(\mata-\lambda\id)$.

  The \define{algebraic multiplicity} of an eigenvalue is the multiplicity of the corresponding root of the characteristic polynomial.
\end{Definition}

\begin{Lemma}{eigenvalue-equivalent}
  The two definitions of an eigenvalue are consistent.
\end{Lemma}

\begin{Theorem}{eigenvalue-count}
  Every matrix in $\C^{n\times n}$ has at most $n$ distinct eigenvalues. The algebraic multiplicities of all eigenvalues add up to $n$.
\end{Theorem}

\begin{proof}
  The ``at most'' follows from the fact that a polynomial contains
  linear factors $x-\lambda_i$ for each of its roots
  $\lambda_i$. Thus, if the characteristic polynomial has $k$ roots it
  has at least degree $k$. On the other hand, the characteristic
  polynomial has degree $n$, such that $k\le n$.

  The second statement is due to the fact that every polynomial over
  $\C$ is a product of linear factors.
\end{proof}

\begin{remark}
  The last theorem is not true in $\R$, as it is not algebraically
  closed. Thus, even a real matrix may have complex eigenvalues and
  eigenvectors. Therefore, all results in this chapter will be on
  complex matrices, but some simplifications for real matrices will be
  pointed out.
\end{remark}

\begin{Definition}{eigenvalue-simple}
  An eigenvalue is \define{simple}, if its algebraic and geometric multiplicity are one. It is \define{semi-simple}, if its algebraic and geometric multiplicities are equal.
\end{Definition}

\begin{todo}
  GM:
  Added Definition of Dominant Eigenvalue
\end{todo}
\begin{Definition}{dominant_ev}
  The \(r\) largest Eigenvalues by magnitude \(\lambda_1, \ldots \lambda_r\) are called \define{dominant eigenvalues}, if
  \[ |\lambda_1| = \cdots = |\lambda_r|\]
  Their corresponding eigenvectors are called \define{dominant eigenvectors}.
\end{Definition}
\begin{Definition}{right-left-ev}
  Refining \slideref{Definition}{eigenvalue}, we distinguish between a right eigenvector $\vv$ such that
  \begin{gather*}
    \mata \vv = \lambda \vv,
  \end{gather*}
  and a left eigenvector $\vu$ such that
  \begin{gather*}
    \vu^T \mata = \lambda \vu^T.
  \end{gather*}

\end{Definition}

\begin{Lemma}{eigenvalues-conjugate}
  Every eigenvalue $\lambda$ of $\mata\in\C^{n\times n}$ is also an
  eigenvalue of $\mata^T$. Furthermore, a left eigenvector of $\mata$
  is a right eigenvector of $\mata^T$ for the same eigenvalue and vice
  versa.
\end{Lemma}

\begin{proof}
  The determinant does not change when the matrix is transposed, therefore
  \begin{gather}
    \chi(\mata^T)
    = \det(\mata^T-\lambda \id)
    = \det(\mata-\lambda \id)
    = \chi(\mata).
  \end{gather}
  Thus, the eigenvalues of $\mata$ and of $\mata^T$ coincide.
\end{proof}

\subsection{Normal and Hermitian matrices}

\begin{Definition}{normal-Hermitian}
  A matrix $\mata\in\C^{n\times n}$ is called \define{normal} if there holds
  \begin{gather}
      A^*A = AA^*.
  \end{gather}
  It is called \define{Hermitian} or \define{complex symmetric}, if there holds
  \begin{gather}
      A=A^*.
  \end{gather}
\end{Definition}

\begin{Lemma}{Hermitian-eigenvalues-real}
  All eigenvalues of a Hermitian matrix are real.
\end{Lemma}

\begin{Theorem*}{Hermitian-diagonalizable}{Spectral theorem for Hermitian matrices}
  A Hermitian matrix $\mata\in\C^{n\times n}$ is diagonalizable with
  an orthogonal basis of eigenvectors and real eigenvalues. That is,
  there is a real, diagonal matrix $\matlambda$ and a unitary matrix
  $\matq$ such that
  \begin{gather}
    \mata = \matq^T \matlambda\matq.
  \end{gather}
\end{Theorem*}

%\begin{proof}
%\end{proof}

\begin{Corollary}{symmetric-diagonalizable}
  A symmetric matrix $\mata\in\R^{n\times n}$ is diagonalizable with
  an orthogonal basis of eigenvectors and real eigenvalues.
\end{Corollary}

\begin{Theorem*}{normal-diagonalizable}{Spectral theorem for normal matrices}
  A matrix $\mata\in\C^{n\times n}$ is normal if and only if it is diagonalizable by a unitary matrix.

  It is normal if and only if there exists an orthonormal basis of eigenvectors.
\end{Theorem*}

\begin{todo}
  GM: Proof Missing?
\end{todo}
\begin{proof}

\end{proof}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
