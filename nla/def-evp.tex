\begin{intro}
  The following results can be found in any book on linear
  algebra. Thus, we will just keep the arguments short. There will be a
  focus on normal matrices justified by results on conditioning of
  eigenvalue problems later on.

  Thus, spectral theory based on module theory will not be needed in
  this class. The spectral theorem for normal matrices on the other
  hand is fairly simple and can be proved without too much overhead.
\end{intro}

\begin{Definition}{eigenvalue}
  An \define{eigenvalue} of a matrix $\mata\in \C^{n\times n}$ is a
  complex number $\lambda$ such that the matrix
  \begin{gather}
   \mata-\lambda\id
  \end{gather}
  is singular.

  The set of all eigenvalues of $\mata$ is called the
  \define{spectrum} $\sigma(\mata)$.

  The \define{eigenspace} for $\lambda$ is the kernel of
  $A-\lambda\id$, that is, the set
\begin{gather}
    \esp{\lambda} = \bigl\{
    \vv \in \C^n \;\big\vert\;
    \mata\vv = \lambda\vv \bigr\}.
\end{gather}
The \define{geometric multiplicity} of $\lambda$ is the dimension of
$\esp{\lambda}$.


An \define{eigenvector} for $\lambda$ is a (normed) vector in
$\esp\lambda$. We refer to an eigenvector $\lambda$ and a
corresponding eigenvector $\vv$ as \define{eigenpair}.
\end{Definition}

\begin{Definition}{eigenvalue-algebraic}
  An \define{eigenvalue} of a matrix $\mata\in \C^{n\times n}$ is a root of the characteristic polynomial $\chi(\lambda) = \det(\mata-\lambda\id)$.

  The \define{algebraic multiplicity} of an eigenvalue is the multiplicity of the corresponding root of the characteristic polynomial.
\end{Definition}

\begin{Lemma}{eigenvalue-equivalent}
  The two definitions of an eigenvalue are consistent.
\end{Lemma}

\begin{Theorem}{eigenvalue-count}
  Every matrix in $\C^{n\times n}$ has at most $n$ distinct eigenvalues. The algebraic multiplicities of all eigenvalues add up to $n$.
\end{Theorem}

\begin{proof}
  The ``at most'' follows from the fact that a polynomial contains
  linear factors $x-\lambda_i$ for each of its roots
  $\lambda_i$. Thus, if the characteristic polynomial has $k$ roots it
  has at least degree $k$. On the other hand, the characteristic
  polynomial has degree $n$, such that $k\le n$.

  The second statement is due to the fact that every polynomial over
  $\C$ is a product of linear factors.
\end{proof}

\begin{remark}
  The last theorem is not true in $\R$, as it is not algebraically
  closed. Thus, even a real matrix may have complex eigenvalues and
  eigenvectors. Therefore, all results in this chapter will be on
  complex matrices, but some simplifications for real matrices will be
  pointed out.
\end{remark}

\begin{Definition}{eigenvalue-simple}
  An eigenvalue is \define{simple}, if its algebraic and geometric multiplicity are one. It is \define{semi-simple}, if its algebraic and geometric multiplicities are equal.
\end{Definition}

\begin{Definition}{dominant_ev}
  We call $\lambda$ the \define{dominant eigenvalue} if for any other
  eigenvalue $\mu\in\sigma(\mata)$, there holds
  \begin{gather}
    \abs{\lambda} > \abs{\mu}.
  \end{gather}
  This definition naturally extends to several eigenvalues: let the
  eigenvalues be ordered by magnitude,
  \begin{gather}
    \abs{\lambda_{1}} \ge \abs{\lambda_{2}} \ge \dots \ge \abs{\lambda_{n}}.
  \end{gather}
  Then, the first $r$ eigenvalues are called dominant if there holds
  \begin{gather}
    \abs{\lambda_{r}} \ge\abs{\lambda_{r+1}}.
  \end{gather}
  Their corresponding eigenvectors are called \define{dominant
    eigenvectors}.
\end{Definition}
\begin{Definition}{right-left-ev}
  Refining \slideref{Definition}{eigenvalue}, we distinguish between a right eigenvector $\vv$ such that
  \begin{gather*}
    \mata \vv = \lambda \vv,
  \end{gather*}
  and a left eigenvector $\vu$ such that
  \begin{gather*}
    \vu^T \mata = \lambda \vu^T.
  \end{gather*}
\end{Definition}

\begin{Lemma}{eigenvalues-conjugate}
  Every eigenvalue $\lambda$ of $\mata\in\C^{n\times n}$ is also an
  eigenvalue of $\mata^T$. Furthermore, a left eigenvector of $\mata$
  is a right eigenvector of $\mata^T$ for the same eigenvalue and vice
  versa.
\end{Lemma}

\begin{proof}
  The determinant does not change when the matrix is transposed, therefore
  \begin{gather}
    \chi(\mata^T)
    = \det(\mata^T-\lambda \id)
    = \det(\mata-\lambda \id)
    = \chi(\mata).
  \end{gather}
  Thus, the eigenvalues of $\mata$ and of $\mata^T$ coincide.
\end{proof}

\begin{Theorem}{invariant-eigenvalues}
  For $\lambda_i\in \sigma(\mata)$ with algebraic multiplicity $m_i$, the
  nullspace $V_i$ of $(\mata-\lambda_i\id)^{m_i}$ is an invariant subspace
  of $\C^n$. There holds
  \begin{gather}
    \C^n = \bigoplus_{i} V_i,
  \end{gather}
  where the sum is over all distinct eigenvalues.
\end{Theorem}

\begin{Definition}{spectral-projector}
  Let $\vv_1,\dots,\vv_n$ be a basis of $\C^n$ observing the invariant
  subspace structure of t he previous theorem. Let in particular
  $\vv_{j_{i_1}},\dots,\vv_{j_{i_{m_i}}}$ be the basis vectors spanning
  $V_i$. Then, the \define{spectral projector} of a vector $\vv = \sum \alpha_j \vv_j$ to
  $V_i$ is defined by:
  \begin{gather}
    \Pi_i \vv = \sum_{k=1}^{m_i} \alpha_{j_{i_k}} \vv_{j_{i_k}}.
  \end{gather}
\end{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Normal and Hermitian matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Definition}{normal-Hermitian}
  A matrix $\mata\in\C^{n\times n}$ is called \define{normal} if there holds
  \begin{gather}
      A^*A = AA^*.
  \end{gather}
  It is called \define{Hermitian} or \define{complex symmetric}, if there holds
  \begin{gather}
      A=A^*.
  \end{gather}
\end{Definition}

\begin{Lemma}{Hermitian-eigenvalues-real}
  All eigenvalues of a Hermitian matrix are real.
\end{Lemma}

\begin{proof}
  There holds
  \begin{gather}
    \lambda\scal(\vv,\vv) = \scal(\lambda \vv,\vv) = \scal(\mata \vv,\vv)
    = \scal(\vv,\mata\vv) = \scal(\vv,\lambda\vv) = \overline\lambda\scal(\vv,\vv).
  \end{gather}
  Hence, $\lambda=\overline\lambda$.
\end{proof}

\begin{Theorem*}{Hermitian-diagonalizable}{Spectral theorem for Hermitian matrices}
  A Hermitian matrix $\mata\in\C^{n\times n}$ is diagonalizable with
  an orthogonal basis of eigenvectors and real eigenvalues. That is,
  there is a real, diagonal matrix $\matlambda$ and a unitary matrix
  $\matq$ such that
  \begin{gather}
    \mata = \matq^T \matlambda\matq.
  \end{gather}
\end{Theorem*}

\begin{proof}
  We prove the theorem by induction over $n$. For $n = 1$, the statement is obvious.

  Now, let $n>1$. Every matrix has at least one eigenpair, say
  $(\lambda_n,\vq_n)$ with real $\lambda_n$. Now introduce the space $W\subset \C^n$
  orthogonal to $\vq_n$. For any $\vw\in W$, there holds
  \begin{gather}
    \scal(\vq_n,\mata\vw) = \scal(\mata\vq_n,\vw) = \lambda_n\scal(\vq_n,\vw) = 0.
  \end{gather}
  Hence, $W$ is an invariant subspace with respect to $\mata$. With an
  orthonormal basis $\vw_1,\dots,\vw_{n-1}$ for $W$, we can introduce
  an orthonormal basis $\widetilde Q = \vw_1,\dots,\vw_{n-1},\vq_n$, such that
  \begin{gather}
    \widetilde \matq^* \mata \widetilde \matq =
    \begin{pmatrix}
      \widetilde A & 0 \\ 0 & \lambda_n
    \end{pmatrix}.
  \end{gather}
  Since $\widetilde A$ is of dimension $n-1$, we can use the induction
  argument to deduce that there is an orthonormal basis
  $\vq_1,\dots,\vq_{n-1}\in\C^{n-1}$ which converts it to a diagonal matrix with
  real eigenvalues. Introducing $\matq_{n-1} = \vq_1,\dots,\vq_{n-1}$, we obtain
  \begin{gather}
    \begin{pmatrix}
      \lambda_1&&\\
      &\ddots&\\
      &&\lambda_n
    \end{pmatrix}
    =
    \begin{pmatrix}
      1 &\\ &\matq_{n-1}
    \end{pmatrix}^*
    \widetilde \matq^* A \widetilde \matq
    \begin{pmatrix}
      1 &\\ &\matq_{n-1}
    \end{pmatrix}
  \end{gather}
\end{proof}

\begin{Corollary}{symmetric-diagonalizable}
  A symmetric matrix $\mata\in\R^{n\times n}$ is diagonalizable with
  an orthogonal basis of eigenvectors and real eigenvalues.
\end{Corollary}

\begin{Theorem*}{normal-diagonalizable}{Spectral theorem for normal matrices}
  A matrix $\mata\in\C^{n\times n}$ is normal if and only if it is diagonalizable by a unitary matrix.

  It is normal if and only if there exists an orthonormal basis of eigenvectors.
\end{Theorem*}

\begin{proof}

\end{proof}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
