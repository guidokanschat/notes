%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Krylov spaces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Definition}{krylov-space}
  For a matrix $\mata\in\Rnn$ and a vector $\vv\in\R^n$, we define the
  \define{Krylov space}
  \begin{gather}
    \krylov_m = \krylov_m(\mata,\vv)
    = \spann{\vv,\mata\vv,\dots,\mata^{m-1}\vv}.
  \end{gather}
\end{Definition}

\begin{todo}
  Note that because of the power method, the vectors become more and
  more aligned, hance we have to find a stable representation of the
  space.
\end{todo}

\begin{Definition}{grade-of-v}
  For a matrix $\mata\in\Rnn$ and a vector $\vv\in\R^n$ we define the
  \define{grade} of $\vv$, more precise the grade of $\vv$ with
  respect to $\mata$ as the minimal grade of a polynomial $p$ such
  that
  \begin{gather}
    p(\mata)\vv = 0
  \end{gather}
\end{Definition}

\begin{Lemma}{krylov-polynomial}
  Let $\mu$ be the grade of $\vv$ with respect to $\mata$.  The Krylov
  space $\krylov_m(\mata,\vv)$ is isomorphic to $\P_{m-1}$ if and only
  if $m\le \mu$. In this case, for any $\vw\in\krylov_m(\mata,\vv)$
  there is a unique polynomial $p\in\P_{m-1}$ such that
  \begin{gather}
    \label{eq:krylov-polynomial-1}
    \vw = p(\mata)\vv.
  \end{gather}

  The Krylov space $\krylov_m(\mata,\vv)$ is invariant under the
  action of $\mata$ if and only if $m\ge\mu$.
\end{Lemma}

\begin{proof}
  See also~\cite[Propositions 6.1 \& 6.2]{Saad00}.
  The definition of $\krylov_m(\mata,\vv)$ implies that any vector $\vw$ in this space has a representation
  \begin{gather}
    \vw = \sum_{i=0}^{m-1} \alpha_i \mata^i\vv =: p_{\vw}(\mata) \vv.
  \end{gather}
  Since the vectors in this sum span $\krylov_m$, its dimension is
  $m$ if and only if they are linearly independent, which in turn is
  the case if and only if there is a nonzero polynomial
  $p_0\in\P_{m-1}$ such that
  \begin{gather}
    p_0(\mata)\vv = 0.
  \end{gather}
  By assumption, such a polynomial exists if and only if
  $\mu<m$. Thus, we have proven that $\dim \krylov_m = m$ if and only
  if $m\le \mu$ and thus the mapping induced
  by~\eqref{eq:krylov-polynomial-1} is an isomorphism.

  On the other hand, $\krylov_m$ is invariant, if and only if the
  vectors spanning $\krylov_{m+1}$ are linearly dependent. We have
  just proven that this is the case if and only if $\mu<m+1$ or
  $\mu\le m$.
\end{proof}

\begin{Lemma}{krylov-projector}
  Let $\matp_m$ be a projector onto $\krylov_m=\krylov_m(\mata,\vv)$ and define
  \begin{align}
    \mata_m\colon \krylov_m &\to \krylov_m\\
    \vv&\mapsto \matp_m \mata.
  \end{align}
  Then, for any polynomial $q\in\P_{m-1}$ there holds
  \begin{gather}
    \label{eq:krylov-projector-1}
    q(\mata)\vv = q(\mata_m)\vv.
  \end{gather}
  For any polynomial $p\in\P_m$, there holds
  \begin{gather}
    \label{eq:krylov-projector-2}
    \matp_m p(\mata)\vv = p(\mata_m)\vv.
  \end{gather}
\end{Lemma}

\begin{proof}
  See \cite[Proposition 6.3]{Saad00}.  First, we
  prove~\eqref{eq:krylov-projector-1} by proving it for the monomial
  basis $q_i(x) = x^i$, $i=1,\dots,m-1$. It is obviously true for
  $q_0\equiv 1$. Now, we assume it is true for $q_{i-1}$ and conclude for
  $q_{i}$. Since $i\le m-1$, we have $q_{i}\in\P_{m-1}$ and hence
  $q_{i}(\mata)\vv\in \krylov_m$.  We observe that
  \begin{gather}
    q_{i}(\mata)\vv = \matp_m q_{i}(\mata)\vv = \matp_m \mata q_{i-1}(\mata)\vv
    = \mata_m q_i(\mata_m)\vv
    = q_{i}(\mata_m)\vv.
  \end{gather}
  Thus, we have proven the
  statement for $q\in\P_{m-1}$. The proof for $q_{m}(x) = x^m$ uses
  the same induction step, just with the exception that the action of
  $\matp_m$ on the left is not the identity anymore, since
  $q_m(\mata)\vv$ may not be in $\krylov_m$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Arnoldi and Lanczos processes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  We now develop algorithms which for a given matrix $\mata\in\Rnn$
  and initial vector $\vv\in\R^n$ construct the Krylov space
  $\krylov_m$ and an orthogonal basis dimension by dimension. This
  algorithm is named after Walter E. Arnoldi in the general case and
  after Cornelius Lanczos in the symmetric case.
\end{intro}

\begin{Lemma}{Hessenberg-Krylov-1}
  Let $\mata\in\Rnn$ be given. Let $\matq$ be orthogonal with column
  vectors $\vq_j$ and $\matH$ be Hessenberg, such that
  \begin{gather}
    \label{eq:krylov:1}
    \matH = \matq^*\mata\matq.
  \end{gather}
  Then, for any $j=1,\dots,n-1$
  \begin{gather}
    \label{eq:krylov:2}
    \mata\vq_j = \sum_{k=1}^{j+1} h_{kj}\vq_k.
  \end{gather}
  If $\matH[1:m,1:m]$ is unreduced for some $m\le n$, the vectors
  $\vq_1,\dots,\vq_m$ form a basis of the Krylov space
  $\krylov_{m}(\mata,\vq_1)$.
\end{Lemma}

\begin{proof}
  We rewrite~\eqref{eq:krylov:1} as
  \begin{gather}
    \mata\matq = \matq\matH.
  \end{gather}
  Column $j$ of this relation is exactly~\eqref{eq:krylov:2}. Now we
  revert this relation and write
  \begin{gather}
    \label{eq:krylov:3}
    h_{j+1,j} \vq_{j+1} = \mata\vq_j - \sum_{k=1}^j h_{kj} \vq_j.
  \end{gather}
  Clearly, $\vq_1\in\krylov_1(\mata,\vq_1)$.
  Furthermore,~\eqref{eq:krylov:3} yields the induction argument that
  $\vq_{j+1}\in \krylov_{j+1}(\mata,\vq_1)$ if
  $\vq_1,\dots,\vq_j\in \krylov_j(\mata,\vq_1)$. Note that this
  argument is not possible if $h_{j+1,j}=0$.

  Now that we know that $\vq_1,\dots,\vq_m\in\krylov_{m}(\mata,\vq_1)$
  are an orthogonal set, they must form a basis since the dimension of
  $\krylov_{m}(\mata,\vq_1)$ is $m$.
\end{proof}

\begin{Theorem}{arnoldi-projection}
  Let $\matq_m = (\vq_1,\dots,\vq_m)$ be the first $m$ columns of the
  matrix $\matq$ in $\matH = \matq^*\mata\matq$ and let
  $\overline{\matH}_m\in\R^{m+1\times m}$ and
  $\matH_m\in\R^{m\times m}$ be the upper left blocks of $\matH$ of
  according dimensions. Then, there holds
  \begin{gather}
    \label{eq:arnoldi-projection-2}
    \mata\matq_m
    = \matq_{m+1}\overline{\matH}_m
  \end{gather}
  Furthermore, the matrix
  \begin{gather}
    \label{eq:arnoldi-projection-3}
    \matH_m = \matq_m^T\mata\matq_m
  \end{gather}
  is the projection of $\mata$ to $\krylov_m(\mata,\vq_1)$ expressed
  in the basis $\matq_m$.
\end{Theorem}

\begin{proof}
  The first statement only rephrases~\eqref{eq:krylov:2}. For the
  second, we observe that $\matq_m^*\matq_{m+1}$ is the identity
  $\id_m$ with an additional zero-column to the right. Hence,
  \begin{gather}
    \matH_m = \matq_m^*\matq_{m+1}\overline\matH_{m}.
  \end{gather}
  Entering this into~\eqref{eq:arnoldi-projection-2} yields the result.
\end{proof}

\begin{Lemma}{Hessenberg-essentially-equal}
  Let $\matH_m = \matq_m^*\mata\matq$ with a matrix $\mata\in\Rnn$, a
  Hessenberg matrix $\matH\in\R^{m\times m}$ and an orthogonal matrix
  $\matq\in\R^{n\times m}$. Let $\matu_m\R^{n\times m}$ be a second
  orthogonal matrix, essentially equal to $\matq_m$. Then,
  $\matg_m = \matu_m^*\mata\matu_m$ is a Hessenberg matrix as well.
\end{Lemma}

\begin{proof}
  We note that each column $\vu_i$ of $\matu_m$ is equal to the
  corresponding column $\vq_i$ of $\matq_m$ up to a factor $\pm 1$. Hence,
  \begin{gather}
    \abs{g_{ij}} = \abs{\ve_i^*\matg_m\ve_j}
    = \abs{\vu_i^*\mata\vu_j}
    = \abs{\vq_i^*\mata\vq_j}
    = \abs{\ve_i^*\matH_m\ve_j}
    = \abs{h_{ij}}.
  \end{gather}
  In particular, $\matg_m$ has zero entries wherever $\matH$ has them.
\end{proof}

\begin{Theorem}{Hessenberg-Krylov-2}
  Let the grade of $\vv$ with respect to $\mata$ be not less than
  $m$. Define the \define{Krylov matrix}
  \begin{gather}
    \matk_m = \left(\vv,\mata\vv,\dots,\mata^{m-1}\vv\right).
  \end{gather}
  Let $\matq_m$ be the matrix of the QR factorization
  \begin{gather}
    \matk_m = \matq_m\matr_m.
  \end{gather}
  Then $\matq_m^*\mata\matq_m$ is Hessenberg.
\end{Theorem}

\begin{proof}
  We show that the matrix $\matq_m$ here is essentially equal to the
  matrix $\matq_m$ in \slideref{Theorem}{arnoldi-projection}, which we
  will call $\tilde \matq_m$ here. The result then follows from the
  preceding lemma.

  Using \slideref{Lemma}{qr-columns} about the span of the left
  columns of the QR factorization, we have for $j=1,\dots,m$
  \begin{gather}
    \spann{\vq_1,\dots,\vq_j} = \spann{\vv,\mata\vv,\dots,\mata^{j-1}\vv} = \krylov_j(\mata,\vv).
  \end{gather}
  Comparing this
  to~\eqref{eq:krylov:2}, the condition there is
  \begin{gather}
    \spann{\tilde\vq_1,\dots,\tilde\vq_j} = \spann{\vv,\mata\tilde\vq_1,\dots,\mata\tilde\vq_{j-1}}
    = \krylov_j(\mata,\vv).
  \end{gather}
  Since we have $\vq_1 = \tilde\vq_1 = \nicefrac{\vv}{\norm{\vv}}$, we
  can use an induction argument to show that
  $\tilde\vq_j\parallel\vq_j$, which implies that they are essentially equal.
\end{proof}

\begin{Corollary}{Hessenberg-Krylov}
  The last theorem shows that the Hessenberg similarity
  transformations in \slideref{Lemma}{Hessenberg-Krylov-1} and
  \slideref{Theorem}{arnoldi-projection} can be chosen with any
  normalized vector $\vq_1$ as long as its grade is sufficiently
  high.
\end{Corollary}

\begin{Algorithm*}{arnoldi-1}{Arnoldi}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vq_1\in\R^n, \norm{\vq_1}_2 = 1$
    \For{$j=1,\dots,m$}
    \State $\vw_j \gets \mata\vq_j$
    \For{$i=1,\dots,j$}
    \State $h_{ij} \gets \scal(\mata\vq_j,\vq_i)$
    \State $\vw_j \gets \vw_j - h_{ij}\vq_i$
    \EndFor
    \State $h_{j+1,j} \gets \norm{\vw_j}_2$
    \If{$h_{j+1,j}=0$} \textbf{stop}\EndIf
    \State $\vq_{j+1} = \nicefrac{\vw_j}{h_{j+1,j}}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{arnoldi-w}
  We can rewrite~\eqref{eq:arnoldi-projection-2} using the vector
  $\vw_j$ as it was computed by orthogonalization as
  \begin{gather}
    \label{eq:arnoldi-projection-1}
    \mata\matq_m
    = \matq_m\matH_m+\vw_m\ve_m^T,
  \end{gather}
  with $\ve_m\in \R^m$.
\end{Lemma}

\begin{proof}
  From line 9, we have $\vw_m = h_{m+1,m} \vq_{m+1}$. Hence, the sum
  in~\eqref{eq:krylov:2} can be rewritten without using the matrix
  $\overline\matH_m$ as
  \begin{align}
    \mata\vq_m
    &= \sum_{k=1}^{m} h_{km}\vq_k + h_{m+1,m} \vq_{m+1}\\
    &= \matq_m\matH_m \ve_m + \vw_m,
  \end{align}
  which is the $m$-th column of the matrix $\matq_m\matH_m+\vw_m\ve_m^T$.
\end{proof}  

\begin{Notation}{arnoldi-step}
  We refer to step $m$ of the Arnoldi process as the step generating
  the vector $\vq_m$.
\end{Notation}

\begin{remark}
  The next step of the Arnoldi method can always be computed if
  $h_{m+1,m}\neq 0$ in line 9. If it cannot be, we speak of a
  \define{breakdown} of the method.
\end{remark}

\begin{Lemma}{arnoldi-breakdown}
  The Arnoldi method stops at step $m$ if and only if the subspace
  $\krylov_m$ is invariant under $\mata$ or equivalently, the minimal
  polynomial of $\vq_1$ is of degree $m$.
\end{Lemma}

\begin{proof}
  The algorithm stops in line 8 if and only if $\mata\vq_j$ is a
  linear combination of the previously computed basis vectors, such
  that $\vw_j=0$ in line 5 at the end of the loop. This happens
  exactly when the Krylov space is invariant.
\end{proof}

\begin{Lemma}{arnoldi-symmetric}
  If the matrix $\mata$ is symmetric, the vectors of the Arnoldi
  procedure admit the three-term recurrence relation
  \begin{gather}
    \delta_{j+1}\vq_{j+1} = \mata \vq_j - \gamma_j \vq_j - \delta_j\vq_{j-1}
  \end{gather}
  with $\vq_0 = 0$, $\gamma_1 = 0$ and
  \begin{gather}
    \gamma_j =  \scal(\mata\vq_j,\vq_j), \qquad \delta_j = \scal(\mata\vq_j,\vq_{j-1}),
    \qquad j=1,\dots
  \end{gather}
  In particular, explicit orthogonalization is only necessary with respect to
  the previous two basis vectors.
\end{Lemma}

\begin{remark}
\end{remark}

\begin{proof}
  The Arnoldi process corresponds to a truncated reduction to
  Hessenberg form according to \slideref{Theorem}{arnoldi-projection}
  and \slideref{Theorem}{Hessenberg-Krylov-2}. Hence, like in the
  previous chapter, the Hessenberg matrix $\matH_m$ becomes symmetric
  and tridiagonal if $\mata$ is symmetric. Indeed, this implies that
  the coefficients $h_{ij}$ computed in line 4 of
  \slideref{Algorithm}{arnoldi-1} are evaluating to zero if
  $\abs{i-j}>1$. Therefore, the inner products
  $\scal(\mata\vq_j,\vq_i)$ are zero for $i\le j-2$. This implies the
  three-term recurrence for $j>1$, where we explicitly copied the
  definitions of $h_{jj}$ and $h_{j,j-1}$.

  The initial vector $\vq_1$ remains as in the original algorithm and
  for $j=1$, we have to provide initial values according to the
  original Arnoldi method, where the third term is missing
\end{proof}

\begin{Algorithm*}{lanczos}{Lanczos}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vq_1\in\R^n, \norm{\vq_1}_2 = 1$, $\vq_0=0$, $\delta_1=0$.
    \For{$j=1,\dots,m$}
    \State $\vw_j \gets \mata\vq_j - \delta_j \vq_{j-1}$
    \State $\gamma_{j} \gets \scal(\vw_j,\vq_j)$
    \State $\vw_j \gets \vw_j - \gamma_j\vq_i$
    \State $\delta_{j+1} \gets \norm{\vw_j}_2$
    \If{$\delta_{j+1}=0$} \textbf{stop}\EndIf
    \State $\vq_{j+1} = \nicefrac{\vw_j}{\delta_{j+1}}$
    \EndFor
  \end{algorithmic}  
\end{Algorithm*}

\begin{Remark}{Lanczos-delta}
  In this algorithm, $\delta_j$ is not computed as in
  \slideref{Lemma}{arnoldi-symmetric}, but as the norm of the
  intermediate vector $\vw_j$. Here, we used the symmetry of the
  tridiagonal matrix, which allows us to save an additional inner
  product.
\end{Remark}

\begin{remark}
  The main difference between Arnoldi and Lanczos is the fact that
  every step of the Lanczos process only requires the previous two
  vectors, while Arnoldi requires storing the whole history.

  This implies, that Lanczos itself is much closer to the concept of
  an iterative method than Arnoldi, weren't it for the computation of
  $\vy_m$.

  We will now develop a new algorithm based on the observation that
  also the corrections for $\vx^{(m)}$ can be computed incrementally.
\end{remark}

\begin{Problem}{Arnoldi-invariant}
  Assume that after $m$ steps of the Arnoldi process with initial
  vector $\vb$, there holds $h_{m+1,m} = 0$.
  \begin{enumerate}
  \item How can~\eqref{eq:arnoldi-projection-2} be simplified in this
    case?
  \item What does this mean for the structure of the complete
    Hessenberg matrix in~\eqref{eq:krylov:1}?
  \item Show that in this case $\krylov_m(\mata,\vb)$ is an
    $\mata$-invariant subspace of $\R^n$.
  \item Assume additionally that $\mata$ is nonsingular. Show that the
    solution $\vx$ of $\mata\vx = \vb$ is in $\krylov_m(\mata,\vb)$.
  \end{enumerate}
\end{Problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The conjugate gradient method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  
\end{intro}

\begin{Theorem}{arnoldi-linear-system}
  Given an initial vector $\vx^{(0)}$ and initial residual
  $\vr^{(0)} = \vb - \mata \vx^{(0)}$. Let $\beta=\norm{\vr^{(0)}}$
  and choose $\vq_1 =\nicefrac{\vr^{(0)}}{\beta}$ in Arnoldi's
  method. Then, the Galerkin approximation
  \begin{align}
    \vx^{(m)} &\in \vx^{(0)} + \krylov_m(\mata,\vr^{(0)})\\
    \vb-\mata \vx^{(m)} &\perp \krylov_m(\mata,\vr^{(0)}),
  \end{align}
  is obtained in two steps. Solve in $\R^m$
  \begin{gather}
    \matH_m\vy = \beta\ve_1,
  \end{gather}
  and let
  \begin{gather}
    \vx^{(m)} = \vx^{(0)} + \matq_m \vy.
  \end{gather}
\end{Theorem}

\begin{proof}
  Let $\vv$ be the update vector such that
  $\vx^{(m)} = \vx^{(0)} + \vv$ and note that this implies
  $\vv=\matq_m\vy$.

  The orthogonality condition implies for all basis vectors $\vq_i\in\krylov_m(\mata,\vr^{(0)})$
  \begin{gather}
    0 = \scal(\vb-\mata \vx^{(m)},\vq_i) = \scal(\vr^{(0)} - \mata\vv,\vq_i).
  \end{gather}
  Hence,
  \begin{gather}
    \vq_i^* \mata\vv = \vq_i^*\vr^{(0)},\qquad i=1,\dots,m,
  \end{gather}
  or in matrix notation
  \begin{gather}
    \matq_m^*\mata\vv = \matq_m^* \vr^{(0)}.
  \end{gather}
  Since $\matq_m$ is orthogonal and $\vr^{(0)} = \beta\vq_1$, the
  right hand side is $\beta\ve_1 \in \R^m$. Entering the representation of
  $\vv$ by $\vy$, we obtain
  \begin{gather}
    \matq^*\mata\matq\vy = \matH_m \vy = \beta\ve_1.
  \end{gather}
\end{proof}

\begin{Theorem}{arnoldi-linear-residual}
  Let $\vx^{(m)}$ be the Galerkin solution in
  \slideref{Theorem}{arnoldi-linear-system}. Then, there holds
  \begin{gather}
    \vb-\mata\vx^{(m)} = -h_{m+1,m} \ve_m^T \vy_m \vq_{m+1},
  \end{gather}
  and therefore
  \begin{gather}
    \norm{\vb-\mata\vx^{(m)}}_2 = h_{m+1,m} \abs{\ve_m^T \vy_m}.
  \end{gather}  
\end{Theorem}

\begin{proof}
  See also~\cite[Proposition 6.7]{Saad00} We use the definition of the
  residual of $\vx^{(m)}$ and the representation of
  \slideref{Theorem}{arnoldi-projection} to obtain
  \begin{align}
    \vr^{(m)} = b-\mata\vx^{(m)}
    &= b- \mata\left(\vx^{(0)} + \matq_m\vy_m\right)\\
    &=\vr^{(0)} - \mata\matq_m\vy_m\\
    &= \beta\vq_1 - \matq_m\matH_m\vy_m - \vw_m\ve_m^T\vy_m.
  \end{align}
  Since $\vy_m$ solves the projected system
  $\matH_m\vy_m = \beta\ve_1$, there holds
  \begin{gather}
    \beta\vq_1 - \matq_m\matH_m\vy_m = \beta\vq_1 - \beta \matq_m\ve_1 = 0.
  \end{gather}
  Furthermore, we rewrite
  \begin{gather}
    \vw_m\bigl(\ve_m^T\vy_m\bigr) = h_{m+1,m} \bigl(\ve_m^T\vy_m\bigr) \vq_{m+1}.
  \end{gather}
\end{proof}

\begin{remark}
  Following \slideref{Theorem}{projected-invertible} this algorithm
  works reliably only for $\mata$ symmetric, positive definite. This
  is the case we study here and we will improve the algorithm.

  In particular, we will derive an incremental version of the
  algorithm which starts with $\vx^{(0)}$ and produces the next
  iterate by increasing $m$ instead of starting a new projection
  step. Hence, we will change our view from discussing the step
  $k\to k+m$ to discussing the iteration $0\to 1\to m\to m+1$.

  To this end, we first rephrase the previous theorem in terms of a
  tridiagonal matrix.
\end{remark}

\begin{Lemma*}{lanczos-linear}{Lanczos solver}
  The Ritz-Galerkin approximation
  \begin{align}
    \vx^{(m)} &\in \vx^{(0)} + \krylov_m(\mata,\vr^{(0)})\\
    \vb-\mata \vx^{(m)} &\perp \krylov_m(\mata,\vr^{(0)}),
  \end{align}
  is obtained as $\vx^{(m)} = \vx^{(0)} + \matq_m \vy_m$, where $\matq_m$ is the basis obtained from the Lanczos process and $\vy_m$ solves
  \begin{gather}
    \matH_m\vy_m = \beta\ve_1,
    \qquad \matH_m =
    \begin{pmatrix}
      \gamma_1 & \delta_2\\
      \delta_2 & \gamma_2 & \ddots\\
      &\ddots&\ddots&\delta_m\\
      &&\delta_{m}&\gamma_m
    \end{pmatrix}.
  \end{gather}
  For the residual, there holds
  \begin{gather}
    \vr^{(m)} = \vb-\mata\vx^{(m)} = -\delta_{m+1} \ve_m^T \vy_m \vq_{m+1}.
  \end{gather}
\end{Lemma*}

\begin{proof}
  These are the statements of
  \slideref{Theorem}{arnoldi-linear-system} and
  \slideref{Theorem}{arnoldi-linear-residual}. Since the Lanczos
  process is just a specialization of the Arnoldi process, they still
  hold.
\end{proof}

\begin{intro}
  While the basis of $\krylov_m(\mata,\vr^{(0)})$ can be computed
  incrementally, only using the previous two vectors, the update
  \begin{gather}
    \vx^{(m)} = \vx^{(0)} + \matq_m \vy_m
  \end{gather}
  requires storing the whole basis $\matq_m$. This is a problem, if
  many iteration steps are required. Hence, in the following steps, we
  will a method, which computes the update using only a few
  vectors. We will see that our first attempt fails, as the method is
  potentially unstable, but we succeed in a second step.
\end{intro}

\begin{Lemma}{lanczos-incremental}
  Given the vector $\vx^{(m)}$ in the Lanczos solver, the vector
  $\vx^{(m+1)}$ can be computed incrementally by an update of the form
  \begin{gather}
    \vx^{(m+1)} = \vx^{(m)} + \zeta_{m+1}\vg^{(m+1)},
  \end{gather}
  where the vector $\vg^{(m)}$ itself is computed only using the
  vectors $\vg^{(m-1)}$ and $\vq^{(m)}$.
\end{Lemma}

\begin{proof}
  See also \cite[Section 6.7.1]{Saad00}. First, we compute the LU factorization of $\matH_m$ as
  \begin{gather}
    \matH_m =
    \begin{pmatrix}
      1\\\mu_2&1\\
      &\ddots&\ddots\\
      &&\mu_m&1
    \end{pmatrix}
    \times
    \begin{pmatrix}
      \eta_1&\delta_2\\
      &\ddots&\ddots\\
      &&\eta_{m-1}&\delta_m\\
      &&&\eta_m
    \end{pmatrix},
  \end{gather}
  where
  \begin{gather}
    \mu_k = \frac{\delta_k}{\eta_{k-1}},
    \qquad
    \eta_k = \gamma_k - \mu_k \delta_k.
  \end{gather}
  We note that LU decomposition works from top to bottom and left to
  right, such that already processed rows do not change when adding
  more steps to the algorithm.

  Now we introduce matrices and vectors
  \begin{gather}
    \matg_m = \matq_m \matu_m^{-1} \in \R^{n\times m},
    \qquad \vz_m = \matl_m^{-1} \beta \ve_1\in \R^m,
  \end{gather}
  such that
  \begin{align}
    \vx^{(m)}
    &= \vx^{(0)} + \matq_m \matH_m^{-1}\beta \ve_1\\
    &= \vx^{(0)} + \matq_m \matu_m^{-1} \matl_m^{-1} \beta \ve_1\\
    &= \vx^{(0)} + \matg_m \vz_m
  \end{align}
  With $\zeta_k$ following the recursion
  $\zeta_k = - \mu_k \zeta_{k-1}$, we realize
  \begin{gather}
    \vz_{m+1} =
    \begin{pmatrix}
      \zeta_1\\\vdots\\\zeta_{m+1}
    \end{pmatrix}
    =
    \begin{pmatrix}
      \vz_{m}\\\zeta_{m+1}
    \end{pmatrix}.
  \end{gather}
  Let $\vg_k$ be the column vectors of $\matg_m$. Writing the
  definition of $\matg_m$ in the form $\matg_m \matu_m = \matq_m$, we
  obtain the relation
  \begin{gather}
    \label{eq:nla:krylov:lanczos-p}
    \vg_k = \frac1{\eta_k}\bigl(\vq_k - \delta_k \vg_{k-1}\bigr),
    \qquad\vg_1 = \frac1{\eta_1} \vq_1.
  \end{gather}
  Thus,
  \begin{gather}
    \matg_{m+1} =
    \begin{pmatrix}
      \vg_1,\dots,\vg_{m+1}
    \end{pmatrix}
    =
    \begin{pmatrix}
      \matg_{m},\vg_{m+1}
    \end{pmatrix}.
  \end{gather}
  There holds
  \begin{align}
    \vx^{(m+1)}
    &= \vx^{(0)} + \matg_{m+1}\vz_{m+1}\\
    &= \vx^{(0)} + \sum_{k=1}^{m+1} \zeta_k \vg_k,\\
    &= \vx^{(m)} + \zeta_{m+1} \vg_{m+1}.
  \end{align}
\end{proof}

\begin{remark}
  Thus, we have proven a simple update formula for the iterates
  $\vx^{(m)}$ based on an additional vector $\vg_m$, which is again
  obtained by an update formula.

  We have used an LU decomposition without pivoting though in the
  derivation of this algorithm, which might suffer from numerical
  instability. Therefore, we will now analyze the main properties of
  the sequences $\vq_m$, $\vg_m$ and $\vr_m$ and then write a new
  algorithm to obtain these sequences.
\end{remark}

\begin{Lemma}{lanczos-orthogonality}
  For the sequences of vectors $\vx_m$, $\vr_m = \vb-\mata\vx_{m}$ and $\vg_m$
  constructed in the preceding lemma, there holds
  \begin{align}
    \vg_m &\in \krylov_{m}(\mata,\vr_0)\\
    \vx_m &\in \vx_0 + \krylov_{m}(\mata,\vr_0)\\
    \vr_m &\in \krylov_{m+1}(\mata,\vr_0)
  \end{align}
  Additionally, they have the following orthogonality properties:
  \begin{xalignat}2
    \scal(\vr_i,\vr_k) &= 0 & i&< k \le m\\
    \scal(\mata\vg_i,\vg_k) &= 0 & i&< k \le m.
  \end{xalignat}
\end{Lemma}

\begin{proof}
  By~\eqref{eq:nla:krylov:lanczos-p},
  $\vg_m\in\krylov_{m+1}(\mata,\vr_0)$, hence the inclusion of
  $\vx_m$.  By construction, the residual
  $\vr_m \in \krylov_{m+1}(\mata,\vr_0)$ and orthogonal to
  $\krylov_{m}(\mata,\vr_0)$. Hence, it is coaligned with $\vq_{m+1}$
  and orthogonal to all previous residuals.

  The second orthogonality relation is equivalent to the fact that
  $\matg_m^T\mata\matg_m$ is diagonal.
  \begin{align}
    \matg_m^T\mata\matg_m
    &= \matu_m^{-T}\matq_m^T\mata\matq_m\matu_m^{-1}\\
    &= \matu_m^{-T}\matH_m\matu_m^{-1}\\
    &= \matu_m^{-T}\matl_m.
  \end{align}
  The first term is symmetric.  The last is the product of two lower
  triangular matrices and thus lower triangular. Thus, the matrix must
  be diagonal.
\end{proof}

\begin{Algorithm*}{cg}{Conjugate Gradient Method}
  \begin{algorithmic}[1]
    \State $\vr_0 = b-\mata \vx_0$
    \State $\vp_0 = \vr_0$
    \For{$m=0,1,\dots$}
    \State $\alpha_m = \frac{\scal(\vr_m,\vr_m)}{\scal(\mata\vp_m,\vp_m)}$
    \State $\vx_{m+1} = \vx_m + \alpha_m \vp_m$
    \State $\vr_{m+1} = \vr_m - \alpha_m \mata \vp_m$
    \State $\beta_m =\frac{\scal(\vr_{m+1},\vr_{m+1})}{\scal(\vr_m,\vr_m)}$
    \State $\vp_{m+1} = r_{m+1} + \beta_m \vp_m$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{cg-orthogonality}
  Assume that $\vr_m \neq 0$.
  For $k\le m$ the vectors $\vx_k$, $\vr_k = \vb-\mata\vx^{(k)}$ and $\vp_k$
  of the conjugate gradient method admit
  \begin{align}
    \vp_k &\in \krylov_{k+1}(\mata,\vr_0),\\
    \vr_k &\in \krylov_{k+1}(\mata,\vr_0).
  \end{align}
  Additionally, they have the following orthogonality properties:
  \begin{xalignat}2
    \scal(\vr_i,\vr_k) &= 0 & i&< k\le m,\\
    \scal(\mata\vp_i,\vp_k) &= 0 & i&< k \le m,\\
    \scal(\vp_i,\vr_k) &= 0 & i&< k\le m.
  \end{xalignat}
  Furthermore,
  $\scal(\vr_k,\vp_k) = \scal(\vr_k,\vr_k)$.
\end{Lemma}

\begin{proof}
  We show the result by induction over $m$. It is true for $m=0$. Now
  assume that it holds for $m$ and we conclude for $m+1$. From line 6,
  we obtain $\vr_{m+1} \in \krylov_{m+1}(\mata,\vr_0)$, since
  $\vp_m\in \krylov_{m+1}(\mata,\vr_0)$. Then, line 8 yields
  immediately $\vp_{m+1} \in \krylov_{m+1}(\mata,\vr_0)$.


  From line 6 the definition of $\alpha_m$ and the equality of the
  inner products in the statement of the lemma, we have
  \begin{align}
    \scal(\vr_{m+1},\vp_m)
    &= \scal(\vr_m,\vp_m) - \frac{\scal(\vr_m,\vr_m)}{\scal(\mata\vp_m,\vp_m)} \scal(\mata\vp_m,\vp_m)\\
    &= \scal(\vr_m,\vp_m) - \frac{\scal(\vr_m,\vp_m)}{\scal(\mata\vp_m,\vp_m)} \scal(\mata\vp_m,\vp_m) = 0.
  \end{align}
  For any $k<m$, there holds
  \begin{gather}
    \scal(\vr_{m+1},\vp_k) = \scal(\vr_m,\vp_k) - \alpha_k \scal(\mata\vp_m,\vp_k) = 0-0 = 0.
  \end{gather}
  Hence, we have proven
  \begin{gather}
    \scal(\vr_{m+1},\vp_k) = 0 \qquad k < m.
  \end{gather}
  Now, we observe
  \begin{gather}
    \label{eq:krylov-cg-1}
    \scal(\vr_{m+1},\vp_{m+1}) = \scal(\vr_{m+1},\vr_{m+1} + \beta_k\vp_k) = \scal(\vr_{m+1},\vr_{m+1}).
  \end{gather}

  From line 8, we obtain for $k\le m$
  \begin{gather}
    \scal(\vp_{m+1},\mata\vp_k)
    = \scal(\vr_{m+1},\mata\vp_k) + \beta_m \scal(\vp_m,\mata\vp_k)
  \end{gather}
  For the first term, we use first line 6 and then line 8 to obtain
  \begin{align}
    \scal(\vr_{m+1},\mata\vp_k)
    &= \tfrac1{\alpha_k}\scal(\vr_{m+1},\vr_{k}-\vr_{k+1}) + \beta_m \scal(\vp_m,\mata\vp_k)\\
    &= \tfrac1{\alpha_k}\scal(\vr_{m+1}, \vp_k-\beta_{k-1}\vp_{k-1} - \vp_{k+1} + \beta_k\vp_k)\\
    &= -\tfrac1{\alpha_k}\scal(\vr_{m+1},\vp_{k+1}).
  \end{align}
  Hence, for $k<m$ we immediately have
  \begin{gather}
    \scal(\vp_{m+1},\mata\vp_k) = 0.
  \end{gather}
  For $k=m$, we obtain
  \begin{align}
    \scal(\vp_{m+1},\mata\vp_m)
    &= -\tfrac1{\alpha_k}\scal(\vr_{m+1},\vp_{k+1}) + \beta_m \scal(\vp_m,\mata\vp_k)\\
    &= -\frac{\scal(\mata\vp_m,\vp_m)}{\scal(\vr_m,\vr_m)} \scal(\vr_{m+1},\vp_{k+1})
      + \frac{\scal(\vr_{m+1},\vr_{m+1})}{\vr_{m},\vr_{m}}\scal(\vp_m,\mata\vp_k)\\
    &= 0,
  \end{align}
  again by the equality of inner products in~\eqref{eq:krylov-cg-1}.
  Hence, we have proven
  \begin{gather}
    \scal(\mata\vp_{m+1}, \vp_k) = 0 \qquad k\le m.
  \end{gather}
  Finally, there holds for $k\le m$
  \begin{gather}
    \scal(\vr_{m+1},\vr_k) = \scal(\vr_{m+1}, \vp_k - \beta_{k-1}\vp_{k-1}) = 0.
  \end{gather}
\end{proof}

\begin{Corollary}{cg-Lanczos}
  The sequences $\vx_m$ and $\vr_m$ produced by the conjugate gradient
  method coincide with those of the Lanczos solver (in exact
  arithmetic). The vectors $\vp_m$ of the cg method coincide up to
  scalar factors with the vectors $\vg_{m+1}$ in
  \slideref{Lemma}{lanczos-incremental}.
\end{Corollary}

\begin{Remark}{cg-naming}
  The conjugate gradient (cg) method closely resembles the steepest
  descent method as a line search method.
  The steepest descent method follows the negative gradient of the
  energy functional (\slideref{Lemma}{steepest-descent})
  \begin{gather}
    F(\vy) = \norm{\vy-\vx}_{\mata}^2.
  \end{gather}
  The difference is, that the search directions are
  orthogonalized with respect to the $\mata$-inner product.
  An earlier term for $\mata$-orthogonal was $\mata$-conjugate, hence
  the name \define{Conjugate Gradient Method}.
\end{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convergence analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  Here, we will develop a sequence of error estimates for the
  conjugate gradient method based on its optimality as a projection
  method stated in \slideref{Theorem}{projection-orthogonal-optimal}. These
\end{intro}

\begin{Theorem}{cg-optimality}
  For the error $\ve^{(m)} = \vx^{(m)}-\vx$ after $k$ steps of the cg
  method there holds
  \begin{align}
    \norm{\ve^{(m)}}_A
    & = \min_{p\in \P_{m-1}}
      \norm{\ve^{(0)} + \mata p(\mata) \ve^{(0)}}_A\\
    &= \min_{\substack{p\in\P_m\\p(0) = 1}} \norm{p(\mata)\ve^{(0)}}_A.
  \end{align}
\end{Theorem}

\begin{proof}
  There holds $\vx^{(m)} = \vx^{(0)} + \vw$ with
  $\vw \in \krylov_m(\mata,\vr^{(0)})$. Then, by
  \slideref{Lemma}{krylov-polynomial}, there is $p\in\P_{m-1}$ such
  that $\vw = -p(\mata) \vr^{(0)}$. Thus
  \begin{align}
    \vx^{(m)} - \vx
    &= \vx^{(0)} - \vx - p(\mata)\vr^{(0)}\\
    &= \vx^{(0)} - \vx - p(\mata) (\vb-\mata\vx^{(0)})\\
    &= \vx^{(0)} - \vx - p(\mata) (\mata\vx-\mata\vx^{(0)})\\
    &= \vx^{(0)} - \vx - \mata p(\mata) (\vx^{(0)}-\vx).
  \end{align}
  On the other hand, we have the optimality of projection methods in
  \slideref{Theorem}{projection-orthogonal-optimal}, which states
  \begin{align}
    \norm{\ve^{(m)} -\vx}_A
    &= \min_{\vw\in \krylov_m(\mata,\vr^{(0)})} \norm{\vx^{(0)}+\vw-\vx}_A\\
    &= \min_{\vw\in \krylov_m(\mata,\vr^{(0)})} \norm{\ve^{(0)}+\vw}_A.
  \end{align}
  Thus, we obtain the first expression. The second follows by noticing
  that the set of polynomials $q(x) = 1+xp(x)$ with $p\in\P_{k-1}$ is
  just the set of polynomials in $\P_m$ with coefficient $a_0 = 1$.
\end{proof}

\begin{Corollary}{cg-vs-descent}
  The error after $m$ steps of the conjugate gradient method is never
  greater than the error after equally many steps of the steepest
  descent method.
\end{Corollary}

\begin{proof}
  The error of the steepest descent method is
  \begin{gather}
    \ve^{(m)} = \prod_{i=1}^{m} (\id-\alpha_i\mata) \ve^{(0)}.
  \end{gather}
  This can be rewritten as
  \begin{gather}
    \ve^{(m)} = p_m(\mata) \ve^{(0)},
  \end{gather}
  where $p_m\in\P_m$ is the polynomial of degree $m$ defined by
  \begin{gather}
    p_m(x) = \prod_{i=1}^{m} (1-\alpha_ix).
  \end{gather}
  There holds $p_m(0)=1$ and thus this polynomial is in the set over
  which the conjugate gradient method minimizes.
\end{proof}

\begin{Corollary}{cg-optimality-spectrum}
  Let $\sigma(\mata) = \{\lambda_1,\dots,\lambda_n\} \subset \R^+$ be
  the spectrum of $\mata$. Then, the error after $m$ steps of the
  conjugate gradient method follows the estimate
  \begin{gather}
    \norm{\ve^{(m)}}_A \le \rho_m \norm{\ve^{(0)}}_A,
  \end{gather}
  where
  \begin{gather}
    \rho_m = \min_{\substack{p\in\P_m\\p(0) = 1}} \max_{\lambda\in\sigma(\mata)} \abs{p(\lambda)}.
  \end{gather}
\end{Corollary}

\begin{proof}
  First, since we apply the cg iteration to s.p.d. matrices, there is
  an orthogonal basis $\{\vq_1,\dots,\vq_n\}$ of eigenvectors of
  $\mata$ with real, positive eigenvalues $\lambda_i$. Thus, we can
  write with some coefficients $\alpha_i$:
  \begin{gather}
    \ve^{(0)} = \sum_{i=1}^n \alpha_i \vq_i,
    \qquad
    \ve^{(m)} = \sum_{i=1}^n \alpha_i p(\lambda_i) \vq_i.    
  \end{gather}
  Hence, we can estimate
  \begin{align}
    \norm{\ve^{(m)}}_A^2
    &= \sum_{i=1}^n \lambda_i\alpha_i^2 p(\lambda_i)^2 \norm{\vq_i}_A^2\\
    & \le \max_i p(\lambda_i)^2 \sum_{i=1}^n \lambda_i\alpha_i^2\norm{\vq_i}_A^2\\
    &= \max_i p(\lambda_i)^2 \norm{\ve^{(0)}}_A^2.
  \end{align}
  The statement now follows from \slideref{Theorem}{cg-optimality}.
\end{proof}

\begin{remark}
  It is important to note that the preceding corollary is close to
  optimal. In particular, it can exploit any information which is
  known about the spectrum. In particular, the following statements are true:
  \begin{enumerate}
  \item If the matrix $\mata$ only has $m$ distinct eigenvalues, then
    the cg iteration converges to the solution in $m$ steps.
  \item If the eigenvalues are clustered around $m$ values, then the
    gain in the first $m$ steps will be large.
  \item If the spectrum consists of a continuum plus a single outlier,
    the method will perform almost as if the outlier did not exist.
  \end{enumerate}
  The following corollary now deals with the case that the spectrum
  has no particular structure, but fills a continuous interval
  somewhat evenly.
\end{remark}

\begin{Corollary}{cg-condition-number}
  The error after $m$ steps of the cg method admits the estimate
  \begin{gather}
    \norm{e^{(m)}}_{\mata} \le 2 \left(
      \frac{\sqrt{\cond_2(\mata)}-1}{\sqrt{\cond_2(\mata)}+1}\right)^m
    \norm{e^{(0)}}_{\mata}.
  \end{gather}
\end{Corollary}

\begin{proof}
  We make no further assumtions on the spectrum of $\mata$ except that
  all eigenvalues are in the interval
  $[\lambda_{\min},\lambda_{\max}]$. Thus, we estimate the factors $\rho_m$ in \slideref{Corollary}{cg-optimality-spectrum} by
  \begin{gather}
    \rho_m \le \min_{\substack{p\in\P_m\\p(0) = 1}}
    \max_{\lambda\in[\lambda_{\min},\lambda_{\max}]} \abs{p(\lambda)}.
  \end{gather}
  
  We use the optimality property of Chebyshev polynomials stated in
  \slideref{Corollary}{chebyshev-minimal-2} and note that the
  optimality stated there is exactly what we need. Thus,
  $p = \widehat\pchebyshev_m(\lambda)$ solves the minimization
  problem and there holds
  \begin{align}
    \rho_m
    &\le \frac1{\pchebyshev_k\left(
      \frac{\lambda_{\max}+\lambda_{\min}}{\lambda_{\max}-\lambda_{\min}}
      \right)}\\
    & \le 2 \left(\frac{\sqrt{\lambda_{\max}}-\sqrt{\lambda_{\min}}}{\sqrt{\lambda_{\max}}+\sqrt{\lambda_{\min}}}
      \right)^m
  \end{align}
  Dividing both sides of the fraction by $\sqrt{\lambda_{\min}}$ yields the result.
\end{proof}

\begin{remark}
  Note that the estimate in \slideref{Corollary}{cg-condition-number}
  is not a classical contraction estimate which allows us to deduce
  the error reduction from step $m$ to $m+1$. Indeed, the factor 2
  before the quotient renders this exercise impossible, and the
  estimate for the steepest descent method is our rescue.

  Instead, the estimate shows that with growing dimension of the
  Krylov space, eventually the factor 2 is compensated by the first
  $m$ powers of the quotient and then the additional powers guarantee
  faster convergence.
\end{remark}

% \begin{Problem}{cg-implementation}
%   Implement the conjugate gradient method. Apply it to the 
% \end{Problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The generalized minimal residual method (GMRES)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  Now, we turn our attention again to nonsymmetric matrices. Here, the
  full Arnoldi process is necessary, as the Lanczos process is only
  applicable to symmetric matrices. Hence, for $m$ steps of the
  method, we have to store the whole Arnoldi basis of $m$
  vectors. Furthermore, in each step, we must orthogonalize explicitly
  with respect to every previous vector.

  Then, we also will apply the oblique projection method with
  $L=\mata K$, as we have a guarantee here that the projected problems
  are solvable. But the projected matrix is not $\matH_m$ as for the
  orthogonal projection method, since the test space is
  different. Hence, we construct the projected problem analog to the
  normal equations of least-squares approximation.
\end{intro}

\begin{todo}
  Put this theorem into an appendix?
\end{todo}

\begin{Theorem}{normal-equations}
  Let $\mata\in\R^{m\times n}$ with $m\ge n$ and $\vb\in\R^m$. Then, a vector $\vx$ is the solution to the minimization problem
  \begin{gather}
    \norm{\vb-\mata\vx}_2 \stackrel{!}{=} \min,
  \end{gather}
  if an only if $\vx$ solves the \define{normal equations}
  \begin{gather}
    \mata^T\mata\vx = \mata^T \vb.
  \end{gather}
\end{Theorem}

\begin{Lemma}{oblique-normal}
  Let $\mata\in\Rnn$ be invertible. Let $K$ be a subspace of $\Rnn$. Then, the oblique projection
  \begin{gather}
    \vv\in K, \qquad \vb-\mata\vv \perp \mata K,
  \end{gather}
  and the orthogonal projection
  \begin{gather}
    \vw\in K, \qquad \mata^T \vb-\mata^T\mata\vw \perp K,
  \end{gather}
  are identical.
\end{Lemma}

\begin{proof}
  Homework.
\end{proof}

\begin{Lemma}{gmres-projected}
  The projected normal equations of the oblique projection method with Arnoldi basis $\matq_m$ read
  \begin{gather}
    {\overline\matH}^T_m\overline\matH_m \vy = {\overline\matH}^T_m \beta \ve_1.
  \end{gather}
  These are equivalent to finding $\vy\in\R^m$ such that
  \begin{gather}
    \norm{\beta\ve_1 - \overline\matH_m \vy_m}_2 \stackrel{!}{=} \min.
  \end{gather}
\end{Lemma}

\begin{proof}
  According to \slideref{Theorem}{arnoldi-projection}, we obtain
  \begin{gather}
    \matq_m^T\mata^T\mata\matq_m
    = \overline\matH_m^T \matq_{m+1}^T\matq_{m+1}\overline\matH_m
    = \overline\matH_m^T\overline\matH_m.
  \end{gather}
  Thus, $\overline\matH_m^T\overline\matH_m$ is the Galerkin
  projection of $\mata^T\mata$ to the subspace generated by
  $\matq_m$.

  Furthermore,
  $\vr^{(0)} = \vb-\mata\vx^{(0)} = \beta\matq_{m+1}\ve_1$, such that
  \begin{gather}
    \matq_m^T\mata^T\vr^{(0)} = \beta\overline\matH_m^T \ve_1.
  \end{gather}
  Thus, the linear system
  \begin{gather}
    \overline\matH_m^T\overline\matH_m \vy_m = \beta\overline\matH_m^T \ve_1
  \end{gather}
  is the (orthogonal) Galerkin projection of the normal equations to
  the search space spanned by $\matq_m$.

  Equivalence with the minimization problem follows from the theorem
  on the normal equations.
\end{proof}

\begin{todo}
  Write in incremental form? Add pseudocode version in incremental form?
\end{todo}

\begin{Algorithm*}{gmres}{Generalized Minimal Residual}
  Given $\vx^{(0)}\in\R^n$, the ``iterate'' $\vx^{(m)}$ of the
  \define{GMRES} method is computed by the following steps.
  \begin{enumerate}
  \item Compute the Arnoldi basis $\matq_{m+1}$ of
    $\krylov_{m+1}(\mata,\vr^{(0)})$ and the matrix $\overline\matH_m\in\R^{m+1\times m}$.
  \item Solve the least squares problem
    \begin{gather}
      \norm{\beta\ve_1 - \overline\matH_m \vy_m}_2 \stackrel{!}{=} \min
    \end{gather}
    in $\R^m$ with $\beta = \norm{\vr^{(0)}}_2$.
  \item Let
    \begin{gather}
      \vx^{(m)} = \vx^{(0)} + \matq_{m}\vy_m
    \end{gather}
  \end{enumerate}
\end{Algorithm*}

\begin{Theorem}{gmres-projection}
  The GMRES method computes the update of the $m$-dimensional oblique
  projection step. It minimizes the residual over the search space
  $\vx^{(0)} +\krylov_m(\mata,\vr^{(0)})$ and there holds
  \begin{gather}
    \label{eq:gmres-norm}
    \norm{\vr^{(m)}}_2 = \norm{\vb-\mata\vx^{(m)}}_2 =
    \norm{\beta\ve_1 - \overline\matH \vy_m}_2.
  \end{gather}
\end{Theorem}

\begin{proof}
  We use the equivalence between minimizing the residual and the
  solution to the normal equations, namely
  \slideref{Lemma}{oblique-normal} and
  \slideref{Theorem}{normal-equations}.  Furthermore, $\mata^T\mata$
  is symmetric, positive definite, the latter assuming $\mata$ is
  invertible. Hence, we can apply the orthogonal projection method.

  Following \slideref{Theorem}{arnoldi-linear-system}, we can compute
  $\vx^{(m)} = \vx^{(0)} + \matq_m\vy$, where $\vy$ solves the
  projected system. According to \slideref{Lemma}{gmres-projected},
  this vector solves the stated minimization problem.
  
  By the minimization property of the orthogonal projection, the error
  after the projection step is minimal in the $\mata^T\mata$ norm,
  which is equal to the Euclidean norm of the residual.

  Finally, we have
  \begin{align}
    \vr^{(m)}
    &= \vb-\mata\vx^{(0)} - \mata\matq_m\vy_m\\
    &=\beta\matq_{m+1}\ve_1 - \matq_{m+1}\overline\matH_m\vy_m\\
    &= \matq_{m+1}\left(\beta\ve_1 - \overline\matH_m\vy_m\right).
  \end{align}
  Since $\matq_{m+1}$ is orthogonal, this implies that
  \begin{gather}
    \norm{\vr^{(m)}}_2 = \norm{\beta\ve_1 - \overline\matH_m\vy_m}_2
  \end{gather}
\end{proof}

\begin{remark}
  \slideref{Algorithm}{gmres} should not be implemented as it is
  presented, but rather in the fashion of an iterative method, where
  $m$ is increased step by step. To this end, we observe that the
  generation of the Arnoldi basis proceeds incrementally, adding new
  vectors, but not changing the previous ones.

  Therefore, $\overline\matH_{m+1}$ differs from $\overline\matH_{m}$
  only by adding an additional row and column. Then, the least-squares
  system is solved by computing the QR factorization of
  $\overline\matH_{m+1}$, which again amounts to updating the last
  column only.
\end{remark}

\begin{remark}
  Equation~\eqref{eq:gmres-norm} implies in particular, that the norm
  of the residual in the GMRES method can be computed without
  computing a ``big'' matrix-vector operation $\mata\vx^{(m)}$, but is
  indeed the misfit of the $m$-dimensional least-squares problem for
  $\vy_m$. This implies on the other hand, that even the solution
  $\vx^{(m)}$ should only be computed as soon as the resiudal is
  sufficiently small.
\end{remark}

\begin{Lemma}{gmres-breakdown}
  If the GMRES method breaks down at step $m$, then
  $\vr^{(m)}=0$. Thus, the GMRES method only encounters a
  \putindex{lucky breakdown}.
\end{Lemma}

\begin{proof}
  A breakdown of the method occurs if $h_{m+,m} = 0$, which in turn
  implies $\vr^{(m)} = 0$.
\end{proof}

\begin{Theorem}{gmres-optimality}
  For the residual $\vr^{(m)} = \vb-\mata\vx^{(m)}$ after $m$  steps of
  the GMRES method, there holds
  \begin{gather}
    \norm{\vr^{(m)}}_2 = \min_{\substack{p\in\P_m\\p(0) = 1}}
    \norm{p(\mata)\vr^{(0)}}_2.
  \end{gather}
  If the matrix $\mata$ is diagonalizable such that $\mata=\matv\matlambda\matv^{-1}$, then there holds
    \begin{gather}
    \norm{\vr^{(m)}}_2 \le \rho_m \cond_2(\matv) \norm{\vr^{(0)}}_2,
  \end{gather}
  where $\matv$ is the matrix of eigenvectors and
  \begin{gather}
    \rho_m = \min_{\substack{p\in\P_m\\p(0) = 1}} \max_{\lambda\in\sigma(\mata)} \abs{p(\lambda)}.
  \end{gather}
\end{Theorem}

\begin{proof}
  See \cite[Proposition 6.32]{Saad00}.
\end{proof}

\begin{Corollary}{gmres-estimate}
  Let $\mata$ be diagonalizable such that its spectrum is contained in
  an ellipse in the complex plane with center $c$, focal distance $d$
  and semimajor axis $a$. Then,
  \begin{gather}
     \rho_m \approx \left(
      \frac{a+\sqrt{a^2-d^2}}{c+\sqrt{c^2-d^2}}
      \right)^k.
  \end{gather}
\end{Corollary}

\begin{proof}
  See \cite[Corollary 6.33]{Saad00}.
\end{proof}

\begin{remark}
  This theorem is analogue to \slideref{Theorem}{cg-optimality} and
  \slideref{Corollary}{cg-optimality-spectrum} and the proofs are
  identical, based on \slideref{Lemma}{krylov-polynomial} and
  \slideref{Theorem}{projection-oblique-optimal}.

  Note that the step from minimizing over $p(\mata)$ to $p(\lambda)$
  requires that the matrix $\mata$ is diagonalizable and produces
  optimal results only if it is normal. While this condition is not
  strictly necessary, there are examples of arbitrarily bad
  convergence. Indeed, for every nonincreasing sequence of $n-1$
  numbers, you can find a matrix and an initial vector such that the
  norms of residuals of the GMRES method will follow that
  sequence~\cite{GreenbaumPtakStrakos96}.
\end{remark}

\begin{intro}
  According to \slideref{Theorem}{Hessenberg-Krylov-2}, the GMRES
  algorithm uses the Arnoldi process in order to compute a QR
  factorization of the Krylov matrix
  $\matk_m = (\vr_0,\mata\vr_0,\dots,\mata^{m-1}\vr_0)$. In
  \slideref{Algorithm}{arnoldi-1}, Gram-Schmidt orthogonalization is
  used to this effect. Hence, we expect numerical instability if many
  steps of the method are needed. This in particular, as
  $\mata^m\vr_0$ converges to a dominant eigenvector which will make
  the Krylov matrix ill-conditioned. We should thus investigate in a
  more stable way of computing the Arnoldi basis and the projected
  matrix.

  Our starting point is \slideref{Theorem}{Hessenberg-Krylov-2}, which
  states that we can compute the Arnoldi basis by a QR factorization
  of the Krylov matrix. Two steps are needed to develop a working
  algorithm. First, since we still want to grow the Krylov space
  dimension by dimension, we need an incremental version of the QR
  factorization. But indeed, the standard method described
  in~\ref{intro:ortho:1} already fulfills this requirement, we just
  have to rewrite it column by column. Second, we have to construct
  the Hessenberg matrix $\matH_m$ from this QR factorization.
\end{intro}

\begin{Lemma}{qr-Arnoldi-1}
  Consider the Krylov matrix
  \begin{gather}
    \matk_\infty = \bigl(\vv,\mata\vv,\mata^2\vv,\dots\bigr).
  \end{gather}
  Its QR factorization can be computed recursively from
  $\vq_1 = \nicefrac{\vv}{\norm{\vv}}$ for $j=2,\ldots$ by
  \begin{align}
    \label{eq:Arnoldi-Householder:1}
    \vv_j &= \matp_{j-1}\cdots \matp_1\mata\vq_{j-1}\\
    \label{eq:Arnoldi-Householder:2}
    \vq_j &= \matp_1\cdots \matp_j\ve_j.
  \end{align}
  Here, $\matp_j$ is the Householder reflection of the form
  \begin{gather}
    \label{eq:Arnoldi-Householder:3}
    \matp_j =
    \begin{pmatrix}
      \id_{j-1}\\& \tilde \matp_{j}
    \end{pmatrix},
  \end{gather}
  which eliminates $v_{j;j+1}$ to $v_{j;n}$.  The columns of the
  Hessenberg matrix $\overline\matH_m$ in the Arnoldi process can be computed
  as $\vh_{j-1} = \matp_j\vv_j$.
\end{Lemma}

\begin{proof}
  Already in the proof of \slideref{Theorem}{Hessenberg-Krylov-2}, we
  have argued that we may replace the column $\mata^{j}\vv$ by
  $\mata\vq_{j}$ to obtain an essentially equal QR factorization. Hence,
  we compute the QR factorization of the matrix
  \begin{gather}
    \tilde K_\infty = \bigl(\vq_1,\mata\vq_1,\mata\vq_2,\ldots\bigr),
  \end{gather}
  where $\vq_j$ is the column of the matrix $\matq$ computed in the
  previous step..  $\vv_j$ can be viewed as the $j$-th column of the
  matrix
  \begin{gather}
    \matk^{(j-1)} = \matp_{j-1}\cdots \matp_1 \tilde\matk_{\infty},
  \end{gather}
  after $j-1$ steps of the QR factorization. Then, the next step is computed by the
  Householder reflection $\matp_j$. Finally,~\eqref{eq:ortho:1} states
  \begin{gather}
    \matq_j = \matp_1\dots\matp_j.
  \end{gather}
  The $j$-th column of this matrix is $\vq_j$ as stated by
  this lemma.

  We note that this representation of the QR factorization is
  essentially a reordering of operations. While in the standard
  representation, we apply the Householder reflection in each step to
  all further columns of the matrix, here we go column by column and
  apply all previously computed reflections at once.

  It remains to prove the statement on the Hessenberg matrix
  $\matH$. The representation
  $\mata\matq_{m} = \matq_{m+1}\overline\matH_{m}$ yields
  \begin{gather}
    \overline\matH_{m} = \matq_{m+1}^T\mata\matq_{m}.
  \end{gather}
  Hence, for the $m$-th column of this matrix
  \begin{gather}
    \vh_m = \matq_{m+1}^T\mata\vq_{m} = \matp_{m+1}
    \bigl(\matp_{m}\matp_{m-1}\dots\matp_{1}\bigr)\mata\vq_{m}
    = \matp_{m+1} \vv_{m+1}.
  \end{gather}
  Since the matrix $\overline\matH_{m}$ is built column by column from
  left to right, this formula holds for every column.
  % The Householder reflections $\matp_{k+1}$ act as identity on the
  % span of the first $k$ unit vectors. Hence, $\matp_{k}\vh_j = \vh_j$ for $k\ge j+2$ and
  % \begin{gather}
  %   \matp_{j+2}\cdots\matp_{m+1}\vh_j = \vh_j = \matq_{j+1}^T\mata\vq_j.
  % \end{gather}
  % Since $\mata\vq_j\in\krylov_{j+1}(\vv,\mata)$ and
  % $\matq_{j+1}\matq_{j+1}^T$ is the identity on this space, we obtain
  % \begin{gather}
  %   \matq_{m+1}\vh_j = \mata\vq_j,\qquad j=1,\dots,m,
  % \end{gather}
  % which are the columns of the matrix equation
  % \begin{gather}
  %   \matq_{m+1}\overline\matH_m = \mata\matq_{m}.
  % \end{gather}
\end{proof}

\begin{Algorithm*}{arnoldi-householder}{Arnoldi with Householder}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv\in\R^n$
    \State $\vq_1 = \nicefrac{\vv}{\norm{\vv}}_2$
    \For{$j=2,\dots,m$}
    \State $\vv \gets \matp_{j-1}\cdots\matp_1\mata\vq_{j-1}$
    \State Choose $\matp_j$: $\matp_j^*\vv = (v_1,\dots,v_{j-1},\alpha,0,\dots,0)^T$.
    \Comment{Householder}
    \State $\vq_j \gets \matp_1^*\dots\matp_j^*\ve_j$
    \State $\vh_{j-1} \gets \matp_j^*\vv$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{remark}
  The GMRES method encounters a problem if convergence is slow: the
  effort for computing the last basis vector in $\matq_m$ is of order
  $mn$, the effort for solving the least-squares problem is
  $m^3$. Similarly, the storage requirement for $\matq_m$ is $mn$ and
  for $\overline \matH_m$, it is $m^2$. Thus, the algorithm is
  feasible for large sparse matrices only if $m\ll n$.

  Therefore, we introduce two variants of the GMRES algorithm:
  \begin{itemize}
  \item Restarted GMRES simply deletes the whole basis $\matq_m$ and the matrix $\overline\matH_m$ every $m$-th step starts fresh.
  \item Truncated GMRES orthogonalizes only with respect to the last
    $m$ basis vectors.
  \end{itemize}
\end{remark}

\begin{Algorithm*}{gmres-restart}{Restarted GMRES}
  \begin{enumerate}
  \item Choose a maximal dimension $m$ of the Krylov space
  \item Begin with an initial guess $\vx^{(0)}$.
  \item For $k>0$, given $\vx^{(k)}$, perform the GMRES method with
    basis size $m$ to obtain $\vx^{(k+m)}$.
  \item Check the stopping criterion as usual inside the GMRES method
  \end{enumerate}
\end{Algorithm*}

\begin{Algorithm*}{gmres-truncated}{Truncated GMRES}
  Run the GMRES method as usual, but in the Arnoldi process, only
  orthogonalize with respect to the last $k$ vectors for fixed $k$.
\end{Algorithm*}

\begin{remark}
  Note that both modifications destroy the minimization property of the method.
\end{remark}

\begin{Theorem}{gmres-pos-def}
  If $\mata$ is positive definite, then the GMRES method as well as
  the restarted GMRES method converge.
\end{Theorem}

\begin{proof}
  See \cite[Theorem 6.30]{Saad00}.
\end{proof}

\begin{remark}
  The main practical difference between the conjugate gradient and
  GMRES methods is the short recurrence. While running hundreds of
  steps of cg is not an issue, GMRES becomes infeasible beyond some 30
  steps due to the extensive storage and the explicit
  orthogonalization. There has been research into finding solutions
  for this problem, but having both optimization and short recurrence
  is impossible in general (in your homework you saw an example of
  nonsymmetric matrices where it is possible).
\end{remark}

\input{gmres}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bi-Lanczos and BiCG-stab}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  The problem of short recurrence for nonsymmetric matrices has been
  addressed by biorthogonalization methods. They go back to
  \slideref{Lemma}{projection-basis} in the non-orthogonal case and
  build two biorthogonal sequences.
\end{intro}

\begin{Algorithm*}{bi-lanczos}{Lanczos Biorthogonalization}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1,\vw_1\in\R^n, \scal(\vv_1,\vw_1) = 1$, $\vv_0=\vw_0=0$, $\beta_1=\delta_1=0$.
    \For{$j=1,\dots,m$}
    \State $\alpha_{j} \gets \scal(\mata \vv_j,\vw_j)$
    \State $\hat\vv_{j+1} \gets \mata\vv_j - \alpha_j\vv_j - \beta_j \vv_{j-1}$
    \State $\hat\vw_{j+1} \gets \mata^T\vw_j - \alpha_j\vw_j - \delta_j \vw_{j-1}$
    \State $\delta_{j+1} \gets \sqrt{\abs{\scal(\hat\vv_{j+1},\hat\vw_{j+1})}}$
    \If{$\delta_{j+1}=0$} \textbf{stop}\EndIf
    \State $\beta_{j+1} \gets \nicefrac{\scal(\hat\vv_{j+1},\hat\vw_{j+1})}{\delta_{j+1}}$
    \State $\vv_{j+1} = \nicefrac{\hat\vv_{j+1}}{\beta_{j+1}}$
    \State $\vw_{j+1} = \nicefrac{\vw_{j+1}}{\delta_{j+1}}$
    \EndFor
  \end{algorithmic}  
\end{Algorithm*}

\begin{Theorem}{bi-lanczos-projection}
  If the Lanczos biorthogonalization does not break down before step
  $m$, then, the sequences $\{\vv_k\}_{k=1,\dots,m}$ and
  $\{\vw_k\}_{k=1,\dots,m}$ are bases for $\krylov_m(\mata,\vv_1)$ and
  $\krylov_m(\mata^T,\vw_1)$, respectively. They form a biorthogonal system and there holds
  \begin{align}
    \mata\matv_m &= \matv_m \matt_m + \delta_{m+1}\vv_{m+1}\ve_m^T,\\
    \mata^T\matw_m &= \matw_m \matt_m^T + \beta_{m+1}\vw_{m+1}\ve_m^T,\\
    \matw_m^T \mata \matv_m &= \matt_m,
  \end{align}
  where
  \begin{gather}\small
    \matt_m =
    \begin{pmatrix}
      \alpha_1 & \beta_2\\
      \delta_2 & \alpha_2 & \beta_2\\
      &\ddots & \ddots & \ddots\\
      &&\delta_{m-1} & \alpha_{m-1}&\beta_{m-1}\\
      &&&\delta_m&\alpha_m
    \end{pmatrix}.
  \end{gather}
\end{Theorem}

\begin{remark}
  The norms of the vectors $\vv_k$ and $\vw_k$ generated by Lanczos
  biorthogonalization are not controlled and may grow or shrink in an
  undesired way. This situation can be improved, since upon closer
  inspection of lines 8 and 9, biorthogonality is maintained as long
  as the coefficients $\beta_j$ and $\delta_j$ are chosen such that
  \begin{gather}
    \beta_{j+1}\delta_{j+1} = \scal(\hat\vv_{j+1},\hat\vw_{j+1}).
  \end{gather}
  
  Nevertheless, there is no mechanism which prevents this inner
  product from being zero before the Krylov spaces become
  invariant. Thus, Lanczos biorthogonalization can suffer from unlucky
  breakdowns.
\end{remark}

\begin{remark}  
  All biorthogonalization methods have some disadvantages in common:
  they do not minimize the error or the residual in each step. There
  might also be near breakdowns, where a denominator is almost zero,
  which must be detected, since after such an incident, the future
  convergence might suffer.

  Often, the necessity to multiply with $\mata^T$ is considered a
  downside. Hence, people have investigated transpose-free variants.
  
  This has spurred the development of a whole bunch of algorithms,
  from BiCG to QMR to BiCG-stab (already involving 8 auxiliary vectors) to
  BiCG-stab($m$), which combines $m$ GMRES steps with an outer
  BiCG-stab algorithm.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Preconditioning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  If matrices are ill-conditioned, convergence of cg or
  GMRES will still be slow, even if considerable faster than steepest
  descent or minimal residual. No general purpose algorithms with a
  better dependence on the condition number have been found.

  Thus, the only way out lies in a positive answer to the question:
  given a linear system $\mata\vx = \vb$, can we modify the system
  into one with better convergence properties?
\end{intro}

\begin{Definition}{preconditioned-system}
  We say that a linear system $\mata\vx=\vb$ is preconditioned from
  the left by the matrix $\matb^{-1}$, if we solve instead
  \begin{gather}
    \matb^{-1}\mata\vx = \matb^{-1}\vb.
  \end{gather}
  It is preconditioned from the right, if we solve
  \begin{gather}
    \mata\matb^{-1}\vy = \vb,\quad \vx = \matb^{-1}\vy.
  \end{gather}
\end{Definition}

\begin{Definition}{preconditioner}
  A \define{preconditioner} is a matrix $\matb^{-1}$ such that the
  condition number of
  \begin{gather}
    \matb^{-1}\mata\qquad\text{or}\qquad \mata\matb^{-1}
  \end{gather}
  is much less than the one of $\mata$. Furthermore, the evaluation
  of $\matb^{-1}\vr$ is at least much cheaper than the one of
  $\mata^{-1}\vr$, ideally not much more expensive than the product
  $\mata\vx$.
\end{Definition}

\begin{intro}
  Preconditioning the conjugate gradient method poses an additional
  challenge, since for two symmetric, positive definite matrices
  $\mata$ and $\matb^{-1}$, their product $\matb^{-1}\mata$ is not
  necessarily symmetric. A possible solution is symmetric
  preconditioning,
  \begin{gather}
    \matc^{-\nicefrac12} \mata \matc^{-\nicefrac12} \vy
    = \matc^{-\nicefrac12}\vb,
    \quad \vx = \matc^{-\nicefrac12} \vy.
  \end{gather}
  Here, $\matc^{-\nicefrac12}$ is a preconditioner for
  $\mata^{\nicefrac12}$. Unfortunately, it is not easy to obtain such
  a method. Therefore, we follow another route.
\end{intro}

\begin{Lemma}{preconditioner-symmetry}
  Let $\mata$ and $\matb$ be symmetric, positive definite matrices. Then, $\matb^{-1}\mata$ is self-adjoint with respect to the $\matb$-inner product, i.~e.
  \begin{gather}
    \scal(\matb^{-1}\mata\vx,\vx)_{\matb}
    = \scal(\mata\vx,\vx)_2
    = \scal(\vx,\mata\vx)_2
    = \scal(\vx,\matb\matb^{-1}\mata\vx)_2
    =\scal(\vx,\matb^{-1}\mata\vx)_{\matb}.
  \end{gather}
\end{Lemma}

\begin{remark}
  The term ``orthogonality'' in the Lanczos procedure can be in
  reference to any inner product, if orthogonalization is performed
  with that inner product. In that case, the property ``symmetric'' of
  the matrix must be replaced by ``self-adjoint with respect to the inner product''

  We can now construct a preconditioned conjugate gradient method by
  changing the inner products and introducing the residual of the
  preconditioned equation as a new auxiliary vector
  \begin{gather}
    \vz_{j} = \matb^{-1} \vr_{j} = \matb^{-1}\vb - \matb^{-1}\mata\vx_{j}.
  \end{gather}
  Then, we obtain
  \begin{align}
    \scal(\vz_j,\vz_j)_{\matb} &= \scal(\matb\matb^{-1}\vr_j,\vz_j)_2 = \scal(\vr_j,\vz_j)_2,\\
    \scal(\matb^{-1}\mata\vp_j,\vp_j)_{\matb} &= \scal(\mata\vp_j,\vp_j)_2 = \scal(\vp_j,\vp_j)_{\mata}.
  \end{align}
  Thus, the algorithm can be implemented without application of the
  matrix $\matb$, which may not be easy to implement.
\end{remark}

\begin{Algorithm*}{pcg}{Preconditioned Conjugate Gradient}
  \begin{algorithmic}[1]
    \State $\vr_0 = \vb-\mata \vx_0$, $\vz_0 = \matb^{-1}\vr_0$
    \State $\vp_0 \gets \vz_0$
    \For{$j=0,1,\dots$}
    \State $\alpha_j \gets \frac{\scal(\vr_j,\vz_j)}{\scal(\mata\vp_j,\vp_j)}$
    \State $\vx_{j+1} \gets \vx_j + \alpha_j \vp_j$
    \State $\vr_{j+1} \gets \vr_j - \alpha_j \mata \vp_j$
    \State $\vz_{j+1} \gets \matb^{-1} \vr_{j+1}$
    \State $\beta_j =\frac{\scal(\vr_{j+1},\vz_{j+1})}{\scal(\vr_j,\vz_j)}$
    \State $\vp_{j+1} = \vz_{j+1} + \beta_j \vp_j$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
