\subsection{Projection methods}

\begin{Definition}{galerkin-method}
  Let $\mata\in\Rnn$ and $\vb, \vx\in\R^n$ with $\mata\vx=\vb$. Then,
  the vector $\tilde\vx\in\R^n$ is called the \define{Galerkin
    approximation} of $\vx$ in a subspace $K$ orthogonal to a subspace
  $L$, if there holds
  \begin{align}
    \tilde\vx &\in K,\\
    \vb-\mata\tilde\vx &\perp L.
                         \label{eq:krylov:1}
  \end{align}
  This type of approximation is called \define{Galerkin method}, more
  specifically \define{Ritz-Galerkin method} in the case $K=L$ and
  \define{Petrov-Galerkin method} in the case $K\neq L$.
\end{Definition}

\begin{remark}
  For the case $L=\mata K$ we deduce from the optimization property of
  orthogonal projections that $\mata\tilde \vx$ in~\eqref{eq:krylov:1}
  is the vector in the subspace $\mata K$ closest to $\vb$. Thus,
  $\tilde\vx$ minimizes the \putindex{residual} $\vb-\mata\vy$ over
  all choices $\vy\in K$.
 Note that this does not hold for $L\neq \mata K$.
\end{remark}

\begin{Definition}{projection-step}
  Given a vector $\vx^{(k)}\in\R^n$ and its residual
  $\vr^{(k)}=\vb-\mata\vx^{(k)}$. Then, we say that the vector
  $\vx^{(k+m)}\in\R^n$ is obtained by a \define{projection step}, if
  \begin{gather}
    \vx^{(k+m)} = \vx^{(k)} + \vv,
  \end{gather}
  where after the choice of $m$-dimensional subspaces $K$ and $L$ the update $\vv$ is
  determined by the condition
  \begin{align}
    \vv &\in K\\
    \vr^{(k)} - \mata\vv &\perp L.
  \end{align}
\end{Definition}

Note that the notation $\vx^{(k+m)}$ in the previous definition is
quite arbitrary and we could have written $\vx^{(k+1)}$ as
well. Nevertheless, algorithms we will study later have a structure
where the $m$-dimensional projection step will resemble $m$
one-dimensional steps, at least from the algorithmic point of
view. Thus, we adopt the view of a single step in dimension $m$ as an
$m$-fold step.

\begin{Example}{projection-gauss-seidel}
  The Gauss-Seidel substep is a projection step with the choice
  \begin{gather}
    K=L=\spann{\ve_i}.
  \end{gather}
\end{Example}

\begin{notation}
  We will mostly only consider a single step of the method in
  \slideref{Definition}{projection-step}. Therefore, we will consider
  the step from an initial guess $\vx^{(0)}$ to an approximate solution
  $\tilde \vx$ to simplify the notation.
\end{notation}

\begin{Definition}{projection-method-matrix}
  Let $\matv=(\vv_1,\dots,\vv_m)$ and $\matw=(\vw_1,\dots,\vw_m)$ be bases for
  the subspaces $K$ and $L$, respectively. Then, the solution
  $\tilde \vx$ to the projection step is determined by $\vy\in\R^m$ and
  \begin{align}
    \tilde\vx &= \vx^{(0)} + \matv\vy\\
    \matw^*\mata\matv \vy &= \matw^* \vr^{(0)}.
  \end{align}
  It is thus obtained by solving an $m$-by-$m$ linear system, called
  the projected system or the \define{Galerkin
    equations}. $\matw^*\mata\matv\in\R^{m\times m}$ is the
  \define{projected matrix}.
\end{Definition}

\begin{proof}
  See \cite[Section 5.1.2]{Saad00}.
\end{proof}

\begin{Theorem}{projected-invertible}
  Let one of the following conditions hold:
  \begin{enumerate}
  \item $\mata$ is symmetric, positive definite and $L=K$.
  \item $\mata$ is invertible and $L = \mata K$.
  \end{enumerate}
  Then, the projected matrix $\matw^*\mata\matv$ is invertible for any
  bases $\matv$ and $\matw$ of $K$ and $L$, respectively.
\end{Theorem}

\begin{Theorem}{projection-orthogonal-optimal}
  Let $\mata\in\Rnn$ be symmetric, positive definite. Then,
  $\tilde \vx$ is the result of the orthogonal ($L=K$) projection
  method with initial vector $\vx^{(0)}$ if and only if it minimizes
  the \putindex{A-norm} of the error over the space $\vx^{(0)}+K$. Namely, for the
  solution $\vx$ of $\mata\vx=\vb$ there holds
  \begin{gather}
    \norm{\tilde\vx-\vx}_A = \min_{\vy\in\vx^{(0)}+K} \norm{\vy-\vx}_A.
  \end{gather}
  Here, $\norm\vy_A = \sqrt{\vy^*\mata\vy}$.
\end{Theorem}

\begin{Theorem}{projection-oblique-optimal}
  Let $\mata\in\Rnn$ and $L=\mata K$. Then, $\tilde \vx$ is the result
  of the (oblique) projection method with initial vector $\vx^{(0)}$
  if and only if it minimizes the Euclidean norm of the residual
  $\vb-\mata\tilde\vx$ over the space $\vx^{(0)}+K$. Namely, there
  holds
  \begin{gather}
    \norm{\vb-\mata\tilde\vx}_2
    = \min_{\vy\in\vx^{(0)}+K} \norm{\vb-\mata\vy}_2
  \end{gather}
\end{Theorem}

\begin{Example}{projection-1d}
  A one-dimensional projection method can be characterized by two
  vectors $\vv$ and $\vw$. Then,
  \begin{gather}
    K = \spann{\vv^{(k)}},
    \qquad L = \spann{\vw^{(k)}}.
  \end{gather}
  The new solution is
  \begin{gather}
    \vx^{(k+1)} = \vx^{(k)}+\alpha \vv^{(k)},
  \end{gather}
  where
  \begin{gather}
    \alpha = \frac{\scal(\vr^{(k)},\vw)}{\scal(\mata\vv,\vw)}
  \end{gather}
  is determined from the Galerkin condition
  \begin{gather}
    \vr^{(k)}-\mata\vv^{(k)} \perp \vw^{(k)}
  \end{gather}
\end{Example}

\begin{Algorithm*}{steepest-descent-algol}{The steepest descent method}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$ s.p.d.; $\quad\vx,\vb\in\R^n$
    \State $\vr\gets \vb-\mata\vx$
    \Repeat
    \State $\alpha\gets\frac{\scal(\vr,\vr)}{\scal(\mata\vr,\vr)}$
    \State $\vx \gets \vx + \alpha\vr$
    \State $\vr \gets \vr-\alpha\mata\vr$
    \Until convergence
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{steepest-descent}
  Each step of the steepest descent method computes the minimum of
  $F(\vy) = \norm{\vy-\vx}_A^2$ along the line from the current
  iterate $\vx^{(k)}$ in direction $-\nabla F(\vx^{(k)})$.
  Furthermore, there holds
  \begin{gather}
    \vr^{(k+1)} \perp \vr^{(k)}.
  \end{gather}
  
\end{Lemma}

\begin{remark}
  In this algorithm, the operation $\mata\vr$ is applied twice. Since
  in most implementations this will be the part with the most
  computational operations, we introduce an auxiliary vector
  $\vp = \mata\vr$, which can be used in lines 3 and 5. At the cost of
  one additional vector, the numerical effort per step is almost
  cut in half.
\end{remark}

\begin{Remark}{convergence-residual}
  The residual $\vr = \vb - \mata\tilde\vx$ of the current iterate
  $\tilde\vx$ measures the misfit of $\tilde\vx$ in the equation
  $\mata\vx = \vb$. Since the error $\tilde\vx-\vx$ is unknown, we can
  use it as criterion for the accuracy of the current
  solution. Indeed, there holds
  \begin{gather}
    \norm{\vx-\tilde\vx}
    = \norm*{\mata^{-1}\bigl(\mata\vx - \mata\tilde\vx\bigr)}
    = \norm*{\mata^{-1}\bigl(\vb - \mata\tilde\vx\bigr)}
    \le \norm{\mata^{-1}}\norm{\vr}.
  \end{gather}
  Therefore, the criterion for convergence of the algorithm is typically
  \begin{gather}
    \norm{\vr} \le \text{TOL},
  \end{gather}
  where TOL is a tolerance chosen by the user.

  Note, that the computation of $\vr$ is subject to roundoff
  errors. Thus, the tolerance should always be chosen
  \begin{gather}
    \text{TOL} > c\norm{\vb}\eps,
  \end{gather}
  where $\eps$ is the machine accuracy and $c$ should account for
  error accumulation in the matrix-vector product.
\end{Remark}

\begin{Algorithm*}{steepest-descent-python1}{Steepest descent in Python}
  \lstinputlisting{python/steepest-descent.py}
\end{Algorithm*}

\begin{remark}
  Depending on the quality of the compiler/interpreter, the code line
  \begin{lstlisting}[language=Python,numbers=none]
    x += alpha*r
  \end{lstlisting}
  may involve creating an auxiliary vector $\vp = \alpha \vr$ and
  adding this vector to $\vx$.  Obviously, this could be avoided by
  directly implementing
  \begin{lstlisting}[language=Python,numbers=none]
    for i in range(0,n):
      x[i] += alpha*r[i]
  \end{lstlisting}
  Since operations like this are ubuquitous in scientific computing,
  they were standardized early on in the BLAS (basic linear algebra
  subroutines) library. It contains the FORTRAN function
  \begin{lstlisting}[language=Fortran,numbers=none]
    SUBROUTINE DAXPY( n, alpha, x, incx, y, incy)
  \end{lstlisting}
  which computes $\vy\gets\vy+\alpha\vx$ for double precision vectors
  of length $n$ (the increment arguments allow to skip elements). For
  usage in Python, there is a wrapper
  \begin{lstlisting}[language=Python,numbers=none]
    scipy.linalg.blas.daxpy(x, y[, n, a, offx, incx, offy, incy])
  \end{lstlisting}
\end{remark}

\begin{Algorithm*}{steepest-descent-python2}{Steepest descent with daxpy}
  \lstinputlisting{python/steepest-descent-axpy.py}
\end{Algorithm*}

\begin{Algorithm*}{minimal-residual-algol}{The minimal residual method}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vx,\vb\in\R^n$
    \State $\vr\gets \vb-\mata\vx$
    \Repeat
    \State $\alpha\gets\frac{\scal(\mata\vr,\vr)}{\scal(\mata\vr,\mata\vr)}$
    \State $\vx \gets \vx + \alpha\vr$
    \State $\vr \gets \vr-\alpha\mata\vr$
    \Until convergence
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{minimal-residual}
  Each step of the minimal method computes the minimum of
  $F(\vy) = \norm{\vb-\mata\vy}_2^2$ along the line from the current
  iterate $\vx^{(k)}$ in direction $\vr$. Furthermore, there holds
  \begin{gather}
    \vr^{(k+1)} \perp \mata \vr^{(k)}
  \end{gather}
\end{Lemma}

\begin{Lemma}{lucky-breakdown}
  The iteration sequences $\{\vx^{(k)}\}$ of the steepest descent and
minimal residual methods, respectively, are well defined except for
the case where $\vx^{(k)}$ is the exact solution $\vx$ and thus
$\vr^{(k)} = 0$.

We refer to this phenomenon as \define{lucky breakdown}, since the
method only fails after the exact solution has been found.
\end{Lemma}

\begin{Lemma*}{kantorovich-inequality}{Kantorovich inequality}
  Let $\mata\in\Rnn$ be symmetric, positive definite with minimal and
  maximal eigenvalues $\lambda_{\min}$ and $\lambda_{\max}$. Then, for
  $\vx\in\R^n$ there holds
  \begin{gather}
    \frac{\scal(\mata\vx,\vx)\scal(\mata^{-1}\vx,\vx)}{\scal(\vx,\vx)^2}
    \le \frac{(\lambda_{\min}+\lambda_{\max})^2}{4\lambda_{\min}\lambda_{\max}}
  \end{gather}
\end{Lemma*}

\begin{proof}
  See~\cite[Lemma 5.8]{Saad00}.
\end{proof}

\begin{Theorem}{steepest-descent-convergence}
  Let $\mata\in\Rnn$ be symmetric, positive definite with extremal
  eigenvalues $\lambda_{\min}$ and $\lambda_{\max}$. Then, the error
  $\ve^{(k)} = \vx-\vx^{(k)}$ of the steepest descent method admits
  the estimate
  \begin{gather}
    \norm{\ve^{(k+1)}}_A \le \rho \norm{\ve^{(k)}}_A,
  \end{gather}
  where the contraction number is
  \begin{gather}
    \rho
    = \frac{\lambda_{\max}-\lambda_{\min}}{\lambda_{\max}+\lambda_{\min}}
    = \frac{\cond_2\mata - 1}{\cond_2\mata+1}
    = 1-\frac2{\cond_2\mata+1}
  \end{gather}
\end{Theorem}

\begin{proof}
  See~\cite[Theorem 5.9]{Saad00}.
\end{proof}

\begin{Theorem}{minimal-residual-convergence}
  Let $\mata\in\Rnn$ such that its symmetric part $(\mata+\mata^T)/2$
  is positive definite. Then, the residuals $\vr^{(k)}$ of the minimal
  residual method admit the estimate
  \begin{gather}
    \norm{\vr^{(k+1)}}_2 \le \rho \norm{\vr^{(k)}}_2,
  \end{gather}
  where the contraction number is
  \begin{gather}
    \rho = \left(1-\frac{\mu^2}{\norm{\mata}^2}\right)^{\nicefrac12}
  \end{gather}
  and $\mu$ is the smallest eigenvalue of $(\mata+\mata^T)/2$.
\end{Theorem}

\subsection{Krylov spaces}

\begin{Definition}{krylov-space}
  For a matrix $\mata\in\Rnn$ and a vector $\vv\in\R^n$, we define the
  \define{Krylov space}
  \begin{gather}
    \krylov_m = \krylov_m(\mata,\vv)
    = \spann{\vv,\mata\vv,\dots,\mata^{m-1}\vv}.
  \end{gather}
\end{Definition}

\begin{Definition}{grade-of-v}
  For a matrix $\mata\in\Rnn$ and a vector $\vv\in\R^n$ we define the
  \define{grade} of $\vv$, more precise the grade of $\vv$ with
  respect to $\mata$ as the minimal grade of a polynomial $p$ such
  that
  \begin{gather}
    p(\mata)\vv = 0
  \end{gather}
\end{Definition}

\begin{Lemma}{krylov-polynomial}
  Let $\mu$ be the grade of $\vv$ with respect to $\mata$.  The Krylov
  space $\krylov_m(\mata,\vv)$ is isomorphic to $\P_{m-1}$ if and only
  if $m\le \mu$. In this case, for any $\vw\in\krylov_m(\mata,\vv)$
  there is a unique polynomial $p\in\P_{m-1}$ such that
  \begin{gather}
    \label{eq:krylov-polynomial-1}
    \vw = p(\mata)\vv.
  \end{gather}

  The Krylov space $\krylov_m(\mata,\vv)$ is invariant under the
  action of $\mata$ if and only if $m\ge\mu$.
\end{Lemma}

\begin{proof}
  See \cite[Propositions 6.1 \& 6.2]{Saad00}.
  The definition of $\krylov_m(\mata,\vv)$ implies that any vector $\vw$ in this space has a representation
  \begin{gather}
    \vw = \sum_{i=0}^{m-1} \alpha_i \mata^i\vv =: p_{\vw}(\mata) \vv.
  \end{gather}
  Since the vectors in this sum span $\krylov_m$, its dimension is
  $m$ if and only if they are linearly independent, which in turn is
  the case if and only if there is a nonzero polynomial
  $p_0\in\P_{m-1}$ such that
  \begin{gather}
    p_0(\mata)\vv = 0.
  \end{gather}
  By assumption, such a polynomial exists if and only if
  $\mu<m$. Thus, we have proven that $\dim \krylov_m = m$ if and only
  if $m\le \mu$ and thus the mapping induced
  by~\eqref{eq:krylov-polynomial-1} is an isomorphism.

  On the other hand, $\krylov_m$ is invariant, if and only if the
  vectors spanning $\krylov_{m+1}$ are linearly dependent. We have
  just proven that this is the case if and only if $\mu<m+1$ or
  $\mu\le m$.
\end{proof}

\begin{Lemma}{krylov-projector}
  Let $\matq_m$ be a projector onto $\krylov_m=\krylov_m(\mata,\vv)$ and define
  \begin{align}
    \mata_m\colon \krylov_m &\to \krylov_m\\
    \vv&\mapsto \matq_m \mata.
  \end{align}
  Then, for any polynomial $q\in\P_{m-1}$ there holds
  \begin{gather}
    \label{eq:krylov-projector-1}
    q(\mata)\vv = q(\mata_m)\vv.
  \end{gather}
  For any polynomial $p\in\P_m$, there holds
  \begin{gather}
    \label{eq:krylov-projector-2}
    \matq_m p(\mata)\vv = p(\mata_m)\vv.
  \end{gather}
\end{Lemma}

\begin{proof}
  See \cite[Proposition 6.3]{Saad00}.  First, we
  prove~\eqref{eq:krylov-projector-1} by proving it for the monomial
  basis $q_i(x) = x^i$. It is obviously true for $q_0\equiv 1$. Now,
  we assume it is true for $q_i$ and conclude for $q_{i+1}$. To this
  end, we observe
  \begin{gather}
    q_{i+1}(\mata)\vv = \mata q_i(\mata)\vv = \mata q_i(\mata_m)\vv.
  \end{gather}
  Since $i<m-1$, this vector is in $\krylov_m$ and we obtain
  \begin{gather}
    q_{i+1}(\mata)\vv
    = \matq_m q_{i+1}(\mata)\vv
    = \matq_m \mata q_i(\mata_m)\vv
    = q_{i+1}(\mata_m)\vv,
  \end{gather}
  since $q_i(\mata_m)\vv\in\krylov_m$. Thus, we have proven the
  statement for $q\in\P_{m-1}$. The proof for $q_{m}(x) = x^m$ uses
  the same induction step, just with the exception that the action of
  $\matq_m$ on the left is not the identity anymore, since
  $q_m(\mata)\vv$ may not be in $\krylov_m$.
\end{proof}

\subsection{The Arnoldi procedure and GMRES}

\begin{intro}
  We now develop algorithms which for a given matrix $\mata\in\Rnn$
  and initial vector $\vv\in\R^n$ construct the Krylov space
  $\krylov_m$ and an orthogonal basis dimension by dimension. This
  algorithm is named after Walter E. Arnoldi in the general case and
  after Cornelius Lanczos in the symmetric case. We begin with the
  Arnoldi process, which leads to the GMRES iteration in thsi section
  and continue with the conjugate gradient method, which is based on
  the Lanczos process in the next one.
\end{intro}

\begin{Algorithm*}{arnoldi-1}{Arnoldi with modified Gram-Schmidt}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$
    \For{$j=1,\dots,m$}
    \State $\vw_j \gets \mata\vv_j$
    \For{$i=1,\dots,j$}
    \State $h_{ij} \gets \scal(\mata\vv_j,\vv_i)$
    \State $\vw_j \gets \vw_j - h_{ij}\vv_i$
    \EndFor
    \State $h_{j+1,j} \gets \norm{\vw_j}_2$
    \If{$h_{j+1,j}=0$} \textbf{stop}\EndIf
    \State $\vv_{j+1} = \nicefrac{\vw_j}{h_{j+1,j}}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{arnoldi-krylov}
  Assume that the Arnoldi algorithm does not stop before step
  $m$. Then, the vectors $\vv_1,\dots,\vv_m$ form an orthonormal basis
  of $\krylov_m(\mata,\vv_1)$.
\end{Lemma}

\begin{proof}
  See \cite[Proposition 6.4]{Saad00}.
\end{proof}

\begin{Algorithm*}{arnoldi-householder}{Arnoldi with Householder}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$
    \State Choose $\matq_1$ such that $\matq_1\vv_1 = \ve_1$.
    \For{$j=1,\dots,m$}
    \State $\vv_j \gets \matq_1\dots\matq_j\ve_j$
    \State $\matq_{j+1}^*\matr \gets \matq_j\dots\matq_1(\vv_1,\mata\vv_1,\dots,\mata\vv_m)$
    \Comment{Householder}
    \If{$r_{j+1,j+1}=0$} \textbf{stop}\EndIf
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{arnoldi-householder}
  If the Arnoldi algorithm with Householder orthogonalization did not
  stop before step $m$, then the set $\vv_1,\dots,\vv_m$ is an
  orthonormal basis for $\krylov_m(\mata,\vv_1)$.
\end{Lemma}

\begin{proof}
  By line 4 of the algorithm, for $j=1,\dots,m$
  \begin{gather}
    \matq_1^*\dots\matq_{j+1}^* \matr = (\vv_1,\mata\vv_1,\dots,\mata\vv_j)
  \end{gather}
  is a QR factorization of the (rectangular) matrix on the
  right. Therefore, we can apply \slideref{Lemma}{qr-columns}, which
  readily implies the result.
\end{proof}

\begin{Theorem}{arnoldi-projection}
  Let $\matv_m = (\vv_1,\dots,\vv_m)$ be the matrix of basis vectors
  generated by the Arnoldi method and let
  $\overline{\matH}_m\in\R^{m+1\times m}$ be the Hessenberg matrix of
  entries $h_{ij}$ computed in the algorithm. Let further
  $\matH_m\in\R^{m\times m}$ be the same matrix without the last
  row. Then, there holds
  \begin{align}
    \label{eq:arnoldi-projection-1}
    \mata\matv_m &= \matv_m\matH_m+\vw_m\ve_m^T\\
    \label{eq:arnoldi-projection-2}
                 &= \matv_{m+1}\overline{\matH}_m
  \end{align}
  and
  \begin{gather}
    \label{eq:arnoldi-projection-3}
    \matH_m = \matv_m^T\mata\matv_m.
  \end{gather}
\end{Theorem}

\begin{proof}
  See also \cite[Proposition 6.5]{Saad00}.
  From lines 2--6 of \slideref{Algorithm}{arnoldi-1}, we deduce
  \begin{gather}
    \mata \vv_j = \vw_j + \sum_{i=1}^j h_{ij} \vv_i,
    \qquad j=1,\dots,m.
    =  \sum_{i=1}^{j+1} h_{ij} \vv_i,
  \end{gather}
  For $j<m$, we use line 9 to obtain
  \begin{gather}
    \mata \vv_j =  \sum_{i=1}^{j+1} h_{ij} \vv_i.
  \end{gather}
  Since $\mata \vv_j$ is the $j$th column of $\mata\matv_m$ and
  $\matH_m$ is Hessenberg, we can rewrite this as
  \begin{gather}
    \mata\matv_m\ve_j = \matv_m\matH\ve_j,\qquad j=1,\dots,m-1.
  \end{gather}
  For $j=m$, we cannot replace $\vw_m$, therefore,
  \begin{gather}
    \mata\matv_m\ve_m = \matv_m\matH\ve_m + \vw_m.
  \end{gather}
  Using $\ve_m^T\ve_j=0$ for $j<m$, we can combine these to the matrix
  representation
  \begin{gather}
    \mata\matv_m = \matv_m\matH + \vw_m\ve_m^T.
  \end{gather}
  Adding the vector $\vv_{m+1}$ from the next step of the algorithm,
  we get the alternative formulation~\eqref{eq:arnoldi-projection-2}.
  In order to prove~\eqref{eq:arnoldi-projection-3}, we multiply from
  the left with $\matv_m^T$. The proof is finished by observing that
  $\matv_m^T\matv_m$ is the identity on $\R^m$ and $\matv_m^T\vw_m=0$.
\end{proof}

\begin{remark}
  The next step of the Arnoldi method can always be computed if
  $h_{j+1,j}\neq 0$ in line 9. If it cannot be, we speak of a
  \define{breakdown} of the method.
\end{remark}

\begin{Lemma}{arnoldi-breakdown}
  The Arnoldi method stops at step $j$ if and only if the subspace $\krylov_j$ is invariant under $\mata$ or equivalently, the minimal polynomial of $\vv_1$ is of degree $j$.
\end{Lemma}

\begin{proof}
  The algorithm stops in line 8 if and only if $\mata\vv_j$ is a
  linear combination of the previously computed basis vectors, such
  that $\vw_j=0$ in line 5 at the end of the loop. This happens
  exactly when the Krylov space is invariant.
\end{proof}

\begin{Theorem}{arnoldi-linear-system}
  Given an initial vector $\vx^{(0)}$ and
  $\vr^{(0)} = \vb - \mata \vx^{(0)}$. Let $\beta=\norm{\vr^{(0)}}$
  and choose $\vv_1 =\nicefrac{\vr^{(0)}}{\beta}$ in Arnoldi's
  method. Then, the Galerkin approximation
  \begin{align}
    x^{(m)} &\in x^{(0)} + \krylov_m(\mata,r^{(0)})\\
    b-\mata x^{(m)} &\perp \krylov_m(\mata,r^{(0)}),
  \end{align}
  is obtained in two steps. Solve in $\R^m$
  \begin{gather}
    \matH_m\vy_m = \beta\ve_1,
  \end{gather}
  and let
  \begin{gather}
    \vx^{(m)} = \vx^{(0)} + \matv_m \vy_m.
  \end{gather}
\end{Theorem}

\begin{Theorem}{arnoldi-linear-residual}
  Let $\vx^{(m)}$ be the Galerkin solution in
  \slideref{Theorem}{arnoldi-linear-system}. Then, there holds
  \begin{gather}
    \vb-\mata\vx^{(m)} = -h_{m+1,m} \ve_m^T \vy_m \vv_{m+1},
  \end{gather}
  and therefore
  \begin{gather}
    \norm{\vb-\mata\vx^{(m)}}_2 = h_{m+1,m} \abs{\ve_m^T \vy_m}.
  \end{gather}  
\end{Theorem}

\begin{proof}
  See also~\cite[Proposition 6.7]{Saad00}
  We use the definition of the residual of $\vx^{(m)}$ and the representation of \slideref{Theorem}{arnoldi-projection} to obtain
  \begin{align}
    \vr^{m} = b-\mata\vx^{(m)}
    &= b- \mata\left(\vx^{(0)} + \matv_m\vy_m\right)\\
    &=\vr^{(0)} - \mata\matv_m\vy_m\\
    &= \beta\vv_1 - \matv_m\matH_m\vy_m - \vw_m\ve_m^T\vy_m.
  \end{align}
  Since $\vy_m$ solves the projected system
  $\matH_m\vy_m = \beta\ve_1$, there holds
  \begin{gather}
    \beta\vv_1 - \matv_m\matH_m\vy_m = \beta\vv_1 - \beta \matv_m\ve_1 = 0.
  \end{gather}
  Furthermore, we rewrite
  \begin{gather}
    \vw_m\bigl(\ve_m^T\vy_m\bigr) = h_{m+1,m} \bigl(\ve_m^T\vy_m\bigr) \vv_{m+1}.
  \end{gather}
\end{proof}

\begin{remark}
  Following \slideref{Theorem}{projected-invertible} this algorithm
  works reliably only for $\mata$ symmetric, positive definite. We
  will study this case in more detail in the next section and derive a
  much improved version there. Here, we continue adapting it for the
  nonsymmetric case.
\end{remark}

\begin{Theorem}{normal-equations}
  Let $\mata\in\R^{m\times n}$ with $m\ge n$ and $\vb\in\R^m$. Then, a vector $\vx$ is the solution to the minimization problem
  \begin{gather}
    \norm{\vb-\mata\vx}_2 \stackrel{!}{=} \min,
  \end{gather}
  if an only if $\vx$ solves the \define{normal equations}
  \begin{gather}
    \mata^T\mata\vx = \mata^T \vb.
  \end{gather}
\end{Theorem}

\begin{Lemma}{oblique-normal}
  Let $\mata\in\Rnn$ be invertible. Let $K$ be a subspace of $\Rnn$. Then, the oblique projection
  \begin{gather}
    \vv\in K, \qquad \vb-\mata\vv \perp \mata K,
  \end{gather}
  and the orthogonal projection
  \begin{gather}
    \vw\in K, \qquad \mata^T \vb-\mata^T\mata\vw \perp K,
  \end{gather}
  are identical.
\end{Lemma}

\begin{proof}
  Homework.
\end{proof}

\begin{Algorithm*}{gmres}{Generalized Minimal Residual}
  Given $\vx^{(0)}\in\R^n$, the ``iterate'' $\vx^{(m)}$ of the
  \define{GMRES} method is computed by the following steps.
  \begin{enumerate}
  \item Compute the Arnoldi basis $\matv_{m+1}$ of
    $\krylov_{m+1}(\mata,\vr^{(0)})$ and the matrix $\overline\matH_m\in\R^{m+1\times m}$.
  \item Solve the least squares problem
    \begin{gather}
      \norm{\beta\ve_1 - \overline\matH_m \vy}_2 \stackrel{!}{=} \min
    \end{gather}
    in $\R^m$ with $\beta = \norm{\vr^{(0)}}_2$.
  \item Let
    \begin{gather}
      \vx^{(m)} = \vx^{(0)} + \matv_{m}\vy
    \end{gather}
  \end{enumerate}
\end{Algorithm*}

\begin{Theorem}{gmres-projection}
  The GMRES method computes the update of the $m$-dimensional oblique
  projection step.
\end{Theorem}

\begin{Theorem}{gmres-breakdown}
  If the GMRES method breaks down at step $m$, then
  $\vr^{(m)}=0$. Thus, the GMRES method only encounters a
  \putindex{lucky breakdown}.
\end{Theorem}

\begin{remark}
  The GMRES method encounters a problem if convergence is slow: the
  effort for computing the last basis vector in $\matv_m$ is of order
  $mn$, the effort for solving the least-squares problem is
  $m^3$. Similarly, the storage requirement for $\matv_m$ is $mn$ and
  for $\overline \matH_m$, it is $m^2$. Thus, the algorithm is
  feasible for large sparse matrices only if $m\ll n$.

  Therefore, we introduce two variants of the GMRES algorithm:
  \begin{itemize}
  \item Restarted GMRES simply deletes the whole basis $\matv_m$ and the matrix $\overline\matH_m$ every $k$-th step starts fresh.
  \item Truncated GMRES orthogonalizes only with respect to the last
    $k$ basis vectors.
  \end{itemize}
\end{remark}

\begin{Algorithm*}{gmres-restart}{Restarted GMRES}
  \begin{enumerate}
  \item Choose a maximal dimension $m$ of the Krylov space
  \item Begin with an initial guess $\vx^{(0)}$.
  \item For $k>0$, given $\vx^{(km)}$, perform the GMRES method with
    basis size $m$ to obtain $\vx^{((k+1)m)}$.
  \item Check the stopping criterion as usual inside the GMRES method
  \end{enumerate}
\end{Algorithm*}

\begin{Algorithm*}{gmres-truncated}{Truncated GMRES}
  Run the GMRES method as usual, but in the Arnoldi process, only
  orthogonalize with respect to the last $k$ vectors for fixed $k$.
\end{Algorithm*}

\begin{remark}
  Note that both modifications destroy the minimization property of the method.
\end{remark}

\subsection{The Lanczos procedure and the conjugate gradient method}

\begin{Lemma}{arnoldi-symmetric}
  If the matrix $\mata$ is symmetric, the vectors of the Arnoldi
  procedure admit the three-term recurrence relation
  \begin{gather}
    \beta_{j+1}\vv_{j+1} = \mata \vv_j - \alpha_j \vv_j - \beta_j\vv_{j-1}
  \end{gather}
  with $\vv_0 = 0$, $\beta_1 = 0$ and
  \begin{gather}
    \alpha_j =  h_{jj}, \qquad \beta_j = h_{j-1,j},
    \qquad j=1,\dots
  \end{gather}
  In particular, orthogonalization is only necessary with respect to
  the previous two basis vectors.
\end{Lemma}

\begin{proof}
  See \cite[Section 6.6.1]{Saad00}.
\end{proof}

\begin{Algorithm*}{lanczos}{Lanczos}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$, $\vv_0=0$, $\beta_1=0$.
    \For{$j=1,\dots,m$}
    \State $\vw_j \gets \mata\vv_j - \beta_j \vv_{j-1}$
    \State $\alpha_{j} \gets \scal(\vw_j,\vv_j)$
    \State $\vw_j \gets \vw_j - \alpha_j\vv_i$
    \State $\beta_{j+1} \gets \norm{\vw_j}_2$
    \If{$\beta_{j+1}=0$} \textbf{stop}\EndIf
    \State $\vv_{j+1} = \nicefrac{\vw_j}{\beta_{j+1}}$
    \EndFor
  \end{algorithmic}  
\end{Algorithm*}

\begin{Lemma*}{lanczos-linear}{Lanczos solver}
  The Galerkin approximation   \begin{align}
    x^{(m)} &\in x^{(0)} + \krylov_m(\mata,r^{(0)})\\
    b-\mata x^{(m)} &\perp \krylov_m(\mata,r^{(0)}),
  \end{align}
  is obtained as $\vx^{(m)} = \vx^{(0)} + \matv_m \vy_m$, where $\matv_m$ is the basis obtained from the Lanczos process and $\vy_m$ solves
  \begin{gather}
    \matH_m\vy_m = \beta\ve_1,
    \qquad \matH_m =
    \begin{pmatrix}
      \alpha_1 & \beta_2\\
      \beta_2 & \alpha_2 & \ddots\\
      &\ddots&\ddots&\beta_m\\
      &&\beta_{m}&\alpha_m
    \end{pmatrix}.
  \end{gather}
  For the residual, there holds
  \begin{gather}
    \vr^{(m)} = \vb-\mata\vx^{(m)} = -\beta_{m+1} \ve_m^T \vy_m \vv_{m+1}.
  \end{gather}
\end{Lemma*}

\begin{proof}
  These are the statements of
  \slideref{Theorem}{arnoldi-linear-system} and
  \slideref{Theorem}{arnoldi-linear-residual}. Since the Lanczos
  process is just a specialization of the Arnoldi process, they still
  hold.
\end{proof}

\begin{remark}
  The main difference between Arnoldi and Lanczos is the fact that
  every step of the Lanczos process onle requires the previous two
  vectors, while Arnoldi requires storing the whole history.

  This implies, that Lanczos itself is much closer to the concept of
  an iterative method than Arnoldi, weren't it for the computation of
  $\vy_m$.

  We will now develop a new algorithm based on the observation that
  also the corrections for $\vx^{(m)}$ can be computed incrementally.
\end{remark}

\begin{Lemma}{lanczos-incremental}
  Given the vector $\vx^{(m)}$ in the Lanczos solver, the vector
  $\vx^{(m+1)}$ can be computed incrementally by an update of the form
  \begin{gather}
    \vx^{(m+1)} = \vx^{(m)} + \zeta_{m+1}\vp^{(m+1)},
  \end{gather}
  where the vector $\vp^{(m+1)}$ itself is computed only using the
  vectors $\vp^{(m)}$ and $\vv^{(m+1)}$.
\end{Lemma}

\begin{proof}
  See \cite[Section 6.7.1]{Saad00}.

  First, we compute the LU factorization of $\matH_m$ as
  \begin{gather}
    \matH_m =
    \begin{pmatrix}
      1\\\lambda_2&1\\
      &\ddots&\ddots\\
      &&\lambda_m&1
    \end{pmatrix}
    \times
    \begin{pmatrix}
      \eta_1&\beta_2\\
      &\ddots&\ddots\\
      &&\eta_{m-1}&\beta_m\\
      &&&\eta_m
    \end{pmatrix},
  \end{gather}
  where
  \begin{gather}
    \lambda_k = \frac{\beta_k}{\eta_{k-1}},
    \qquad
    \eta_k = \alpha_k - \lambda_k \beta_k.
  \end{gather}
  We note that LU decomposition works from top to bottom and left to
  right, such that already processed rows do not change when adding
  more steps to the algorithm.

  Now we introduce matrices and vectors
  \begin{gather}
    \matp_m = \matv_m \matu_m^{-1} \in \R^{n\times m},
    \qquad \vz_m = \matl_m^{-1} \beta \ve_1\in \R^m,
  \end{gather}
  such that
  \begin{align}
    \vx^{(m)}
    &= \vx^{(0)} + \matv_m \matu_m^{-1} \matl_m^{-1} \beta \ve_1\\
    &= \vx^{(0)} + \matp_m \vz_m
  \end{align}
  With $\zeta_k$ following the recursion
  $\zeta_k = - \lambda_k \zeta_{k-1}$, we realize
  \begin{gather}
    \vz_m =
    \begin{pmatrix}
      \zeta_1\\\vdots\\\zeta_m
    \end{pmatrix}
    =
    \begin{pmatrix}
      \vz_{m-1}\\\zeta_m
    \end{pmatrix}.
  \end{gather}
  Let $\vp_k$ be the column vectors of $\matp_m$. Writing the
  definition of $\matp_m$ in the form $\matp_m \matu_m = \matv_m$, we
  obtain the relation
  \begin{gather}
    \label{eq:nla:krylov:lanczos-p}
    \vp_k = \frac1{\eta_k}\bigl(\vv_k - \beta_k \vp_{k-1}\bigr).
  \end{gather}
  Thus,
  \begin{gather}
    \matp_m =
    \begin{pmatrix}
      \vp_1,\dots,\vp_m
    \end{pmatrix}
    =
    \begin{pmatrix}
      \matp_{m-1},\vp_m
    \end{pmatrix}.
  \end{gather}
  There holds
  \begin{align}
    \vx^{(m)}
    &= \vx^{(0)} + \sum_{k=1}^m \zeta_k \vp_k,\\
    &= \vx^{(m-1)} + \zeta_m \vp_m.
  \end{align}
\end{proof}

\begin{remark}
  Thus, we have proven a simple update formula for the iterates
  $\vx^{(m)}$ based on an additional vector $\vp_m$, which is again
  obtained by an update formula.

  We have used an LU decomposition without pivoting though in the
  derivation of this algorithm, which might suffer from numerical
  instability. Therefore, we will now analyze the main properties of
  the sequences $\vv_m$, $\vp_m$ and $\vr_m$ and then write a new
  algorithm to obtain these sequences.
\end{remark}

\begin{Lemma}{lanczos-orthogonality}
  The sequences of vector $\vr_m = \vb-\mata\vx^{(m)}$ and $\vp_m$
  constructed in the preceding lemma have the following orthogonality
  properties:
  \begin{xalignat}2
    \scal(\vr_i,\vr_k) &= 0 & i&\neq k\\
    \scal(\mata\vp_i,\vp_k) &= 0 & i&\neq k.
  \end{xalignat}
\end{Lemma}

\begin{proof}
  Orthogonality of the residuals follows from
  \slideref{Lemma}{lanczos-linear}, since $\vr_m$ is aligned with
  $\vv_{m+1}$.

  The second orthogonality relation is equivalent to the fact that
  $\matp_m^T\mata\matp_m$ is diagonal.
  \begin{align}
    \matp_m^T\mata\matp_m
    &= \matu_m^{-T}\matv_m^T\mata\matv_m\matu_m^{-1}\\
    &= \matu_m^{-T}\matH_m\matu_m^{-1}\\
    &= \matu_m^{-T}\matl_m.
  \end{align}
  The first term is symmetric.  The last is the product of two lower
  triangular matrices and thus lower triangular. Thus, the matrix must
  be diagonal.
\end{proof}

\begin{Algorithm*}{cg}{Conjugate Gradient Method}
  \begin{algorithmic}[1]
    \State $\vr_0 = b-\mata \vx_0$
    \State $\vp_0 = \vr_0$
    \For{$j=0,1,\dots$}
    \State $\alpha_j = \frac{\scal(\vr_j,\vr_j)}{\scal(\mata\vp_j,\vp_j)}$
    \State $\vx_{j+1} = \vx_j + \alpha_j \vp_j$
    \State $\vr_{j+1} = \vr_j - \alpha_j \mata \vp_j$
    \State $\beta_j =\frac{\scal(\vr_{j+1},\vr_{j+1})}{\scal(\vr_j,\vr_j)}$
    \State $\vp_{j+1} = r_{j+1} + \beta_j \vp_j$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{remark}
  Note the index shift for the vectors $\vp_j$!
\end{remark}

\begin{Lemma}{cg-orthogonality}
  The sequences $\vx_j$ and $\vr_j$ produced by the cg method coincide
  with those of the Lanczos solver (in exact arithmetic). The sequence
  $\vp_j$ of the cg method coincides up to scalar factors with the one
  in \slideref{Lemma}{lanczos-incremental}.
\end{Lemma}

\begin{proof}
  Note that the vectors $\vp_j$ refers to the cg algorithm, the
  vectors from \slideref{Lemma}{lanczos-incremental} will be
  distinguished clearly.

  We start with the update formula
  $\vx_{j+1} = \vx_j + \alpha_j \vp_j$, which immediately implies
  \begin{gather}
    \vr_{j+1} = \vr_j - \alpha_j \mata\vp_j.
  \end{gather}
  Orthogonality between $\vr_{j+1}$ and $\vr_j$ implies
  \begin{gather}
    \alpha_j = \frac{\scal(\vr_j,\vr_j)}{\scal(\mata\vp_j,\vr_j)}.
  \end{gather}
  Since $\vr_j$ is a multiple of $\vv_{j+1}$, the update
  formula~\eqref{eq:nla:krylov:lanczos-p} for $\vp_{j+1}$ (after index
  shift) implies that the direction is a linear combination of
  $\vr_{j+1}$ and $\vp_j$. After arbitrary scaling,
  \begin{gather}
    \vp_{j+1} = \vr_{j+1} + \beta_j \vp_{j}.
  \end{gather}
  A-orthogonality implies
  \begin{gather}
    \beta_j = -\frac{\scal(\mata \vp_j,\vr_{j+1})}{\scal(\mata \vp_j,\vp_j)}.
  \end{gather}
  Note that the orthogonality relation determines $\vp_{j+1}$ uniquely
  up to scaling in this subspace. After the scaling has been chosen,
  $\vp_{j+1}$ the orthogonality relation for $\vr_{j+1}$ uniquely
  determines the update for $\vx_{j+1}$, such that the sequences
  indeed coincide.

  The definition of the coefficients still differs from the cg
  algorithm. But, we observe
  \begin{gather}
    \scal(\mata\vp_j,\vr_j) = \scal(\mata\vp_j,\vp_j-\beta_{j-1}\vp_{j-1})
    = \scal(\mata\vp_j,\vp_j).
  \end{gather}
  Similarly, from the update formula for the residual, we get
  \begin{gather}
    \mata\vp_j = \frac1{\alpha_j}\bigl(\vr_j-\vr_{j+1}\bigr),
  \end{gather}
  and thus,
  \begin{gather}
    \beta_j = \frac1{\alpha_j}\frac{\scal(\vr_{j+1},\vr_{j+1}-\vr_j)}{\scal(\mata\vp_j,\vp_j)} = \frac{\scal(\vr_{j+1},\vr_{j+1})}{\scal(\vr_{j},\vr_{j})}.
  \end{gather}
  Note that these forms avoid inner products of possibly almost
  orthogonal vectors.
\end{proof}

\subsection{Convergence analysis}

\begin{intro}
  Here, we will develop a sequence of error estimates for the
  conjugate gradient method based on its optimality as a projection
  method stated in \slideref{Theorem}{projection-orthogonal-optimal}. These
\end{intro}

\begin{Theorem}{cg-optimality}
  For the error $\ve^{(k)} = \vx^{(k)}-\vx$ after $k$ steps of the cg
  method there holds
  \begin{align}
    \norm{\ve^{(k)}}_A
    & = \min_{p\in \P_{k-1}}
      \norm{\ve^{(0)} + \mata p(\mata) \ve^{(0)}}_A\\
    &= \min_{\substack{p\in\P_k\\p(0) = 1}} \norm{p(\mata)\ve^{(0)}}_A.
  \end{align}
\end{Theorem}

\begin{proof}
  There holds $\vx^{(k)} = \vx^{(0)} + \vw$ with
  $\vw \in \krylov_k(\mata,\vr^{(0)})$. Then, by
  \slideref{Lemma}{krylov-polynomial}, there is $p\in\P_{k-1}$ such
  that $\vw = -p(\mata) \vr^{(0)}$. Thus
  \begin{align}
    \vx^{(k)} - \vx
    &= \vx^{(0)} - \vx - p(\mata)\vr^{(0)}\\
    &= \vx^{(0)} - \vx - p(\mata) (\vb-\mata\vx^{(0)})\\
    &= \vx^{(0)} - \vx - p(\mata) (\mata\vx-\mata\vx^{(0)})\\
    &= \vx^{(0)} - \vx - \mata p(\mata) (\vx^{(0)}-\vx).
  \end{align}
  On the other hand, we have the optimality of projection methods in
  \slideref{Theorem}{projection-orthogonal-optimal}, which states
  \begin{align}
    \norm{\ve^{(k)} -\vx}_A
    &= \min_{\vw\in \krylov_k(\mata,\vr^{(0)})} \norm{\vx^{(0)}+\vw-\vx}_A\\
    &= \min_{\vw\in \krylov_k(\mata,\vr^{(0)})} \norm{\ve^{(0)}+\vw}_A.
  \end{align}
  Thus, we obtain the first expression. The second follows by noticing
  that the set of polynomials $q(x) = 1+xp(x)$ with $p\in\P_{k-1}$ is
  just the set of polynomials in $\P_k$ with coefficient $a_0 = 1$.
\end{proof}

\begin{Corollary}{cg-optimality-spectrum}
  Let $\sigma(\mata) = \{\lambda_1,\dots,\lambda_n\} \subset \R^+$ be
  the spectrum of $\mata$. Then, the error after $k$ steps of the
  conjugate gradient method follows the estimate
  \begin{gather}
    \norm{\ve^{(k)}}_A \le \rho_k \norm{\ve^{(0)}}_A,
  \end{gather}
  where
  \begin{gather}
    \rho_k = \min_{\substack{p\in\P_k\\p(0) = 1}} \max_{\lambda\in\sigma(\mata)} \abs{p(\lambda)}.
  \end{gather}
\end{Corollary}

\begin{proof}
  First, since we apply the cg iteration to s.p.d. matrices, there is
  an orthogonal basis $\{\vv_1,\dots,\vv_n\}$ of eigenvectors of
  $\mata$ with real, positive eigenvalues $\lambda_i$. Thus, we can
  write with some coefficients $\alpha_i$:
  \begin{gather}
    \ve^{(0)} = \sum_{i=1}^n \alpha_i \vv_i,
    \qquad
    \ve^{(k)} = \sum_{i=1}^n \alpha_i p(\lambda_i) \vv_i.    
  \end{gather}
  Hence, we can estimate
  \begin{align}
    \norm{\ve^{(k)}}_A^2
    &= \sum_{i=1}^n \lambda_i\alpha_i^2 p(\lambda_i)^2 \norm{\vv_i}_A^2\\
    & \le \max_i p(\lambda_i)^2 \sum_{i=1}^n \lambda_i\alpha_i^2\norm{\vv_i}_A^2\\
    &= \max_i p(\lambda_i)^2 \norm{\ve^{(0)}}_A^2.
  \end{align}
  The statement now follows from \slideref{Theorem}{cg-optimality}.
\end{proof}

\begin{remark}
  It is important to note that the preceding corollary is close to
  optimal. In particular, it can exploit any information which is
  known about the spectrum. In particular, the following statements are true:
  \begin{enumerate}
  \item If the matrix $\mata$ only has $k$ distinct eigenvalues, then
    the cg iteration converges to the solution in $k$ steps.
  \item If the eigenvalues are clustered around $k$ values, then the
    gain in the first $k$ steps will be large.
  \item If the spectrum consists of a continuum plus a single outlier,
    the method will perform almost as if the outlier did not exist.
  \end{enumerate}
  The following corollary now deals with the case that the spectrum
  has no particular structure, but fills a continuous interval
  somewhat evenly.
\end{remark}

\begin{Corollary}{cg-condition-number}
  The error after $k$ steps of the cg method admits the estimate
  \begin{gather}
    \norm{e^{(k)}}_A \le 2 \left(
      \frac{\sqrt{\cond_2(A)}-1}{\sqrt{\cond_2(A)}+1}\right)^k
    \norm{e^{(0)}}_A.
  \end{gather}
\end{Corollary}

\begin{proof}
  We make no further assumtions on the spectrum of $\mata$ except that
  all eigenvalues are in the interval
  $[\lambda_{\min},\lambda_{\max}]$. Thus, we estimate the factors $\rho_k$ in \slideref{Corollary}{cg-optimality-spectrum} by
  \begin{gather}
    \rho_k \le \min_{\substack{p\in\P_k\\p(0) = 1}}
    \max_{\lambda\in[\lambda_{\min},\lambda_{\max}]} \abs{p(\lambda)}.
  \end{gather}
  
  We use the optimality property of Chebyshev polynomials stated in
  \slideref{Corollary}{chebyshev-minimal-2} and note that the
  optimality stated there is exactly what we need. Thus,
  $p = \widehat\pchebyshev_k(\lambda)$ solves the minimization
  problem and there holds
  \begin{align}
    \rho_k
    &\le \left(\pchebyshev_k\left(
      \frac{\lambda_{\max}+\lambda_{\min}}{\lambda_{\max}-\lambda_{\min}}
      \right)\right)^{-1}\\
    & \le \left(\frac{\sqrt{\lambda_{\max}}-\sqrt{\lambda_{\min}}}{\sqrt{\lambda_{\max}}+\sqrt{\lambda_{\min}}}
      \right)^k
  \end{align}
  Dividing both sides of the fraction by $\sqrt{\lambda_{\min}}$ yields the result.
\end{proof}

\begin{Theorem}{gmres-optimality}
  For the residual $\vr^{(k)} = \vb-\mata\vx^{(k)}$ after $k$ steps of
  the GMRES method, there holds
  \begin{gather}
    \norm{\vr^{(k)}}_2 = \min_{\substack{p\in\P_k\\p(0) = 1}}
    \norm{p(\mata)\vr^{(0)}}_2.
  \end{gather}
  If the matrix $\mata$ is diagonalizable, then there holds
    \begin{gather}
    \norm{\vr^{(k)}}_2 \le \rho_k \cond_2(\matv) \norm{\vr^{(0)}}_2,
  \end{gather}
  where $\matv$ is the matrix of eigenvectors and
  \begin{gather}
    \rho_k = \min_{\substack{p\in\P_k\\p(0) = 1}} \max_{\lambda\in\sigma(\mata)} \abs{p(\lambda)}.
  \end{gather}
\end{Theorem}

\begin{proof}
  See \cite[Proposition 6.32]{Saad00}.
\end{proof}

\begin{Corollary}{gmres-estimate}
  Let $\mata$ be diagonalizable such that its spectrum is contained in
  al ellips in the complex plane with center $c$, focal distance $d$
  and semimajor axis $a$. Then,
  \begin{gather}
     \rho_k \approx \left(
      \frac{a+\sqrt{a^2-d^2}}{c+\sqrt{c^2-d^2}}
      \right)^k.
  \end{gather}
\end{Corollary}

\begin{proof}
  See \cite[Corollary 6.33]{Saad00}.
\end{proof}

\begin{remark}
  This theorem is analogue to \slideref{Theorem}{cg-optimality} and
  \slideref{Corollary}{cg-optimality-spectrum} and the proofs are
  identical, based on \slideref{Lemma}{krylov-polynomial} and
  \slideref{Theorem}{projection-oblique-optimal}.

  Note that the step from minimizing over $p(\mata)$ to $p(\lambda)$
  requires that the matrix $\mata$ is diagonalizable and produces
  optimal results only if it is normal. While this condition is not
  strictly necessary, there are examples of arbitrarily bad
  convergence. Indeed, for every nonincreasing sequence of $n-1$
  numbers, you can find a sequence such that the norms of residuals of
  the GMRES method will follow that
  sequence~\cite{GreenbaumPtakStrakos96}.
\end{remark}

\begin{Theorem}{gmres-pos-def}
  If $\mata$ is positive definite, then the GMRES method as well as
  the restarted GMRES method converge.
\end{Theorem}

\begin{proof}
  See \cite[Theorem 6.30]{Saad00}.
\end{proof}

\begin{remark}
  The main practical difference between the conjugate gradient and
  GMRES methods is the short recurrence. While running hundreds of
  steps of cg is not an issue, GMRES becomes infeasible beyond some 30
  steps due to the extensive storage and the explicit
  orthogonalization. There has been research into finding solutions
  for this problem, but having both optimization and short recurrence
  is impossible in general (in your homework you saw an example of
  nonsymmetric matrices where it is possible).
\end{remark}

\subsection{Bi-Lanczos and BiCG-stab}

\begin{intro}
  The problem of short recurrence for nonsymmetric matrices has been
  addressed by biorthogonalization methods. Some of them involve the
  multiplication with $\mata$ as well as with $\mata^*$, others are
  ``transpose-free''.

  They all have some disadvantages in common: they do not minimize the
  error or the residual in each step. Worse, they suffer from
  breakdowns which are not lucky, thus not close to the solution. Even
  worse, there might be a near breakdown, where a denominator is
  almost zero, which must be detected, since after such an incident,
  the future convergence might suffer.

  This has spurred the development of a whole bunch of algorithms,
  from BiCG to BiCG-stab (already involving 8 auxiliary vectors) to
  BiCG-stab($m$), which combines $m$ GMRES steps win an outer
  BiCG-stab algorithm.
\end{intro}

\subsection{Preconditioning}

\begin{intro}
  If matrices are extremely ill-conditioned, convergence of cg or
  GMRES will still be slow, even if considerable faster than steepest
  descent or minimal residual. No general purpose algorithms with a
  better dependence on the condition number have been found.

  Thus, the only way out lies in a positive answer to the question:
  given a linear system $\mata\vx = \vb$, can we modify the system
  into one with better convergence properties?
\end{intro}

\begin{Definition}{preconditioner}
  A \define{preconditioner} is a matrix $\matb^{-1}$ such that the
  condition number of
  \begin{gather}
    \matb^{-1}\mata
  \end{gather}
  is much better than the one of $\mata$. Furthermore, the evaluation
  of $\matb^{-1}$ is at least much cheaper than the one of
  $\mata^{-1}$, ideally not much more expensive than the product
  $\mata\vx$.
\end{Definition}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
