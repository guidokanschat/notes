%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Projection methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Definition}{galerkin-method}
  Let $\mata\in\Rnn$ and $\vb, \vx\in\R^n$ with $\mata\vx=\vb$. Then,
  the vector $\tilde\vx\in\R^n$ is called the \define{Galerkin
    approximation} of $\vx$ in a subspace $K$ orthogonal to a subspace
  $L$, if there holds
  \begin{align}
    \tilde\vx &\in K,\\
    \vb-\mata\tilde\vx &\perp L.
                         \label{eq:krylov:1}
  \end{align}
  This type of approximation is called \define{Galerkin method}, more
  specifically \define{Ritz-Galerkin method} in the case $K=L$ and
  \define{Petrov-Galerkin method} in the case $K\neq L$.
\end{Definition}

\begin{remark}
  For the case $L=\mata K$ we deduce from the optimization property of
  orthogonal projections that $\mata\tilde \vx$ in~\eqref{eq:krylov:1}
  is the vector in the subspace $\mata K$ closest to $\vb$. Thus,
  $\tilde\vx$ minimizes the \putindex{residual} $\vb-\mata\vy$ over
  all choices $\vy\in K$.
 Note that this does not hold for $L\neq \mata K$.
\end{remark}

\begin{Definition}{projection-step}
  Given a vector $\vx^{(k)}\in\R^n$ and its residual
  $\vr^{(k)}=\vb-\mata\vx^{(k)}$. Then, we say that the vector
  $\vx^{(k+m)}\in\R^n$ is obtained by a \define{projection step}, if
  \begin{gather}
    \vx^{(k+m)} = \vx^{(k)} + \vv,
  \end{gather}
  where after the choice of $m$-dimensional subspaces $K$ and $L$ the update $\vv$ is
  determined by the condition
  \begin{align}
    \vv &\in K\\
    \vr^{(k)} - \mata\vv &\perp L.
  \end{align}
\end{Definition}

Note that the notation $\vx^{(k+m)}$ in the previous definition is
quite arbitrary and we could have written $\vx^{(k+1)}$ as
well. Nevertheless, algorithms we will study later have a structure
where the $m$-dimensional projection step will resemble $m$
one-dimensional steps, at least from the algorithmic point of
view. Thus, we adopt the view of a single step in dimension $m$ as an
$m$-fold step.

\begin{Example}{projection-gauss-seidel}
  The Gauss-Seidel substep is a projection step with the choice
  \begin{gather}
    K=L=\spann{\ve_i}.
  \end{gather}
\end{Example}

\begin{notation}
  We will mostly only consider a single step of the method in
  \slideref{Definition}{projection-step}. Therefore, we will consider
  the step from an initial guess $\vx^{(0)}$ to an approximate solution
  $\tilde \vx$ to simplify the notation.
\end{notation}

\begin{Definition}{projection-method-matrix}
  Let $\matv=(\vv_1,\dots,\vv_m)$ and $\matw=(\vw_1,\dots,\vw_m)$ be bases for
  the subspaces $K$ and $L$, respectively. Then, the solution
  $\tilde \vx$ to the projection step is determined by $\vy\in\R^m$ and
  \begin{align}
    \tilde\vx &= \vx^{(0)} + \matv\vy\\
    \matw^*\mata\matv \vy &= \matw^* \vr^{(0)}.
  \end{align}
  It is thus obtained by solving an $m$-by-$m$ linear system, called
  the projected system or the \define{Galerkin
    equations}. $\matw^*\mata\matv\in\R^{m\times m}$ is the
  \define{projected matrix}.
\end{Definition}

\begin{proof}
  See \cite[Section 5.1.2]{Saad00}.
\end{proof}

\begin{Theorem}{projected-invertible}
  Let one of the following conditions hold:
  \begin{enumerate}
  \item $\mata$ is symmetric, positive definite and $L=K$.
  \item $\mata$ is invertible and $L = \mata K$.
  \end{enumerate}
  Then, the projected matrix $\matw^*\mata\matv$ is invertible for any
  bases $\matv$ and $\matw$ of $K$ and $L$, respectively.
\end{Theorem}

\begin{Theorem}{projection-orthogonal-optimal}
  Let $\mata\in\Rnn$ be symmetric, positive definite. Then,
  $\tilde \vx$ is the result of the orthogonal ($L=K$) projection
  method with initial vector $\vx^{(0)}$ if and only if it minimizes
  the \putindex{A-norm} of the error over the space $\vx^{(0)}+K$. Namely, for the
  solution $\vx$ of $\mata\vx=\vb$ there holds
  \begin{gather}
    \norm{\tilde\vx-\vx}_A = \min_{\vy\in\vx^{(0)}+K} \norm{\vy-\vx}_A.
  \end{gather}
  Here, $\norm\vy_A = \sqrt{\vy^*\mata\vy}$.
\end{Theorem}

\begin{Theorem}{projection-oblique-optimal}
  Let $\mata\in\Rnn$ and $L=\mata K$. Then, $\tilde \vx$ is the result
  of the (oblique) projection method with initial vector $\vx^{(0)}$
  if and only if it minimizes the Euclidean norm of the residual
  $\vb-\mata\tilde\vx$ over the space $\vx^{(0)}+K$. Namely, there
  holds
  \begin{gather}
    \norm{\vb-\mata\tilde\vx}_2
    = \min_{\vy\in\vx^{(0)}+K} \norm{\vb-\mata\vy}_2
  \end{gather}
\end{Theorem}

\begin{Example}{projection-1d}
  A one-dimensional projection method can be characterized by two
  vectors $\vv$ and $\vw$. Then,
  \begin{gather}
    K = \spann{\vv^{(k)}},
    \qquad L = \spann{\vw^{(k)}}.
  \end{gather}
  The new solution is
  \begin{gather}
    \vx^{(k+1)} = \vx^{(k)}+\alpha \vv^{(k)},
  \end{gather}
  where
  \begin{gather}
    \alpha = \frac{\scal(\vr^{(k)},\vw)}{\scal(\mata\vv,\vw)}
  \end{gather}
  is determined from the Galerkin condition
  \begin{gather}
    \vr^{(k)}-\mata\vv^{(k)} \perp \vw^{(k)}
  \end{gather}
\end{Example}

\begin{Algorithm*}{steepest-descent-algol}{The steepest descent method}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$ s.p.d.; $\quad\vx,\vb\in\R^n$
    \State $\vr\gets \vb-\mata\vx$
    \Repeat
    \State $\alpha\gets\frac{\scal(\vr,\vr)}{\scal(\mata\vr,\vr)}$
    \State $\vx \gets \vx + \alpha\vr$
    \State $\vr \gets \vr-\alpha\mata\vr$
    \Until convergence
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{steepest-descent}
  Each step of the steepest descent method computes the minimum of
  $F(\vy) = \norm{\vy-\vx}_A^2$ along the line from the current
  iterate $\vx^{(k)}$ in direction $-\nabla F(\vx^{(k)})$.
  Furthermore, there holds
  \begin{gather}
    \vr^{(k+1)} \perp \vr^{(k)}.
  \end{gather}
  
\end{Lemma}

\begin{remark}
  In this algorithm, the operation $\mata\vr$ is applied twice. Since
  in most implementations this will be the part with the most
  computational operations, we introduce an auxiliary vector
  $\vp = \mata\vr$, which can be used in lines 3 and 5. At the cost of
  one additional vector, the numerical effort per step is almost
  cut in half.
\end{remark}

\begin{Remark}{convergence-residual}
  The residual $\vr = \vb - \mata\tilde\vx$ of the current iterate
  $\tilde\vx$ measures the misfit of $\tilde\vx$ in the equation
  $\mata\vx = \vb$. Since the error $\tilde\vx-\vx$ is unknown, we can
  use it as criterion for the accuracy of the current
  solution. Indeed, there holds
  \begin{gather}
    \norm{\vx-\tilde\vx}
    = \norm*{\mata^{-1}\bigl(\mata\vx - \mata\tilde\vx\bigr)}
    = \norm*{\mata^{-1}\bigl(\vb - \mata\tilde\vx\bigr)}
    \le \norm{\mata^{-1}}\norm{\vr}.
  \end{gather}
  Therefore, the criterion for convergence of the algorithm is typically
  \begin{gather}
    \norm{\vr} \le \text{TOL},
  \end{gather}
  where TOL is a tolerance chosen by the user.

  Note, that the computation of $\vr$ is subject to roundoff
  errors. Thus, the tolerance should always be chosen
  \begin{gather}
    \text{TOL} > c\norm{\vb}\eps,
  \end{gather}
  where $\eps$ is the machine accuracy and $c$ should account for
  error accumulation in the matrix-vector product.
\end{Remark}

\begin{Algorithm*}{steepest-descent-python1}{Steepest descent in Python}
  \lstinputlisting{python/steepest-descent.py}
\end{Algorithm*}

\begin{remark}
  Depending on the quality of the compiler/interpreter, the code line
  \begin{lstlisting}[language=Python,numbers=none]
    x += alpha*r
  \end{lstlisting}
  may involve creating an auxiliary vector $\vp = \alpha \vr$ and
  adding this vector to $\vx$.  Obviously, this could be avoided by
  directly implementing
  \begin{lstlisting}[language=Python,numbers=none]
    for i in range(0,n):
      x[i] += alpha*r[i]
  \end{lstlisting}
  Since operations like this are ubuquitous in scientific computing,
  they were standardized early on in the BLAS (basic linear algebra
  subroutines) library. It contains the FORTRAN function
  \begin{lstlisting}[language=Fortran,numbers=none]
    SUBROUTINE DAXPY( n, alpha, x, incx, y, incy)
  \end{lstlisting}
  which computes $\vy\gets\vy+\alpha\vx$ for double precision vectors
  of length $n$ (the increment arguments allow to skip elements). For
  usage in Python, there is a wrapper
  \begin{lstlisting}[language=Python,numbers=none]
    scipy.linalg.blas.daxpy(x, y[, n, a, offx, incx, offy, incy])
  \end{lstlisting}
\end{remark}

\begin{Algorithm*}{steepest-descent-python2}{Steepest descent with daxpy}
  \lstinputlisting{python/steepest-descent-axpy.py}
\end{Algorithm*}

\begin{Algorithm*}{minimal-residual-algol}{The minimal residual method}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vx,\vb\in\R^n$
    \State $\vr\gets \vb-\mata\vx$
    \Repeat
    \State $\alpha\gets\frac{\scal(\mata\vr,\vr)}{\scal(\mata\vr,\mata\vr)}$
    \State $\vx \gets \vx + \alpha\vr$
    \State $\vr \gets \vr-\alpha\mata\vr$
    \Until convergence
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{minimal-residual}
  Each step of the minimal method computes the minimum of
  $F(\vy) = \norm{\vb-\mata\vy}_2^2$ along the line from the current
  iterate $\vx^{(k)}$ in direction $\vr$. Furthermore, there holds
  \begin{gather}
    \vr^{(k+1)} \perp \mata \vr^{(k)}
  \end{gather}
\end{Lemma}

\begin{Lemma}{lucky-breakdown}
  The iteration sequences $\{\vx^{(k)}\}$ of the steepest descent and
minimal residual methods, respectively, are well defined except for
the case where $\vx^{(k)}$ is the exact solution $\vx$ and thus
$\vr^{(k)} = 0$.

We refer to this phenomenon as \define{lucky breakdown}, since the
method only fails after the exact solution has been found.
\end{Lemma}

\begin{Lemma*}{kantorovich-inequality}{Kantorovich inequality}
  Let $\mata\in\Rnn$ be symmetric, positive definite with minimal and
  maximal eigenvalues $\lambda_{\min}$ and $\lambda_{\max}$. Then, for
  $\vx\in\R^n$ there holds
  \begin{gather}
    \frac{\scal(\mata\vx,\vx)\scal(\mata^{-1}\vx,\vx)}{\scal(\vx,\vx)^2}
    \le \frac{(\lambda_{\min}+\lambda_{\max})^2}{4\lambda_{\min}\lambda_{\max}}
  \end{gather}
\end{Lemma*}

\begin{proof}
  See~\cite[Lemma 5.8]{Saad00}.
\end{proof}

\begin{Theorem}{steepest-descent-convergence}
  Let $\mata\in\Rnn$ be symmetric, positive definite with extremal
  eigenvalues $\lambda_{\min}$ and $\lambda_{\max}$. Then, the error
  $\ve^{(k)} = \vx-\vx^{(k)}$ of the steepest descent method admits
  the estimate
  \begin{gather}
    \norm{\ve^{(k+1)}}_A \le \rho \norm{\ve^{(k)}}_A,
  \end{gather}
  where the contraction number is
  \begin{gather}
    \rho
    = \frac{\lambda_{\max}-\lambda_{\min}}{\lambda_{\max}+\lambda_{\min}}
    = \frac{\cond_2\mata - 1}{\cond_2\mata+1}
    = 1-\frac2{\cond_2\mata+1}
  \end{gather}
\end{Theorem}

\begin{proof}
  See~\cite[Theorem 5.9]{Saad00}.
\end{proof}

\begin{Theorem}{minimal-residual-convergence}
  Let $\mata\in\Rnn$ such that its symmetric part $(\mata+\mata^T)/2$
  is positive definite. Then, the residuals $\vr^{(k)}$ of the minimal
  residual method admit the estimate
  \begin{gather}
    \norm{\vr^{(k+1)}}_2 \le \rho \norm{\vr^{(k)}}_2,
  \end{gather}
  where the contraction number is
  \begin{gather}
    \rho = \left(1-\frac{\mu^2}{\norm{\mata}^2}\right)^{\nicefrac12}
  \end{gather}
  and $\mu$ is the smallest eigenvalue of $(\mata+\mata^T)/2$.
\end{Theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Krylov spaces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Definition}{krylov-space}
  For a matrix $\mata\in\Rnn$ and a vector $\vv\in\R^n$, we define the
  \define{Krylov space}
  \begin{gather}
    \krylov_m = \krylov_m(\mata,\vv)
    = \spann{\vv,\mata\vv,\dots,\mata^{m-1}\vv}.
  \end{gather}
\end{Definition}

\begin{Definition}{grade-of-v}
  For a matrix $\mata\in\Rnn$ and a vector $\vv\in\R^n$ we define the
  \define{grade} of $\vv$, more precise the grade of $\vv$ with
  respect to $\mata$ as the minimal grade of a polynomial $p$ such
  that
  \begin{gather}
    p(\mata)\vv = 0
  \end{gather}
\end{Definition}

\begin{Lemma}{krylov-polynomial}
  Let $\mu$ be the grade of $\vv$ with respect to $\mata$.  The Krylov
  space $\krylov_m(\mata,\vv)$ is isomorphic to $\P_{m-1}$ if and only
  if $m\le \mu$. In this case, for any $\vw\in\krylov_m(\mata,\vv)$
  there is a unique polynomial $p\in\P_{m-1}$ such that
  \begin{gather}
    \label{eq:krylov-polynomial-1}
    \vw = p(\mata)\vv.
  \end{gather}

  The Krylov space $\krylov_m(\mata,\vv)$ is invariant under the
  action of $\mata$ if and only if $m\ge\mu$.
\end{Lemma}

\begin{proof}
  See \cite[Propositions 6.1 \& 6.2]{Saad00}.
  The definition of $\krylov_m(\mata,\vv)$ implies that any vector $\vw$ in this space has a representation
  \begin{gather}
    \vw = \sum_{i=0}^{m-1} \alpha_i \mata^i\vv =: p_{\vw}(\mata) \vv.
  \end{gather}
  Since the vectors in this sum span $\krylov_m$, its dimension is
  $m$ if and only if they are linearly independent, which in turn is
  the case if and only if there is a nonzero polynomial
  $p_0\in\P_{m-1}$ such that
  \begin{gather}
    p_0(\mata)\vv = 0.
  \end{gather}
  By assumption, such a polynomial exists if and only if
  $\mu<m$. Thus, we have proven that $\dim \krylov_m = m$ if and only
  if $m\le \mu$ and thus the mapping induced
  by~\eqref{eq:krylov-polynomial-1} is an isomorphism.

  On the other hand, $\krylov_m$ is invariant, if and only if the
  vectors spanning $\krylov_{m+1}$ are linearly dependent. We have
  just proven that this is the case if and only if $\mu<m+1$ or
  $\mu\le m$.
\end{proof}

\begin{Lemma}{krylov-projector}
  Let $\matq_m$ be a projector onto $\krylov_m=\krylov_m(\mata,\vv)$ and define
  \begin{align}
    \mata_m\colon \krylov_m &\to \krylov_m\\
    \vv&\mapsto \matq_m \mata.
  \end{align}
  Then, for any polynomial $q\in\P_{m-1}$ there holds
  \begin{gather}
    \label{eq:krylov-projector-1}
    q(\mata)\vv = q(\mata_m)\vv.
  \end{gather}
  For any polynomial $p\in\P_m$, there holds
  \begin{gather}
    \label{eq:krylov-projector-2}
    \matq_m p(\mata)\vv = p(\mata_m)\vv.
  \end{gather}
\end{Lemma}

\begin{proof}
  See \cite[Proposition 6.3]{Saad00}.  First, we
  prove~\eqref{eq:krylov-projector-1} by proving it for the monomial
  basis $q_i(x) = x^i$. It is obviously true for $q_0\equiv 1$. Now,
  we assume it is true for $q_i$ and conclude for $q_{i+1}$. To this
  end, we observe
  \begin{gather}
    q_{i+1}(\mata)\vv = \mata q_i(\mata)\vv = \mata q_i(\mata_m)\vv.
  \end{gather}
  Since $i<m-1$, this vector is in $\krylov_m$ and we obtain
  \begin{gather}
    q_{i+1}(\mata)\vv
    = \matq_m q_{i+1}(\mata)\vv
    = \matq_m \mata q_i(\mata_m)\vv
    = q_{i+1}(\mata_m)\vv,
  \end{gather}
  since $q_i(\mata_m)\vv\in\krylov_m$. Thus, we have proven the
  statement for $q\in\P_{m-1}$. The proof for $q_{m}(x) = x^m$ uses
  the same induction step, just with the exception that the action of
  $\matq_m$ on the left is not the identity anymore, since
  $q_m(\mata)\vv$ may not be in $\krylov_m$.
\end{proof}

\begin{todo}
  Reorganize as follows:
  \begin{enumerate}
  \item Arnoldi and Lanczos process here
  \item Derivation of cg from Lanczos
  \item General Arnoldi and GMRES
  \item Comments on Bi-Lanczos
  \end{enumerate}
\end{todo}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Arnoldi procedure and GMRES}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  We now develop algorithms which for a given matrix $\mata\in\Rnn$
  and initial vector $\vv\in\R^n$ construct the Krylov space
  $\krylov_m$ and an orthogonal basis dimension by dimension. This
  algorithm is named after Walter E. Arnoldi in the general case and
  after Cornelius Lanczos in the symmetric case. We begin with the
  Arnoldi process, which leads to the GMRES iteration in thsi section
  and continue with the conjugate gradient method, which is based on
  the Lanczos process in the next one.
\end{intro}

\begin{Algorithm*}{arnoldi-1}{Arnoldi with Gram-Schmidt}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$
    \For{$j=1,\dots,m$}
    \State $\vw_j \gets \mata\vv_j$
    \For{$i=1,\dots,j$}
    \State $h_{ij} \gets \scal(\mata\vv_j,\vv_i)$
    \State $\vw_j \gets \vw_j - h_{ij}\vv_i$
    \EndFor
    \State $h_{j+1,j} \gets \norm{\vw_j}_2$
    \If{$h_{j+1,j}=0$} \textbf{stop}\EndIf
    \State $\vv_{j+1} = \nicefrac{\vw_j}{h_{j+1,j}}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{arnoldi-krylov}
  Assume that the Arnoldi algorithm does not stop before step
  $m$. Then, the vectors $\vv_1,\dots,\vv_m$ form an orthonormal basis
  of $\krylov_m(\mata,\vv_1)$.
\end{Lemma}

\begin{proof}
  See \cite[Proposition 6.4]{Saad00}.
\end{proof}

\begin{Algorithm*}{arnoldi-householder}{Arnoldi with Householder}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$
    \State Choose $\matq_1$ such that $\matq_1\vv_1 = \ve_1$.
    \For{$j=1,\dots,m$}
    \State $\vv_j \gets \matq_1\dots\matq_j\ve_j$
    \State $\matq_{j+1}^*\matr \gets \matq_j\dots\matq_1(\vv_1,\mata\vv_1,\dots,\mata\vv_m)$
    \Comment{Householder}
    \If{$r_{j+1,j+1}=0$} \textbf{stop}\EndIf
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{arnoldi-householder}
  If the Arnoldi algorithm with Householder orthogonalization did not
  stop before step $m$, then the set $\vv_1,\dots,\vv_m$ is an
  orthonormal basis for $\krylov_m(\mata,\vv_1)$.
\end{Lemma}

\begin{proof}
  By line 4 of the algorithm, for $j=1,\dots,m$
  \begin{gather}
    \matq_1^*\dots\matq_{j+1}^* \matr = (\vv_1,\mata\vv_1,\dots,\mata\vv_j)
  \end{gather}
  is a QR factorization of the (rectangular) matrix on the
  right. Therefore, we can apply \slideref{Lemma}{qr-columns}, which
  readily implies the result.
\end{proof}

\begin{Theorem}{arnoldi-projection}
  Let $\matv_m = (\vv_1,\dots,\vv_m)$ be the matrix of basis vectors
  generated by the Arnoldi method and let
  $\overline{\matH}_m\in\R^{m+1\times m}$ be the Hessenberg matrix of
  entries $h_{ij}$ computed in the algorithm. Let further
  $\matH_m\in\R^{m\times m}$ be the same matrix without the last
  row. Then, there holds
  \begin{align}
    \label{eq:arnoldi-projection-1}
    \mata\matv_m &= \matv_m\matH_m+\vw_m\ve_m^T\\
    \label{eq:arnoldi-projection-2}
                 &= \matv_{m+1}\overline{\matH}_m
  \end{align}
  and
  \begin{gather}
    \label{eq:arnoldi-projection-3}
    \matH_m = \matv_m^T\mata\matv_m.
  \end{gather}
\end{Theorem}

\begin{proof}
  See also \cite[Proposition 6.5]{Saad00}.
  From lines 2--6 of \slideref{Algorithm}{arnoldi-1}, we deduce
  \begin{gather}
    \mata \vv_j = \vw_j + \sum_{i=1}^j h_{ij} \vv_i,
    \qquad j=1,\dots,m.
    =  \sum_{i=1}^{j+1} h_{ij} \vv_i,
  \end{gather}
  For $j<m$, we use line 9 to obtain
  \begin{gather}
    \mata \vv_j =  \sum_{i=1}^{j+1} h_{ij} \vv_i.
  \end{gather}
  Since $\mata \vv_j$ is the $j$th column of $\mata\matv_m$ and
  $\matH_m$ is Hessenberg, we can rewrite this as
  \begin{gather}
    \mata\matv_m\ve_j = \matv_m\matH\ve_j,\qquad j=1,\dots,m-1.
  \end{gather}
  For $j=m$, we cannot replace $\vw_m$, therefore,
  \begin{gather}
    \mata\matv_m\ve_m = \matv_m\matH\ve_m + \vw_m.
  \end{gather}
  Using $\ve_m^T\ve_j=0$ for $j<m$, we can combine these to the matrix
  representation
  \begin{gather}
    \mata\matv_m = \matv_m\matH + \vw_m\ve_m^T.
  \end{gather}
  Adding the vector $\vv_{m+1}$ from the next step of the algorithm,
  we get the alternative formulation~\eqref{eq:arnoldi-projection-2}.
  In order to prove~\eqref{eq:arnoldi-projection-3}, we multiply from
  the left with $\matv_m^T$. The proof is finished by observing that
  $\matv_m^T\matv_m$ is the identity on $\R^m$ and $\matv_m^T\vw_m=0$.
\end{proof}

\begin{remark}
  The next step of the Arnoldi method can always be computed if
  $h_{j+1,j}\neq 0$ in line 9. If it cannot be, we speak of a
  \define{breakdown} of the method.
\end{remark}

\begin{Lemma}{arnoldi-breakdown}
  The Arnoldi method stops at step $j$ if and only if the subspace $\krylov_j$ is invariant under $\mata$ or equivalently, the minimal polynomial of $\vv_1$ is of degree $j$.
\end{Lemma}

\begin{proof}
  The algorithm stops in line 8 if and only if $\mata\vv_j$ is a
  linear combination of the previously computed basis vectors, such
  that $\vw_j=0$ in line 5 at the end of the loop. This happens
  exactly when the Krylov space is invariant.
\end{proof}

\begin{todo}
  Put this into an appendix
\end{todo}

\begin{Theorem}{normal-equations}
  Let $\mata\in\R^{m\times n}$ with $m\ge n$ and $\vb\in\R^m$. Then, a vector $\vx$ is the solution to the minimization problem
  \begin{gather}
    \norm{\vb-\mata\vx}_2 \stackrel{!}{=} \min,
  \end{gather}
  if an only if $\vx$ solves the \define{normal equations}
  \begin{gather}
    \mata^T\mata\vx = \mata^T \vb.
  \end{gather}
\end{Theorem}

\begin{Lemma}{oblique-normal}
  Let $\mata\in\Rnn$ be invertible. Let $K$ be a subspace of $\Rnn$. Then, the oblique projection
  \begin{gather}
    \vv\in K, \qquad \vb-\mata\vv \perp \mata K,
  \end{gather}
  and the orthogonal projection
  \begin{gather}
    \vw\in K, \qquad \mata^T \vb-\mata^T\mata\vw \perp K,
  \end{gather}
  are identical.
\end{Lemma}

\begin{proof}
  Homework.
\end{proof}

\begin{todo}
  Write in incremental form? Add pseudocode version in incremental form?
\end{todo}

\begin{Algorithm*}{gmres}{Generalized Minimal Residual}
  Given $\vx^{(0)}\in\R^n$, the ``iterate'' $\vx^{(m)}$ of the
  \define{GMRES} method is computed by the following steps.
  \begin{enumerate}
  \item Compute the Arnoldi basis $\matv_{m+1}$ of
    $\krylov_{m+1}(\mata,\vr^{(0)})$ and the matrix $\overline\matH_m\in\R^{m+1\times m}$.
  \item Solve the least squares problem
    \begin{gather}
      \norm{\beta\ve_1 - \overline\matH_m \vy_m}_2 \stackrel{!}{=} \min
    \end{gather}
    in $\R^m$ with $\beta = \norm{\vr^{(0)}}_2$.
  \item Let
    \begin{gather}
      \vx^{(m)} = \vx^{(0)} + \matv_{m}\vy_m
    \end{gather}
  \end{enumerate}
\end{Algorithm*}

\begin{Theorem}{gmres-projection}
  The GMRES method computes the update of the $m$-dimensional oblique
  projection step. It minimizes the residual over the search space
  $\vx^{(0)} +\krylov_m(\mata,\vr^{(0)}$ and there holds
  \begin{gather}
    \label{eq:gmres-norm}
    \norm{\vr^{(m)}}_2 = \norm{\vb-\mata\vx^{(m)}}_2 =
    \norm{\beta\ve_1 - \overline\matH \vy_m}_2.
  \end{gather}
\end{Theorem}

\begin{proof}
  We use the equivalence between minimizing the residual and the
  solution to the normal equations, namely
  \slideref{Theorem}{normal-equations}. According to
  \slideref{Theorem}{arnoldi-projection}, we obtain
  \begin{gather}
    \matv_m^T\mata^T\mata\matv_m
    = \overline\matH_m^T \matv_{m+1}^T\matv_{m+1}\overline\matH_m
    = \overline\matH_m^T\overline\matH_m.
  \end{gather}
  Thus, $\overline\matH_m^T\overline\matH_m$ is the Galerkin
  projection of $\mata^T\mata$ to the subspace generated by
  $\matv_m$. Furthermore, it is symmetric, positive definite, the
  latter assuming $\mata$ is invertible.  Furthermore,
  $\vr^{(0)} = \vb-\mata\vx^{(0)} = \beta\matv_{m+1}\ve_1$, such that
  \begin{gather}
    \matv_m^T\mata^T\vr^{(0)} = \beta\overline\matH_m \ve_1.
  \end{gather}
  Thus, the linear system
  \begin{gather}
    \overline\matH_m^T\overline\matH_m \vy_m = \beta\overline\matH_m \ve_1
  \end{gather}
  is the (orthogonal) Galerkin projection of the normal equations to
  the search space. On the other hand, its solution solves the
  minimization problem in the GMRES algorithm.

  By the minimization property of the orthogonal projection, the error
  after the projection step is minimal in the $\mata^T\mata$ norm,
  which is equal to the Euclidean norm of the residual.

  Finally, we have
  \begin{align}
    \vr^{(m)}
    &= \vb-\mata\vx^{(0)} - \mata\matv_m\vy_m\\
    &=\beta\matv_{m+1}\ve_1 - \matv_{m+1}\overline\matH_m\vy_m\\
    &= \matv_{m+1}\left(\beta\ve_1 - \overline\matH_m\vy_m\right).
  \end{align}
  Since $\matv_{m+1}$ is orthogonal, this implies that
  \begin{gather}
    \norm{\vr^{(m)}}_2 = \norm{\beta\ve_1 - \overline\matH_m\vy_m}_2
  \end{gather}
\end{proof}

\begin{remark}
  \slideref{Algorithm}{gmres} should not be implemented as it is
  presented, but rather in the fashion of an iterative method, where
  $m$ is increased step by step. To this end, we observe that the
  generation of the Arnoldi basis proceeds incrementally, adding new
  vectors, but not changing the previous ones.

  Therefore, $\overline\matH_{m+1}$ differs from $\overline\matH_{m}$
  only by adding an additional row and column. Then, the least-squares
  system is solved by computing the QR factorization of
  $\overline\matH_{m+1}$, which again amounts to updating the last
  column only.
\end{remark}

\begin{remark}
  Equation~\eqref{eq:gmres-norm} implies in particular, that the norm
  of the residual in the GMRES method can be computed without
  computing a ``big'' matrix-vector operation $\mata\vx^{(m)}$, but is
  indeed the misfit of the $m$-dimensional least-squares problem for
  $\vy_m$. This implies on the other hand, that even the solution
  $\vx^{(m)}$ should only be computed as soon as the resiudal is
  sufficiently small.
\end{remark}

\begin{Lemma}{gmres-breakdown}
  If the GMRES method breaks down at step $m$, then
  $\vr^{(m)}=0$. Thus, the GMRES method only encounters a
  \putindex{lucky breakdown}.
\end{Lemma}

\begin{remark}
  The GMRES method encounters a problem if convergence is slow: the
  effort for computing the last basis vector in $\matv_m$ is of order
  $mn$, the effort for solving the least-squares problem is
  $m^3$. Similarly, the storage requirement for $\matv_m$ is $mn$ and
  for $\overline \matH_m$, it is $m^2$. Thus, the algorithm is
  feasible for large sparse matrices only if $m\ll n$.

  Therefore, we introduce two variants of the GMRES algorithm:
  \begin{itemize}
  \item Restarted GMRES simply deletes the whole basis $\matv_m$ and the matrix $\overline\matH_m$ every $k$-th step starts fresh.
  \item Truncated GMRES orthogonalizes only with respect to the last
    $k$ basis vectors.
  \end{itemize}
\end{remark}

\begin{Algorithm*}{gmres-restart}{Restarted GMRES}
  \begin{enumerate}
  \item Choose a maximal dimension $m$ of the Krylov space
  \item Begin with an initial guess $\vx^{(0)}$.
  \item For $k>0$, given $\vx^{(k)}$, perform the GMRES method with
    basis size $m$ to obtain $\vx^{(k+m)}$.
  \item Check the stopping criterion as usual inside the GMRES method
  \end{enumerate}
\end{Algorithm*}

\begin{Algorithm*}{gmres-truncated}{Truncated GMRES}
  Run the GMRES method as usual, but in the Arnoldi process, only
  orthogonalize with respect to the last $k$ vectors for fixed $k$.
\end{Algorithm*}

\begin{remark}
  Note that both modifications destroy the minimization property of the method.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Lanczos procedure and the conjugate gradient method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Theorem}{arnoldi-linear-system}
  Given an initial vector $\vx^{(0)}$ and
  $\vr^{(0)} = \vb - \mata \vx^{(0)}$. Let $\beta=\norm{\vr^{(0)}}$
  and choose $\vv_1 =\nicefrac{\vr^{(0)}}{\beta}$ in Arnoldi's
  method. Then, the Galerkin approximation
  \begin{align}
    x^{(m)} &\in x^{(0)} + \krylov_m(\mata,r^{(0)})\\
    b-\mata x^{(m)} &\perp \krylov_m(\mata,r^{(0)}),
  \end{align}
  is obtained in two steps. Solve in $\R^m$
  \begin{gather}
    \matH_m\vy_m = \beta\ve_1,
  \end{gather}
  and let
  \begin{gather}
    \vx^{(m)} = \vx^{(0)} + \matv_m \vy_m.
  \end{gather}
\end{Theorem}

\begin{todo}
  Move this to Lanczos as it is irrelevant here
\end{todo}

\begin{Theorem}{arnoldi-linear-residual}
  Let $\vx^{(m)}$ be the Galerkin solution in
  \slideref{Theorem}{arnoldi-linear-system}. Then, there holds
  \begin{gather}
    \vb-\mata\vx^{(m)} = -h_{m+1,m} \ve_m^T \vy_m \vv_{m+1},
  \end{gather}
  and therefore
  \begin{gather}
    \norm{\vb-\mata\vx^{(m)}}_2 = h_{m+1,m} \abs{\ve_m^T \vy_m}.
  \end{gather}  
\end{Theorem}

\begin{proof}
  See also~\cite[Proposition 6.7]{Saad00}
  We use the definition of the residual of $\vx^{(m)}$ and the representation of \slideref{Theorem}{arnoldi-projection} to obtain
  \begin{align}
    \vr^{(m)} = b-\mata\vx^{(m)}
    &= b- \mata\left(\vx^{(0)} + \matv_m\vy_m\right)\\
    &=\vr^{(0)} - \mata\matv_m\vy_m\\
    &= \beta\vv_1 - \matv_m\matH_m\vy_m - \vw_m\ve_m^T\vy_m.
  \end{align}
  Since $\vy_m$ solves the projected system
  $\matH_m\vy_m = \beta\ve_1$, there holds
  \begin{gather}
    \beta\vv_1 - \matv_m\matH_m\vy_m = \beta\vv_1 - \beta \matv_m\ve_1 = 0.
  \end{gather}
  Furthermore, we rewrite
  \begin{gather}
    \vw_m\bigl(\ve_m^T\vy_m\bigr) = h_{m+1,m} \bigl(\ve_m^T\vy_m\bigr) \vv_{m+1}.
  \end{gather}
\end{proof}

\begin{remark}
  Following \slideref{Theorem}{projected-invertible} this algorithm
  works reliably only for $\mata$ symmetric, positive definite. This
  is the case we study here and we will improve the algorithm
  considerably.
\end{remark}

\begin{Lemma}{arnoldi-symmetric}
  If the matrix $\mata$ is symmetric, the vectors of the Arnoldi
  procedure admit the three-term recurrence relation
  \begin{gather}
    \delta_{j+1}\vv_{j+1} = \mata \vv_j - \gamma_j \vv_j - \delta_j\vv_{j-1}
  \end{gather}
  with $\vv_0 = 0$, $\delta_1 = 0$ and
  \begin{gather}
    \gamma_j =  h_{jj}, \qquad \delta_j = h_{j-1,j},
    \qquad j=1,\dots
  \end{gather}
  In particular, explicit orthogonalization is only necessary with respect to
  the previous two basis vectors.
\end{Lemma}

\begin{todo}
  Add reference to QR method for symmetrix matrices.
\end{todo}
\begin{proof}
  See also~\cite[Section 6.6.1]{Saad00}. From
  \slideref{Theorem}{arnoldi-projection}, we realize that the
  projected matrix $\matH_m$ of the Arnoldi method is symmetric if
  $\mata$ is symmetric. Since it is Hessenberg by construction, we
  conclude that it is tridiagonal. Since it is also the matrix of
  coefficients in the Gram-Schmidt orthogonalization, the statement
  becomes evident.
\end{proof}

\begin{Algorithm*}{lanczos}{Lanczos}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$, $\vv_0=0$, $\delta_1=0$.
    \For{$j=1,\dots,m$}
    \State $\vw_j \gets \mata\vv_j - \delta_j \vv_{j-1}$
    \State $\gamma_{j} \gets \scal(\vw_j,\vv_j)$
    \State $\vw_j \gets \vw_j - \gamma_j\vv_i$
    \State $\delta_{j+1} \gets \norm{\vw_j}_2$
    \If{$\delta_{j+1}=0$} \textbf{stop}\EndIf
    \State $\vv_{j+1} = \nicefrac{\vw_j}{\delta_{j+1}}$
    \EndFor
  \end{algorithmic}  
\end{Algorithm*}

\begin{Lemma*}{lanczos-linear}{Lanczos solver}
  The Galerkin approximation   \begin{align}
    x^{(m)} &\in x^{(0)} + \krylov_m(\mata,r^{(0)})\\
    b-\mata x^{(m)} &\perp \krylov_m(\mata,r^{(0)}),
  \end{align}
  is obtained as $\vx^{(m)} = \vx^{(0)} + \matv_m \vy_m$, where $\matv_m$ is the basis obtained from the Lanczos process and $\vy_m$ solves
  \begin{gather}
    \matH_m\vy_m = \beta\ve_1,
    \qquad \matH_m =
    \begin{pmatrix}
      \gamma_1 & \delta_2\\
      \delta_2 & \gamma_2 & \ddots\\
      &\ddots&\ddots&\delta_m\\
      &&\delta_{m}&\gamma_m
    \end{pmatrix}.
  \end{gather}
  For the residual, there holds
  \begin{gather}
    \vr^{(m)} = \vb-\mata\vx^{(m)} = -\delta_{m+1} \ve_m^T \vy_m \vv_{m+1}.
  \end{gather}
\end{Lemma*}

\begin{proof}
  These are the statements of
  \slideref{Theorem}{arnoldi-linear-system} and
  \slideref{Theorem}{arnoldi-linear-residual}. Since the Lanczos
  process is just a specialization of the Arnoldi process, they still
  hold.
\end{proof}

\begin{remark}
  The main difference between Arnoldi and Lanczos is the fact that
  every step of the Lanczos process onle requires the previous two
  vectors, while Arnoldi requires storing the whole history.

  This implies, that Lanczos itself is much closer to the concept of
  an iterative method than Arnoldi, weren't it for the computation of
  $\vy_m$.

  We will now develop a new algorithm based on the observation that
  also the corrections for $\vx^{(m)}$ can be computed incrementally.
\end{remark}

\begin{Lemma}{lanczos-incremental}
  Given the vector $\vx^{(m)}$ in the Lanczos solver, the vector
  $\vx^{(m+1)}$ can be computed incrementally by an update of the form
  \begin{gather}
    \vx^{(m+1)} = \vx^{(m)} + \zeta_{m+1}\vg^{(m+1)},
  \end{gather}
  where the vector $\vg^{(m)}$ itself is computed only using the
  vectors $\vg^{(m-1)}$ and $\vv^{(m)}$.
\end{Lemma}

\begin{proof}
  See also \cite[Section 6.7.1]{Saad00}. First, we compute the LU factorization of $\matH_m$ as
  \begin{gather}
    \matH_m =
    \begin{pmatrix}
      1\\\lambda_2&1\\
      &\ddots&\ddots\\
      &&\lambda_m&1
    \end{pmatrix}
    \times
    \begin{pmatrix}
      \eta_1&\delta_2\\
      &\ddots&\ddots\\
      &&\eta_{m-1}&\delta_m\\
      &&&\eta_m
    \end{pmatrix},
  \end{gather}
  where
  \begin{gather}
    \lambda_k = \frac{\delta_k}{\eta_{k-1}},
    \qquad
    \eta_k = \gamma_k - \lambda_k \delta_k.
  \end{gather}
  We note that LU decomposition works from top to bottom and left to
  right, such that already processed rows do not change when adding
  more steps to the algorithm.

  Now we introduce matrices and vectors
  \begin{gather}
    \matg_m = \matv_m \matu_m^{-1} \in \R^{n\times m},
    \qquad \vz_m = \matl_m^{-1} \beta \ve_1\in \R^m,
  \end{gather}
  such that
  \begin{align}
    \vx^{(m)}
    &= \vx^{(0)} + \matv_m \matu_m^{-1} \matl_m^{-1} \beta \ve_1\\
    &= \vx^{(0)} + \matg_m \vz_m
  \end{align}
  With $\zeta_k$ following the recursion
  $\zeta_k = - \lambda_k \zeta_{k-1}$, we realize
  \begin{gather}
    \vz_{m+1} =
    \begin{pmatrix}
      \zeta_1\\\vdots\\\zeta_{m+1}
    \end{pmatrix}
    =
    \begin{pmatrix}
      \vz_{m}\\\zeta_{m+1}
    \end{pmatrix}.
  \end{gather}
  Let $\vg_k$ be the column vectors of $\matg_m$. Writing the
  definition of $\matg_m$ in the form $\matg_m \matu_m = \matv_m$, we
  obtain the relation
  \begin{gather}
    \label{eq:nla:krylov:lanczos-p}
    \vg_k = \frac1{\eta_k}\bigl(\vv_k - \delta_k \vg_{k-1}\bigr).
  \end{gather}
  Thus,
  \begin{gather}
    \matg_{m+1} =
    \begin{pmatrix}
      \vg_1,\dots,\vg_{m+1}
    \end{pmatrix}
    =
    \begin{pmatrix}
      \matg_{m},\vg_{m+1}
    \end{pmatrix}.
  \end{gather}
  There holds
  \begin{align}
    \vx^{(m+1)}
    &= \vx^{(0)} + \matg_{m+1}\vz_{m+1}\\
    &= \vx^{(0)} + \sum_{k=1}^{m+1} \zeta_k \vg_k,\\
    &= \vx^{(m)} + \zeta_{m+1} \vg_{m+1}.
  \end{align}
\end{proof}

\begin{remark}
  Thus, we have proven a simple update formula for the iterates
  $\vx^{(m)}$ based on an additional vector $\vg_m$, which is again
  obtained by an update formula.

  We have used an LU decomposition without pivoting though in the
  derivation of this algorithm, which might suffer from numerical
  instability. Therefore, we will now analyze the main properties of
  the sequences $\vv_m$, $\vg_m$ and $\vr_m$ and then write a new
  algorithm to obtain these sequences.
\end{remark}

\begin{Lemma}{lanczos-orthogonality}
  The sequences of vector $\vr_m = \vb-\mata\vx^{(m)}$ and $\vg_m$
  constructed in the preceding lemma have the following orthogonality
  properties:
  \begin{xalignat}2
    \scal(\vr_i,\vr_k) &= 0 & i&\neq k\\
    \scal(\mata\vg_i,\vg_k) &= 0 & i&\neq k.
  \end{xalignat}
\end{Lemma}

\begin{proof}
  Orthogonality of the residuals follows from
  \slideref{Lemma}{lanczos-linear}, since $\vr_m$ is aligned with
  $\vv_{m+1}$.

  The second orthogonality relation is equivalent to the fact that
  $\matg_m^T\mata\matg_m$ is diagonal.
  \begin{align}
    \matg_m^T\mata\matg_m
    &= \matu_m^{-T}\matv_m^T\mata\matv_m\matu_m^{-1}\\
    &= \matu_m^{-T}\matH_m\matu_m^{-1}\\
    &= \matu_m^{-T}\matl_m.
  \end{align}
  The first term is symmetric.  The last is the product of two lower
  triangular matrices and thus lower triangular. Thus, the matrix must
  be diagonal.
\end{proof}

\begin{Algorithm*}{cg}{Conjugate Gradient Method}
  \begin{algorithmic}[1]
    \State $\vr_0 = b-\mata \vx_0$
    \State $\vp_0 = \vr_0$
    \For{$j=0,1,\dots$}
    \State $\alpha_j = \frac{\scal(\vr_j,\vr_j)}{\scal(\mata\vp_j,\vp_j)}$
    \State $\vx_{j+1} = \vx_j + \alpha_j \vp_j$
    \State $\vr_{j+1} = \vr_j - \alpha_j \mata \vp_j$
    \State $\beta_j =\frac{\scal(\vr_{j+1},\vr_{j+1})}{\scal(\vr_j,\vr_j)}$
    \State $\vp_{j+1} = r_{j+1} + \beta_j \vp_j$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{cg-orthogonality}
  The sequences $\vx_j$ and $\vr_j$ produced by the cg method coincide
  with those of the Lanczos solver (in exact arithmetic). The vectors
  $\vp_j$ of the cg method coincide up to scalar factors with the vectors $\vg_{j+1}$
  in \slideref{Lemma}{lanczos-incremental}.
\end{Lemma}

\begin{todo}
  The structure of the argument is not very clear. We are jumping
  back and forth between the algorithms. It would be better to show
  that CG produces the same orthogonality with respect to the
  previous vectors, then use uniqueness. Where to put equality of
  the coefficients?
\end{todo}
  
\begin{proof}
  Note that the vectors $\vp_j$ refers to the cg algorithm, the
  vectors from \slideref{Lemma}{lanczos-incremental} will be
  distinguished clearly.

  We start with the update formula
  $\vx_{j+1} = \vx_j + \alpha_j \vp_j$, which immediately implies
  \begin{gather}
    \vr_{j+1} = \vr_j - \alpha_j \mata\vp_j.
  \end{gather}
  Orthogonality between $\vr_{j+1}$ and $\vr_j$ due to the projection
  property implies the choice
  \begin{gather}
    \alpha_j = \frac{\scal(\vr_j,\vr_j)}{\scal(\mata\vp_j,\vr_j)}.
  \end{gather}
  Since $\vr_j$ is a multiple of $\vv_{j+1}$ by the residual
  representation in \slideref{Theorem}{arnoldi-linear-residual}, the
  update formula~\eqref{eq:nla:krylov:lanczos-p} for $\vg_{j+1}$
  (after index shift) implies that the direction is a linear
  combination of $\vr_{j+1}$ and $\vp_j$. After arbitrary scaling,
  \begin{gather}
    \vp_{j+1} = \vr_{j+1} + \beta_j \vp_{j}.
  \end{gather}
  A-orthogonality implies the choice
  \begin{gather}
    \beta_j = -\frac{\scal(\mata \vp_j,\vr_{j+1})}{\scal(\mata \vp_j,\vp_j)}.
  \end{gather}
  Note that in each step, the orthogonality relation determines $\vp_{j+1}$ uniquely
  up to scaling in this subspace. After the scaling has been chosen,
  the orthogonality relation for $\vr_{j+1}$ uniquely
  determines the update for $\vx_{j+1}$, such that the sequences
  indeed coincide.

  The definition of the coefficients still differs from the cg
  algorithm. But, we observe
  \begin{gather}
    \scal(\mata\vp_j,\vr_j) = \scal(\mata\vp_j,\vp_j-\beta_{j-1}\vp_{j-1})
    = \scal(\mata\vp_j,\vp_j).
  \end{gather}
  Similarly, from the update formula for the residual, we get
  \begin{gather}
    \mata\vp_j = \frac1{\alpha_j}\bigl(\vr_j-\vr_{j+1}\bigr),
  \end{gather}
  and thus,
  \begin{gather}
    \beta_j = \frac1{\alpha_j}\frac{\scal(\vr_{j+1},\vr_{j+1}-\vr_j)}{\scal(\mata\vp_j,\vp_j)} = \frac{\scal(\vr_{j+1},\vr_{j+1})}{\scal(\vr_{j},\vr_{j})}.
  \end{gather}
  Note that these forms avoid inner products of possibly almost
  orthogonal vectors.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convergence analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  Here, we will develop a sequence of error estimates for the
  conjugate gradient method based on its optimality as a projection
  method stated in \slideref{Theorem}{projection-orthogonal-optimal}. These
\end{intro}

\begin{Theorem}{cg-optimality}
  For the error $\ve^{(k)} = \vx^{(k)}-\vx$ after $k$ steps of the cg
  method there holds
  \begin{align}
    \norm{\ve^{(k)}}_A
    & = \min_{p\in \P_{k-1}}
      \norm{\ve^{(0)} + \mata p(\mata) \ve^{(0)}}_A\\
    &= \min_{\substack{p\in\P_k\\p(0) = 1}} \norm{p(\mata)\ve^{(0)}}_A.
  \end{align}
\end{Theorem}

\begin{proof}
  There holds $\vx^{(k)} = \vx^{(0)} + \vw$ with
  $\vw \in \krylov_k(\mata,\vr^{(0)})$. Then, by
  \slideref{Lemma}{krylov-polynomial}, there is $p\in\P_{k-1}$ such
  that $\vw = -p(\mata) \vr^{(0)}$. Thus
  \begin{align}
    \vx^{(k)} - \vx
    &= \vx^{(0)} - \vx - p(\mata)\vr^{(0)}\\
    &= \vx^{(0)} - \vx - p(\mata) (\vb-\mata\vx^{(0)})\\
    &= \vx^{(0)} - \vx - p(\mata) (\mata\vx-\mata\vx^{(0)})\\
    &= \vx^{(0)} - \vx - \mata p(\mata) (\vx^{(0)}-\vx).
  \end{align}
  On the other hand, we have the optimality of projection methods in
  \slideref{Theorem}{projection-orthogonal-optimal}, which states
  \begin{align}
    \norm{\ve^{(k)} -\vx}_A
    &= \min_{\vw\in \krylov_k(\mata,\vr^{(0)})} \norm{\vx^{(0)}+\vw-\vx}_A\\
    &= \min_{\vw\in \krylov_k(\mata,\vr^{(0)})} \norm{\ve^{(0)}+\vw}_A.
  \end{align}
  Thus, we obtain the first expression. The second follows by noticing
  that the set of polynomials $q(x) = 1+xp(x)$ with $p\in\P_{k-1}$ is
  just the set of polynomials in $\P_k$ with coefficient $a_0 = 1$.
\end{proof}

\begin{Corollary}{cg-vs-descent}
  The error after $k$ steps of the conjugate gradient method is never
  greater than the error after equally many steps of the steepest
  descent method.
\end{Corollary}

\begin{proof}
  The error of the steepest descent method is
  \begin{gather}
    \ve^{(k)} = \prod_{i=k}^{1} (\id-\alpha_k\mata) \ve^{(0)}.
  \end{gather}
  This can be rewritten as
  \begin{gather}
    \ve^{(k)} = p_k(\mata) \ve^{(0)},
  \end{gather}
  where $p_k\in\P_k$ is the polynomial of degree $k$ defined by
  \begin{gather}
    p_k(x) = \prod_{i=k}^{1} (1-\alpha_kx).
  \end{gather}
  There holds $p_k(0)=1$ and thus this polynomial is in the set over
  which the conjugate gradient method minimizes.
\end{proof}

\begin{Corollary}{cg-optimality-spectrum}
  Let $\sigma(\mata) = \{\lambda_1,\dots,\lambda_n\} \subset \R^+$ be
  the spectrum of $\mata$. Then, the error after $k$ steps of the
  conjugate gradient method follows the estimate
  \begin{gather}
    \norm{\ve^{(k)}}_A \le \rho_k \norm{\ve^{(0)}}_A,
  \end{gather}
  where
  \begin{gather}
    \rho_k = \min_{\substack{p\in\P_k\\p(0) = 1}} \max_{\lambda\in\sigma(\mata)} \abs{p(\lambda)}.
  \end{gather}
\end{Corollary}

\begin{proof}
  First, since we apply the cg iteration to s.p.d. matrices, there is
  an orthogonal basis $\{\vv_1,\dots,\vv_n\}$ of eigenvectors of
  $\mata$ with real, positive eigenvalues $\lambda_i$. Thus, we can
  write with some coefficients $\alpha_i$:
  \begin{gather}
    \ve^{(0)} = \sum_{i=1}^n \alpha_i \vv_i,
    \qquad
    \ve^{(k)} = \sum_{i=1}^n \alpha_i p(\lambda_i) \vv_i.    
  \end{gather}
  Hence, we can estimate
  \begin{align}
    \norm{\ve^{(k)}}_A^2
    &= \sum_{i=1}^n \lambda_i\alpha_i^2 p(\lambda_i)^2 \norm{\vv_i}_A^2\\
    & \le \max_i p(\lambda_i)^2 \sum_{i=1}^n \lambda_i\alpha_i^2\norm{\vv_i}_A^2\\
    &= \max_i p(\lambda_i)^2 \norm{\ve^{(0)}}_A^2.
  \end{align}
  The statement now follows from \slideref{Theorem}{cg-optimality}.
\end{proof}

\begin{remark}
  It is important to note that the preceding corollary is close to
  optimal. In particular, it can exploit any information which is
  known about the spectrum. In particular, the following statements are true:
  \begin{enumerate}
  \item If the matrix $\mata$ only has $k$ distinct eigenvalues, then
    the cg iteration converges to the solution in $k$ steps.
  \item If the eigenvalues are clustered around $k$ values, then the
    gain in the first $k$ steps will be large.
  \item If the spectrum consists of a continuum plus a single outlier,
    the method will perform almost as if the outlier did not exist.
  \end{enumerate}
  The following corollary now deals with the case that the spectrum
  has no particular structure, but fills a continuous interval
  somewhat evenly.
\end{remark}

\begin{Corollary}{cg-condition-number}
  The error after $k$ steps of the cg method admits the estimate
  \begin{gather}
    \norm{e^{(k)}}_A \le 2 \left(
      \frac{\sqrt{\cond_2(A)}-1}{\sqrt{\cond_2(A)}+1}\right)^k
    \norm{e^{(0)}}_A.
  \end{gather}
\end{Corollary}

\begin{proof}
  We make no further assumtions on the spectrum of $\mata$ except that
  all eigenvalues are in the interval
  $[\lambda_{\min},\lambda_{\max}]$. Thus, we estimate the factors $\rho_k$ in \slideref{Corollary}{cg-optimality-spectrum} by
  \begin{gather}
    \rho_k \le \min_{\substack{p\in\P_k\\p(0) = 1}}
    \max_{\lambda\in[\lambda_{\min},\lambda_{\max}]} \abs{p(\lambda)}.
  \end{gather}
  
  We use the optimality property of Chebyshev polynomials stated in
  \slideref{Corollary}{chebyshev-minimal-2} and note that the
  optimality stated there is exactly what we need. Thus,
  $p = \widehat\pchebyshev_k(\lambda)$ solves the minimization
  problem and there holds
  \begin{align}
    \rho_k
    &\le \left(\pchebyshev_k\left(
      \frac{\lambda_{\max}+\lambda_{\min}}{\lambda_{\max}-\lambda_{\min}}
      \right)\right)^{-1}\\
    & \le \left(\frac{\sqrt{\lambda_{\max}}-\sqrt{\lambda_{\min}}}{\sqrt{\lambda_{\max}}+\sqrt{\lambda_{\min}}}
      \right)^k
  \end{align}
  Dividing both sides of the fraction by $\sqrt{\lambda_{\min}}$ yields the result.
\end{proof}

\begin{remark}
  Note that the estimate in \slideref{Corollary}{cg-condition-number}
  is not a classical contraction estimate which allows us to deduce
  the error reduction from step $k$ to $k+1$. Indeed, the factor 2
  before the quotient renders this exercise impossible, and the
  estimate for the steepest descent method is our rescue.

  Instead, the estimate shows that with growing dimension of the
  Krylov space, eventually the factor 2 is compensated by the first
  $k$ powers of the quotient and then the additional powers guarantee
  faster convergence.
\end{remark}


\begin{Theorem}{gmres-optimality}
  For the residual $\vr^{(k)} = \vb-\mata\vx^{(k)}$ after $k$ steps of
  the GMRES method, there holds
  \begin{gather}
    \norm{\vr^{(k)}}_2 = \min_{\substack{p\in\P_k\\p(0) = 1}}
    \norm{p(\mata)\vr^{(0)}}_2.
  \end{gather}
  If the matrix $\mata$ is diagonalizable, then there holds
    \begin{gather}
    \norm{\vr^{(k)}}_2 \le \rho_k \cond_2(\matv) \norm{\vr^{(0)}}_2,
  \end{gather}
  where $\matv$ is the matrix of eigenvectors and
  \begin{gather}
    \rho_k = \min_{\substack{p\in\P_k\\p(0) = 1}} \max_{\lambda\in\sigma(\mata)} \abs{p(\lambda)}.
  \end{gather}
\end{Theorem}

\begin{proof}
  See \cite[Proposition 6.32]{Saad00}.
\end{proof}

\begin{Corollary}{gmres-estimate}
  Let $\mata$ be diagonalizable such that its spectrum is contained in
  an ellipse in the complex plane with center $c$, focal distance $d$
  and semimajor axis $a$. Then,
  \begin{gather}
     \rho_k \approx \left(
      \frac{a+\sqrt{a^2-d^2}}{c+\sqrt{c^2-d^2}}
      \right)^k.
  \end{gather}
\end{Corollary}

\begin{proof}
  See \cite[Corollary 6.33]{Saad00}.
\end{proof}

\begin{remark}
  This theorem is analogue to \slideref{Theorem}{cg-optimality} and
  \slideref{Corollary}{cg-optimality-spectrum} and the proofs are
  identical, based on \slideref{Lemma}{krylov-polynomial} and
  \slideref{Theorem}{projection-oblique-optimal}.

  Note that the step from minimizing over $p(\mata)$ to $p(\lambda)$
  requires that the matrix $\mata$ is diagonalizable and produces
  optimal results only if it is normal. While this condition is not
  strictly necessary, there are examples of arbitrarily bad
  convergence. Indeed, for every nonincreasing sequence of $n-1$
  numbers, you can find a sequence such that the norms of residuals of
  the GMRES method will follow that
  sequence~\cite{GreenbaumPtakStrakos96}.
\end{remark}

\begin{Theorem}{gmres-pos-def}
  If $\mata$ is positive definite, then the GMRES method as well as
  the restarted GMRES method converge.
\end{Theorem}

\begin{proof}
  See \cite[Theorem 6.30]{Saad00}.
\end{proof}

\begin{remark}
  The main practical difference between the conjugate gradient and
  GMRES methods is the short recurrence. While running hundreds of
  steps of cg is not an issue, GMRES becomes infeasible beyond some 30
  steps due to the extensive storage and the explicit
  orthogonalization. There has been research into finding solutions
  for this problem, but having both optimization and short recurrence
  is impossible in general (in your homework you saw an example of
  nonsymmetric matrices where it is possible).
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bi-Lanczos and BiCG-stab}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  The problem of short recurrence for nonsymmetric matrices has been
  addressed by biorthogonalization methods. They go back to
  \slideref{Lemma}{projection-basis} in the non-orthogonal case and
  build two biorthogonal sequences.
\end{intro}

\begin{Algorithm*}{bi-lanczos}{Lanczos Biorthogonalization}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1,\vw_1\in\R^n, \scal(\vv_1,\vw_1) = 1$, $\vv_0=\vw_0=0$, $\beta_1=\delta_1=0$.
    \For{$j=1,\dots,m$}
    \State $\alpha_{j} \gets \scal(\mata \vv_j,\vw_j)$
    \State $\hat\vv_{j+1} \gets \mata\vv_j - \alpha_j\vv_j - \beta_j \vv_{j-1}$
    \State $\hat\vw_{j+1} \gets \mata^T\vw_j - \alpha_j\vw_j - \delta_j \vw_{j-1}$
    \State $\delta_{j+1} \gets \sqrt{\abs{\scal(\hat\vv_{j+1},\hat\vw_{j+1})}}$
    \If{$\delta_{j+1}=0$} \textbf{stop}\EndIf
    \State $\beta_{j+1} \gets \nicefrac{\scal(\hat\vv_{j+1},\hat\vw_{j+1})}{\delta_{j+1}}$
    \State $\vv_{j+1} = \nicefrac{\hat\vv_{j+1}}{\beta_{j+1}}$
    \State $\vw_{j+1} = \nicefrac{\vw_{j+1}}{\delta_{j+1}}$
    \EndFor
  \end{algorithmic}  
\end{Algorithm*}

\begin{Theorem}{bi-lanczos-projection}
  If the Lanczos biorthogonalization does not break down before step
  $m$, then, the sequences $\{\vv_k\}_{k=1,\dots,m}$ and
  $\{\vw_k\}_{k=1,\dots,m}$ are bases for $\krylov_m(\mata,\vv_1)$ and
  $\krylov_m(\mata^T,\vw_1)$, respectively. They form a biorthogonal system and there holds
  \begin{align}
    \mata\matv_m &= \matv_m \matt_m + \delta_{m+1}\vv_{m+1\ve_m^T},\\
    \mata^T\matw_m &= \matw_m \matt_m^T + \beta_{m+1}\vw_{m+1\ve_m^T},\\
    \matw_m^T \mata \matv_m &= \matt_m,
  \end{align}
  where
  \begin{gather}\small
    \matt_m =
    \begin{pmatrix}
      \alpha_1 & \beta_2\\
      \delta_2 & \alpha_2 & \beta_2\\
      &\ddots & \ddots & \ddots\\
      &&\delta_{m-1} & \alpha_{m-1}&\beta_{m-1}\\
      &&&\delta_m&\alpha_m
    \end{pmatrix}.
  \end{gather}
\end{Theorem}

\begin{remark}
  The norms of the vectors $\vv_k$ and $\vw_k$ generated by Lanczos
  biorthogonalization are not controlled and may grow or shrink in an
  undesired way. This situation can be improved, since upon closer
  inspection of lines 8 and 9, biorthogonality is maintained as long
  as the coefficients $\beta_j$ and $\delta_j$ are chosen such that
  \begin{gather}
    \beta_{j+1}\delta_{j+1} = \scal(\hat\vv_{j+1},\hat\vw_{j+1}).
  \end{gather}
  
  Nevertheless, there is no mechanism which prevents this inner
  product from being zero before the Krylov spaces become
  invariant. Thus, Lanczos biorthogonalization can suffer from unlucky
  breakdowns.
\end{remark}

\begin{remark}  
  All biorthogonalization methods have some disadvantages in common:
  they do not minimize the error or the residual in each step. There
  might also be near breakdowns, where a denominator is almost zero,
  which must be detected, since after such an incident, the future
  convergence might suffer.

  Often, the necessity to multiply with $\mata^T$ is considered a downside.
  
  This has spurred the development of a whole bunch of algorithms,
  from BiCG to QMR to BiCG-stab (already involving 8 auxiliary vectors) to
  BiCG-stab($m$), which combines $m$ GMRES steps win an outer
  BiCG-stab algorithm.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Preconditioning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  If matrices are extremely ill-conditioned, convergence of cg or
  GMRES will still be slow, even if considerable faster than steepest
  descent or minimal residual. No general purpose algorithms with a
  better dependence on the condition number have been found.

  Thus, the only way out lies in a positive answer to the question:
  given a linear system $\mata\vx = \vb$, can we modify the system
  into one with better convergence properties?
\end{intro}

\begin{Definition}{preconditioned-system}
  We say that a linear system $\mata\vx=\vb$ is preconditioned from
  the left by the matrix $\matb^{-1}$, if we solve instead
  \begin{gather}
    \matb^{-1}\mata\vx = \matb^{-1}\vb.
  \end{gather}
  It is preconditioned from the right, if we solve
  \begin{gather}
    \mata\matb^{-1}\vy = \vb,\qquad \vx = \matb^{-1}\vy.
  \end{gather}
\end{Definition}

\begin{Definition}{preconditioner}
  A \define{preconditioner} is a matrix $\matb^{-1}$ such that the
  condition number of
  \begin{gather}
    \matb^{-1}\mata\qquad\text{or}\qquad \mata\matb^{-1}
  \end{gather}
  is much better than the one of $\mata$. Furthermore, the evaluation
  of $\matb^{-1}$ is at least much cheaper than the one of
  $\mata^{-1}$, ideally not much more expensive than the product
  $\mata\vx$.
\end{Definition}

\begin{Lemma}{preconditioner-symmetry}
  Let $\mata$ and $\matb$ be symmetric, positive definite matrices. Then, $\matb^{-1}\mata$ is self-adjoint with respect to the $\matb$-inner product, i.~e.
  \begin{gather}
    \scal(\matb^{-1}\mata\vx,\vx)_{\matb}
    = \scal(\mata\vx,\vx)_2
    = \scal(\vx,\mata\vx)_2
    = \scal(\vx,\matb\matb^{-1}\mata\vx)_2
    =\scal(\vx,\matb^{-1}\mata\vx)_{\matb}.
  \end{gather}
\end{Lemma}

\begin{remark}
  The term ``orthogonality'' in the Lanczos procedure can be in
  reference to any inner product, if orthogonalization is performed
  with that inner product. In that case, the property ``symmetric'' of
  the matrix must be replaced by ``self-adjoint with respect to the inner product''

  We can now construct a preconditioned conjugate gradient method by
  changing the inner products and introducing the residual of the
  preconditioned equation as a new auxiliary vector
  \begin{gather}
    \vz_{j} = \matb^{-1} \vr_{j} = \matb^{-1}\vb - \matb^{-1}\mata\vx_{j}.
  \end{gather}
  Then, we obtain
  \begin{align}
    \scal(\vz_j,\vz_j)_{\matb} &= \scal(\matb\matb^{-1}\vr_j,\vz_j)_2 = \scal(\vr_j,\vz_j)_2,\\
    \scal(\matb^{-1}\mata\vp_j,\vp_j)_{\matb} &= \scal(\mata\vp_j,\vp_j)_2 = \scal(\vp_j,\vp_j)_{\mata}.
  \end{align}
  Thus, the algorithm can be implemented without application of the
  matrix $\matb$, which may not be easy to implement.
\end{remark}

\begin{Algorithm*}{pcg}{Preconditioned Conjugate Gradient}
  \begin{algorithmic}[1]
    \State $\vr_0 = \vb-\mata \vx_0$, $\vz_0 = \matb^{-1}\vr_0$
    \State $\vp_0 \gets \vz_0$
    \For{$j=0,1,\dots$}
    \State $\alpha_j \gets \frac{\scal(\vr_j,\vz_j)}{\scal(\mata\vp_j,\vp_j)}$
    \State $\vx_{j+1} \gets \vx_j + \alpha_j \vp_j$
    \State $\vr_{j+1} \gets \vr_j - \alpha_j \mata \vp_j$
    \State $\vz_{j+1} \gets \matb^{-1} \vr_{j+1}$
    \State $\beta_j =\frac{\scal(\vr_{j+1},\vz_{j+1})}{\scal(\vr_j,\vz_j)}$
    \State $\vp_{j+1} = \vz_{j+1} + \beta_j \vp_j$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
