%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Krylov spaces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Definition}{krylov-space}
  For a matrix $\mata\in\Rnn$ and a vector $\vv\in\R^n$, we define the
  \define{Krylov space}
  \begin{gather}
    \krylov_m = \krylov_m(\mata,\vv)
    = \spann{\vv,\mata\vv,\dots,\mata^{m-1}\vv}.
  \end{gather}
\end{Definition}

\begin{Definition}{grade-of-v}
  For a matrix $\mata\in\Rnn$ and a vector $\vv\in\R^n$ we define the
  \define{grade} of $\vv$, more precise the grade of $\vv$ with
  respect to $\mata$ as the minimal grade of a polynomial $p$ such
  that
  \begin{gather}
    p(\mata)\vv = 0
  \end{gather}
\end{Definition}

\begin{Lemma}{krylov-polynomial}
  Let $\mu$ be the grade of $\vv$ with respect to $\mata$.  The Krylov
  space $\krylov_m(\mata,\vv)$ is isomorphic to $\P_{m-1}$ if and only
  if $m\le \mu$. In this case, for any $\vw\in\krylov_m(\mata,\vv)$
  there is a unique polynomial $p\in\P_{m-1}$ such that
  \begin{gather}
    \label{eq:krylov-polynomial-1}
    \vw = p(\mata)\vv.
  \end{gather}

  The Krylov space $\krylov_m(\mata,\vv)$ is invariant under the
  action of $\mata$ if and only if $m\ge\mu$.
\end{Lemma}

\begin{proof}
  See \cite[Propositions 6.1 \& 6.2]{Saad00}.
  The definition of $\krylov_m(\mata,\vv)$ implies that any vector $\vw$ in this space has a representation
  \begin{gather}
    \vw = \sum_{i=0}^{m-1} \alpha_i \mata^i\vv =: p_{\vw}(\mata) \vv.
  \end{gather}
  Since the vectors in this sum span $\krylov_m$, its dimension is
  $m$ if and only if they are linearly independent, which in turn is
  the case if and only if there is a nonzero polynomial
  $p_0\in\P_{m-1}$ such that
  \begin{gather}
    p_0(\mata)\vv = 0.
  \end{gather}
  By assumption, such a polynomial exists if and only if
  $\mu<m$. Thus, we have proven that $\dim \krylov_m = m$ if and only
  if $m\le \mu$ and thus the mapping induced
  by~\eqref{eq:krylov-polynomial-1} is an isomorphism.

  On the other hand, $\krylov_m$ is invariant, if and only if the
  vectors spanning $\krylov_{m+1}$ are linearly dependent. We have
  just proven that this is the case if and only if $\mu<m+1$ or
  $\mu\le m$.
\end{proof}

\begin{Lemma}{krylov-projector}
  Let $\matq_m$ be a projector onto $\krylov_m=\krylov_m(\mata,\vv)$ and define
  \begin{align}
    \mata_m\colon \krylov_m &\to \krylov_m\\
    \vv&\mapsto \matq_m \mata.
  \end{align}
  Then, for any polynomial $q\in\P_{m-1}$ there holds
  \begin{gather}
    \label{eq:krylov-projector-1}
    q(\mata)\vv = q(\mata_m)\vv.
  \end{gather}
  For any polynomial $p\in\P_m$, there holds
  \begin{gather}
    \label{eq:krylov-projector-2}
    \matq_m p(\mata)\vv = p(\mata_m)\vv.
  \end{gather}
\end{Lemma}

\begin{proof}
  See \cite[Proposition 6.3]{Saad00}.  First, we
  prove~\eqref{eq:krylov-projector-1} by proving it for the monomial
  basis $q_i(x) = x^i$. It is obviously true for $q_0\equiv 1$. Now,
  we assume it is true for $q_i$ and conclude for $q_{i+1}$. To this
  end, we observe
  \begin{gather}
    q_{i+1}(\mata)\vv = \mata q_i(\mata)\vv = \mata q_i(\mata_m)\vv.
  \end{gather}
  Since $i<m-1$, this vector is in $\krylov_m$ and we obtain
  \begin{gather}
    q_{i+1}(\mata)\vv
    = \matq_m q_{i+1}(\mata)\vv
    = \matq_m \mata q_i(\mata_m)\vv
    = q_{i+1}(\mata_m)\vv,
  \end{gather}
  since $q_i(\mata_m)\vv\in\krylov_m$. Thus, we have proven the
  statement for $q\in\P_{m-1}$. The proof for $q_{m}(x) = x^m$ uses
  the same induction step, just with the exception that the action of
  $\matq_m$ on the left is not the identity anymore, since
  $q_m(\mata)\vv$ may not be in $\krylov_m$.
\end{proof}

\begin{todo}
  Reorganize as follows:
  \begin{enumerate}
  \item Arnoldi and Lanczos process here
  \item Derivation of cg from Lanczos
  \item General Arnoldi and GMRES
  \item Comments on Bi-Lanczos
  \end{enumerate}
\end{todo}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Arnoldi and Lanczos procedures}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  We now develop algorithms which for a given matrix $\mata\in\Rnn$
  and initial vector $\vv\in\R^n$ construct the Krylov space
  $\krylov_m$ and an orthogonal basis dimension by dimension. This
  algorithm is named after Walter E. Arnoldi in the general case and
  after Cornelius Lanczos in the symmetric case.
\end{intro}

\begin{Algorithm*}{arnoldi-1}{Arnoldi with Gram-Schmidt}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$
    \For{$j=1,\dots,m$}
    \State $\vw_j \gets \mata\vv_j$
    \For{$i=1,\dots,j$}
    \State $h_{ij} \gets \scal(\mata\vv_j,\vv_i)$
    \State $\vw_j \gets \vw_j - h_{ij}\vv_i$
    \EndFor
    \State $h_{j+1,j} \gets \norm{\vw_j}_2$
    \If{$h_{j+1,j}=0$} \textbf{stop}\EndIf
    \State $\vv_{j+1} = \nicefrac{\vw_j}{h_{j+1,j}}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{arnoldi-krylov}
  Assume that the Arnoldi algorithm does not stop before step
  $m$. Then, the vectors $\vv_1,\dots,\vv_m$ form an orthonormal basis
  of $\krylov_m(\mata,\vv_1)$.
\end{Lemma}

\begin{proof}
  See \cite[Proposition 6.4]{Saad00}.
\end{proof}

\begin{Algorithm*}{arnoldi-householder}{Arnoldi with Householder}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$
    \State Choose $\matq_1$ such that $\matq_1\vv_1 = \ve_1$.
    \For{$j=1,\dots,m$}
    \State $\vv_j \gets \matq_1\dots\matq_j\ve_j$
    \State $\matq_{j+1}^*\matr \gets \matq_j\dots\matq_1(\vv_1,\mata\vv_1,\dots,\mata\vv_m)$
    \Comment{Householder}
    \If{$r_{j+1,j+1}=0$} \textbf{stop}\EndIf
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{arnoldi-householder}
  If the Arnoldi algorithm with Householder orthogonalization did not
  stop before step $m$, then the set $\vv_1,\dots,\vv_m$ is an
  orthonormal basis for $\krylov_m(\mata,\vv_1)$.
\end{Lemma}

\begin{proof}
  By line 4 of the algorithm, for $j=1,\dots,m$
  \begin{gather}
    \matq_1^*\dots\matq_{j+1}^* \matr = (\vv_1,\mata\vv_1,\dots,\mata\vv_j)
  \end{gather}
  is a QR factorization of the (rectangular) matrix on the
  right. Therefore, we can apply \slideref{Lemma}{qr-columns}, which
  readily implies the result.
\end{proof}

\begin{Theorem}{arnoldi-projection}
  Let $\matv_m = (\vv_1,\dots,\vv_m)$ be the matrix of basis vectors
  generated by the Arnoldi method and let
  $\overline{\matH}_m\in\R^{m+1\times m}$ be the Hessenberg matrix of
  entries $h_{ij}$ computed in the algorithm. Let further
  $\matH_m\in\R^{m\times m}$ be the same matrix without the last
  row. Then, there holds
  \begin{align}
    \label{eq:arnoldi-projection-1}
    \mata\matv_m &= \matv_m\matH_m+\vw_m\ve_m^T\\
    \label{eq:arnoldi-projection-2}
                 &= \matv_{m+1}\overline{\matH}_m
  \end{align}
  and
  \begin{gather}
    \label{eq:arnoldi-projection-3}
    \matH_m = \matv_m^T\mata\matv_m.
  \end{gather}
\end{Theorem}

\begin{proof}
  See also \cite[Proposition 6.5]{Saad00}.
  From lines 2--6 of \slideref{Algorithm}{arnoldi-1}, we deduce
  \begin{gather}
    \mata \vv_j = \vw_j + \sum_{i=1}^j h_{ij} \vv_i,
    \qquad j=1,\dots,m.
    =  \sum_{i=1}^{j+1} h_{ij} \vv_i,
  \end{gather}
  For $j<m$, we use line 9 to obtain
  \begin{gather}
    \mata \vv_j =  \sum_{i=1}^{j+1} h_{ij} \vv_i.
  \end{gather}
  Since $\mata \vv_j$ is the $j$th column of $\mata\matv_m$ and
  $\matH_m$ is Hessenberg, we can rewrite this as
  \begin{gather}
    \mata\matv_m\ve_j = \matv_m\matH\ve_j,\qquad j=1,\dots,m-1.
  \end{gather}
  For $j=m$, we cannot replace $\vw_m$, therefore,
  \begin{gather}
    \mata\matv_m\ve_m = \matv_m\matH\ve_m + \vw_m.
  \end{gather}
  Using $\ve_m^T\ve_j=0$ for $j<m$, we can combine these to the matrix
  representation
  \begin{gather}
    \mata\matv_m = \matv_m\matH + \vw_m\ve_m^T.
  \end{gather}
  Adding the vector $\vv_{m+1}$ from the next step of the algorithm,
  we get the alternative formulation~\eqref{eq:arnoldi-projection-2}.
  In order to prove~\eqref{eq:arnoldi-projection-3}, we multiply from
  the left with $\matv_m^T$. The proof is finished by observing that
  $\matv_m^T\matv_m$ is the identity on $\R^m$ and $\matv_m^T\vw_m=0$.
\end{proof}

\begin{remark}
  The next step of the Arnoldi method can always be computed if
  $h_{j+1,j}\neq 0$ in line 9. If it cannot be, we speak of a
  \define{breakdown} of the method.
\end{remark}

\begin{Lemma}{arnoldi-breakdown}
  The Arnoldi method stops at step $j$ if and only if the subspace $\krylov_j$ is invariant under $\mata$ or equivalently, the minimal polynomial of $\vv_1$ is of degree $j$.
\end{Lemma}

\begin{proof}
  The algorithm stops in line 8 if and only if $\mata\vv_j$ is a
  linear combination of the previously computed basis vectors, such
  that $\vw_j=0$ in line 5 at the end of the loop. This happens
  exactly when the Krylov space is invariant.
\end{proof}

\begin{Lemma}{arnoldi-symmetric}
  If the matrix $\mata$ is symmetric, the vectors of the Arnoldi
  procedure admit the three-term recurrence relation
  \begin{gather}
    \delta_{j+1}\vv_{j+1} = \mata \vv_j - \gamma_j \vv_j - \delta_j\vv_{j-1}
  \end{gather}
  with $\vv_0 = 0$, $\delta_1 = 0$ and
  \begin{gather}
    \gamma_j =  h_{jj}, \qquad \delta_j = h_{j-1,j},
    \qquad j=1,\dots
  \end{gather}
  In particular, explicit orthogonalization is only necessary with respect to
  the previous two basis vectors.
\end{Lemma}

\begin{todo}
  Add reference to QR method for symmetrix matrices.
\end{todo}
\begin{proof}
  See also~\cite[Section 6.6.1]{Saad00}. From
  \slideref{Theorem}{arnoldi-projection}, we realize that the
  projected matrix $\matH_m$ of the Arnoldi method is symmetric if
  $\mata$ is symmetric. Since it is Hessenberg by construction, we
  conclude that it is tridiagonal. Since it is also the matrix of
  coefficients in the Gram-Schmidt orthogonalization, the statement
  becomes evident.
\end{proof}

\begin{Algorithm*}{lanczos}{Lanczos}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$, $\vv_0=0$, $\delta_1=0$.
    \For{$j=1,\dots,m$}
    \State $\vw_j \gets \mata\vv_j - \delta_j \vv_{j-1}$
    \State $\gamma_{j} \gets \scal(\vw_j,\vv_j)$
    \State $\vw_j \gets \vw_j - \gamma_j\vv_i$
    \State $\delta_{j+1} \gets \norm{\vw_j}_2$
    \If{$\delta_{j+1}=0$} \textbf{stop}\EndIf
    \State $\vv_{j+1} = \nicefrac{\vw_j}{\delta_{j+1}}$
    \EndFor
  \end{algorithmic}  
\end{Algorithm*}

\begin{remark}
  The main difference between Arnoldi and Lanczos is the fact that
  every step of the Lanczos process onle requires the previous two
  vectors, while Arnoldi requires storing the whole history.

  This implies, that Lanczos itself is much closer to the concept of
  an iterative method than Arnoldi, weren't it for the computation of
  $\vy_m$.

  We will now develop a new algorithm based on the observation that
  also the corrections for $\vx^{(m)}$ can be computed incrementally.
\end{remark}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The conjugate gradient method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Theorem}{arnoldi-linear-system}
  Given an initial vector $\vx^{(0)}$ and
  $\vr^{(0)} = \vb - \mata \vx^{(0)}$. Let $\beta=\norm{\vr^{(0)}}$
  and choose $\vv_1 =\nicefrac{\vr^{(0)}}{\beta}$ in Arnoldi's
  method. Then, the Galerkin approximation
  \begin{align}
    x^{(m)} &\in x^{(0)} + \krylov_m(\mata,r^{(0)})\\
    b-\mata x^{(m)} &\perp \krylov_m(\mata,r^{(0)}),
  \end{align}
  is obtained in two steps. Solve in $\R^m$
  \begin{gather}
    \matH_m\vy_m = \beta\ve_1,
  \end{gather}
  and let
  \begin{gather}
    \vx^{(m)} = \vx^{(0)} + \matv_m \vy_m.
  \end{gather}
\end{Theorem}

\begin{todo}
  Move this to Lanczos as it is irrelevant here
\end{todo}

\begin{Theorem}{arnoldi-linear-residual}
  Let $\vx^{(m)}$ be the Galerkin solution in
  \slideref{Theorem}{arnoldi-linear-system}. Then, there holds
  \begin{gather}
    \vb-\mata\vx^{(m)} = -h_{m+1,m} \ve_m^T \vy_m \vv_{m+1},
  \end{gather}
  and therefore
  \begin{gather}
    \norm{\vb-\mata\vx^{(m)}}_2 = h_{m+1,m} \abs{\ve_m^T \vy_m}.
  \end{gather}  
\end{Theorem}

\begin{proof}
  See also~\cite[Proposition 6.7]{Saad00}
  We use the definition of the residual of $\vx^{(m)}$ and the representation of \slideref{Theorem}{arnoldi-projection} to obtain
  \begin{align}
    \vr^{(m)} = b-\mata\vx^{(m)}
    &= b- \mata\left(\vx^{(0)} + \matv_m\vy_m\right)\\
    &=\vr^{(0)} - \mata\matv_m\vy_m\\
    &= \beta\vv_1 - \matv_m\matH_m\vy_m - \vw_m\ve_m^T\vy_m.
  \end{align}
  Since $\vy_m$ solves the projected system
  $\matH_m\vy_m = \beta\ve_1$, there holds
  \begin{gather}
    \beta\vv_1 - \matv_m\matH_m\vy_m = \beta\vv_1 - \beta \matv_m\ve_1 = 0.
  \end{gather}
  Furthermore, we rewrite
  \begin{gather}
    \vw_m\bigl(\ve_m^T\vy_m\bigr) = h_{m+1,m} \bigl(\ve_m^T\vy_m\bigr) \vv_{m+1}.
  \end{gather}
\end{proof}

\begin{remark}
  Following \slideref{Theorem}{projected-invertible} this algorithm
  works reliably only for $\mata$ symmetric, positive definite. This
  is the case we study here and we will improve the algorithm
  considerably.
\end{remark}

\begin{Lemma*}{lanczos-linear}{Lanczos solver}
  The Galerkin approximation   \begin{align}
    x^{(m)} &\in x^{(0)} + \krylov_m(\mata,r^{(0)})\\
    b-\mata x^{(m)} &\perp \krylov_m(\mata,r^{(0)}),
  \end{align}
  is obtained as $\vx^{(m)} = \vx^{(0)} + \matv_m \vy_m$, where $\matv_m$ is the basis obtained from the Lanczos process and $\vy_m$ solves
  \begin{gather}
    \matH_m\vy_m = \beta\ve_1,
    \qquad \matH_m =
    \begin{pmatrix}
      \gamma_1 & \delta_2\\
      \delta_2 & \gamma_2 & \ddots\\
      &\ddots&\ddots&\delta_m\\
      &&\delta_{m}&\gamma_m
    \end{pmatrix}.
  \end{gather}
  For the residual, there holds
  \begin{gather}
    \vr^{(m)} = \vb-\mata\vx^{(m)} = -\delta_{m+1} \ve_m^T \vy_m \vv_{m+1}.
  \end{gather}
\end{Lemma*}

\begin{proof}
  These are the statements of
  \slideref{Theorem}{arnoldi-linear-system} and
  \slideref{Theorem}{arnoldi-linear-residual}. Since the Lanczos
  process is just a specialization of the Arnoldi process, they still
  hold.
\end{proof}

\begin{Lemma}{lanczos-incremental}
  Given the vector $\vx^{(m)}$ in the Lanczos solver, the vector
  $\vx^{(m+1)}$ can be computed incrementally by an update of the form
  \begin{gather}
    \vx^{(m+1)} = \vx^{(m)} + \zeta_{m+1}\vg^{(m+1)},
  \end{gather}
  where the vector $\vg^{(m)}$ itself is computed only using the
  vectors $\vg^{(m-1)}$ and $\vv^{(m)}$.
\end{Lemma}

\begin{proof}
  See also \cite[Section 6.7.1]{Saad00}. First, we compute the LU factorization of $\matH_m$ as
  \begin{gather}
    \matH_m =
    \begin{pmatrix}
      1\\\lambda_2&1\\
      &\ddots&\ddots\\
      &&\lambda_m&1
    \end{pmatrix}
    \times
    \begin{pmatrix}
      \eta_1&\delta_2\\
      &\ddots&\ddots\\
      &&\eta_{m-1}&\delta_m\\
      &&&\eta_m
    \end{pmatrix},
  \end{gather}
  where
  \begin{gather}
    \lambda_k = \frac{\delta_k}{\eta_{k-1}},
    \qquad
    \eta_k = \gamma_k - \lambda_k \delta_k.
  \end{gather}
  We note that LU decomposition works from top to bottom and left to
  right, such that already processed rows do not change when adding
  more steps to the algorithm.

  Now we introduce matrices and vectors
  \begin{gather}
    \matg_m = \matv_m \matu_m^{-1} \in \R^{n\times m},
    \qquad \vz_m = \matl_m^{-1} \beta \ve_1\in \R^m,
  \end{gather}
  such that
  \begin{align}
    \vx^{(m)}
    &= \vx^{(0)} + \matv_m \matu_m^{-1} \matl_m^{-1} \beta \ve_1\\
    &= \vx^{(0)} + \matg_m \vz_m
  \end{align}
  With $\zeta_k$ following the recursion
  $\zeta_k = - \lambda_k \zeta_{k-1}$, we realize
  \begin{gather}
    \vz_{m+1} =
    \begin{pmatrix}
      \zeta_1\\\vdots\\\zeta_{m+1}
    \end{pmatrix}
    =
    \begin{pmatrix}
      \vz_{m}\\\zeta_{m+1}
    \end{pmatrix}.
  \end{gather}
  Let $\vg_k$ be the column vectors of $\matg_m$. Writing the
  definition of $\matg_m$ in the form $\matg_m \matu_m = \matv_m$, we
  obtain the relation
  \begin{gather}
    \label{eq:nla:krylov:lanczos-p}
    \vg_k = \frac1{\eta_k}\bigl(\vv_k - \delta_k \vg_{k-1}\bigr).
  \end{gather}
  Thus,
  \begin{gather}
    \matg_{m+1} =
    \begin{pmatrix}
      \vg_1,\dots,\vg_{m+1}
    \end{pmatrix}
    =
    \begin{pmatrix}
      \matg_{m},\vg_{m+1}
    \end{pmatrix}.
  \end{gather}
  There holds
  \begin{align}
    \vx^{(m+1)}
    &= \vx^{(0)} + \matg_{m+1}\vz_{m+1}\\
    &= \vx^{(0)} + \sum_{k=1}^{m+1} \zeta_k \vg_k,\\
    &= \vx^{(m)} + \zeta_{m+1} \vg_{m+1}.
  \end{align}
\end{proof}

\begin{remark}
  Thus, we have proven a simple update formula for the iterates
  $\vx^{(m)}$ based on an additional vector $\vg_m$, which is again
  obtained by an update formula.

  We have used an LU decomposition without pivoting though in the
  derivation of this algorithm, which might suffer from numerical
  instability. Therefore, we will now analyze the main properties of
  the sequences $\vv_m$, $\vg_m$ and $\vr_m$ and then write a new
  algorithm to obtain these sequences.
\end{remark}

\begin{Lemma}{lanczos-orthogonality}
  The sequences of vector $\vr_m = \vb-\mata\vx^{(m)}$ and $\vg_m$
  constructed in the preceding lemma have the following orthogonality
  properties:
  \begin{xalignat}2
    \scal(\vr_i,\vr_k) &= 0 & i&\neq k\\
    \scal(\mata\vg_i,\vg_k) &= 0 & i&\neq k.
  \end{xalignat}
\end{Lemma}

\begin{proof}
  Orthogonality of the residuals follows from
  \slideref{Lemma}{lanczos-linear}, since $\vr_m$ is aligned with
  $\vv_{m+1}$.

  The second orthogonality relation is equivalent to the fact that
  $\matg_m^T\mata\matg_m$ is diagonal.
  \begin{align}
    \matg_m^T\mata\matg_m
    &= \matu_m^{-T}\matv_m^T\mata\matv_m\matu_m^{-1}\\
    &= \matu_m^{-T}\matH_m\matu_m^{-1}\\
    &= \matu_m^{-T}\matl_m.
  \end{align}
  The first term is symmetric.  The last is the product of two lower
  triangular matrices and thus lower triangular. Thus, the matrix must
  be diagonal.
\end{proof}

\begin{Algorithm*}{cg}{Conjugate Gradient Method}
  \begin{algorithmic}[1]
    \State $\vr_0 = b-\mata \vx_0$
    \State $\vp_0 = \vr_0$
    \For{$j=0,1,\dots$}
    \State $\alpha_j = \frac{\scal(\vr_j,\vr_j)}{\scal(\mata\vp_j,\vp_j)}$
    \State $\vx_{j+1} = \vx_j + \alpha_j \vp_j$
    \State $\vr_{j+1} = \vr_j - \alpha_j \mata \vp_j$
    \State $\beta_j =\frac{\scal(\vr_{j+1},\vr_{j+1})}{\scal(\vr_j,\vr_j)}$
    \State $\vp_{j+1} = r_{j+1} + \beta_j \vp_j$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{cg-orthogonality}
  The sequences $\vx_j$ and $\vr_j$ produced by the cg method coincide
  with those of the Lanczos solver (in exact arithmetic). The vectors
  $\vp_j$ of the cg method coincide up to scalar factors with the vectors $\vg_{j+1}$
  in \slideref{Lemma}{lanczos-incremental}.
\end{Lemma}

\begin{todo}
  The structure of the argument is not very clear. We are jumping
  back and forth between the algorithms. It would be better to show
  that CG produces the same orthogonality with respect to the
  previous vectors, then use uniqueness. Where to put equality of
  the coefficients?
\end{todo}
  
\begin{proof}
  Note that the vectors $\vp_j$ refers to the cg algorithm, the
  vectors from \slideref{Lemma}{lanczos-incremental} will be
  distinguished clearly.

  We start with the update formula
  $\vx_{j+1} = \vx_j + \alpha_j \vp_j$, which immediately implies
  \begin{gather}
    \vr_{j+1} = \vr_j - \alpha_j \mata\vp_j.
  \end{gather}
  Orthogonality between $\vr_{j+1}$ and $\vr_j$ due to the projection
  property implies the choice
  \begin{gather}
    \alpha_j = \frac{\scal(\vr_j,\vr_j)}{\scal(\mata\vp_j,\vr_j)}.
  \end{gather}
  Since $\vr_j$ is a multiple of $\vv_{j+1}$ by the residual
  representation in \slideref{Theorem}{arnoldi-linear-residual}, the
  update formula~\eqref{eq:nla:krylov:lanczos-p} for $\vg_{j+1}$
  (after index shift) implies that the direction is a linear
  combination of $\vr_{j+1}$ and $\vp_j$. After arbitrary scaling,
  \begin{gather}
    \vp_{j+1} = \vr_{j+1} + \beta_j \vp_{j}.
  \end{gather}
  A-orthogonality implies the choice
  \begin{gather}
    \beta_j = -\frac{\scal(\mata \vp_j,\vr_{j+1})}{\scal(\mata \vp_j,\vp_j)}.
  \end{gather}
  Note that in each step, the orthogonality relation determines $\vp_{j+1}$ uniquely
  up to scaling in this subspace. After the scaling has been chosen,
  the orthogonality relation for $\vr_{j+1}$ uniquely
  determines the update for $\vx_{j+1}$, such that the sequences
  indeed coincide.

  The definition of the coefficients still differs from the cg
  algorithm. But, we observe
  \begin{gather}
    \scal(\mata\vp_j,\vr_j) = \scal(\mata\vp_j,\vp_j-\beta_{j-1}\vp_{j-1})
    = \scal(\mata\vp_j,\vp_j).
  \end{gather}
  Similarly, from the update formula for the residual, we get
  \begin{gather}
    \mata\vp_j = \frac1{\alpha_j}\bigl(\vr_j-\vr_{j+1}\bigr),
  \end{gather}
  and thus,
  \begin{gather}
    \beta_j = \frac1{\alpha_j}\frac{\scal(\vr_{j+1},\vr_{j+1}-\vr_j)}{\scal(\mata\vp_j,\vp_j)} = \frac{\scal(\vr_{j+1},\vr_{j+1})}{\scal(\vr_{j},\vr_{j})}.
  \end{gather}
  Note that these forms avoid inner products of possibly almost
  orthogonal vectors.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convergence analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  Here, we will develop a sequence of error estimates for the
  conjugate gradient method based on its optimality as a projection
  method stated in \slideref{Theorem}{projection-orthogonal-optimal}. These
\end{intro}

\begin{Theorem}{cg-optimality}
  For the error $\ve^{(k)} = \vx^{(k)}-\vx$ after $k$ steps of the cg
  method there holds
  \begin{align}
    \norm{\ve^{(k)}}_A
    & = \min_{p\in \P_{k-1}}
      \norm{\ve^{(0)} + \mata p(\mata) \ve^{(0)}}_A\\
    &= \min_{\substack{p\in\P_k\\p(0) = 1}} \norm{p(\mata)\ve^{(0)}}_A.
  \end{align}
\end{Theorem}

\begin{proof}
  There holds $\vx^{(k)} = \vx^{(0)} + \vw$ with
  $\vw \in \krylov_k(\mata,\vr^{(0)})$. Then, by
  \slideref{Lemma}{krylov-polynomial}, there is $p\in\P_{k-1}$ such
  that $\vw = -p(\mata) \vr^{(0)}$. Thus
  \begin{align}
    \vx^{(k)} - \vx
    &= \vx^{(0)} - \vx - p(\mata)\vr^{(0)}\\
    &= \vx^{(0)} - \vx - p(\mata) (\vb-\mata\vx^{(0)})\\
    &= \vx^{(0)} - \vx - p(\mata) (\mata\vx-\mata\vx^{(0)})\\
    &= \vx^{(0)} - \vx - \mata p(\mata) (\vx^{(0)}-\vx).
  \end{align}
  On the other hand, we have the optimality of projection methods in
  \slideref{Theorem}{projection-orthogonal-optimal}, which states
  \begin{align}
    \norm{\ve^{(k)} -\vx}_A
    &= \min_{\vw\in \krylov_k(\mata,\vr^{(0)})} \norm{\vx^{(0)}+\vw-\vx}_A\\
    &= \min_{\vw\in \krylov_k(\mata,\vr^{(0)})} \norm{\ve^{(0)}+\vw}_A.
  \end{align}
  Thus, we obtain the first expression. The second follows by noticing
  that the set of polynomials $q(x) = 1+xp(x)$ with $p\in\P_{k-1}$ is
  just the set of polynomials in $\P_k$ with coefficient $a_0 = 1$.
\end{proof}

\begin{Corollary}{cg-vs-descent}
  The error after $k$ steps of the conjugate gradient method is never
  greater than the error after equally many steps of the steepest
  descent method.
\end{Corollary}

\begin{proof}
  The error of the steepest descent method is
  \begin{gather}
    \ve^{(k)} = \prod_{i=k}^{1} (\id-\alpha_k\mata) \ve^{(0)}.
  \end{gather}
  This can be rewritten as
  \begin{gather}
    \ve^{(k)} = p_k(\mata) \ve^{(0)},
  \end{gather}
  where $p_k\in\P_k$ is the polynomial of degree $k$ defined by
  \begin{gather}
    p_k(x) = \prod_{i=k}^{1} (1-\alpha_kx).
  \end{gather}
  There holds $p_k(0)=1$ and thus this polynomial is in the set over
  which the conjugate gradient method minimizes.
\end{proof}

\begin{Corollary}{cg-optimality-spectrum}
  Let $\sigma(\mata) = \{\lambda_1,\dots,\lambda_n\} \subset \R^+$ be
  the spectrum of $\mata$. Then, the error after $k$ steps of the
  conjugate gradient method follows the estimate
  \begin{gather}
    \norm{\ve^{(k)}}_A \le \rho_k \norm{\ve^{(0)}}_A,
  \end{gather}
  where
  \begin{gather}
    \rho_k = \min_{\substack{p\in\P_k\\p(0) = 1}} \max_{\lambda\in\sigma(\mata)} \abs{p(\lambda)}.
  \end{gather}
\end{Corollary}

\begin{proof}
  First, since we apply the cg iteration to s.p.d. matrices, there is
  an orthogonal basis $\{\vv_1,\dots,\vv_n\}$ of eigenvectors of
  $\mata$ with real, positive eigenvalues $\lambda_i$. Thus, we can
  write with some coefficients $\alpha_i$:
  \begin{gather}
    \ve^{(0)} = \sum_{i=1}^n \alpha_i \vv_i,
    \qquad
    \ve^{(k)} = \sum_{i=1}^n \alpha_i p(\lambda_i) \vv_i.    
  \end{gather}
  Hence, we can estimate
  \begin{align}
    \norm{\ve^{(k)}}_A^2
    &= \sum_{i=1}^n \lambda_i\alpha_i^2 p(\lambda_i)^2 \norm{\vv_i}_A^2\\
    & \le \max_i p(\lambda_i)^2 \sum_{i=1}^n \lambda_i\alpha_i^2\norm{\vv_i}_A^2\\
    &= \max_i p(\lambda_i)^2 \norm{\ve^{(0)}}_A^2.
  \end{align}
  The statement now follows from \slideref{Theorem}{cg-optimality}.
\end{proof}

\begin{remark}
  It is important to note that the preceding corollary is close to
  optimal. In particular, it can exploit any information which is
  known about the spectrum. In particular, the following statements are true:
  \begin{enumerate}
  \item If the matrix $\mata$ only has $k$ distinct eigenvalues, then
    the cg iteration converges to the solution in $k$ steps.
  \item If the eigenvalues are clustered around $k$ values, then the
    gain in the first $k$ steps will be large.
  \item If the spectrum consists of a continuum plus a single outlier,
    the method will perform almost as if the outlier did not exist.
  \end{enumerate}
  The following corollary now deals with the case that the spectrum
  has no particular structure, but fills a continuous interval
  somewhat evenly.
\end{remark}

\begin{Corollary}{cg-condition-number}
  The error after $k$ steps of the cg method admits the estimate
  \begin{gather}
    \norm{e^{(k)}}_A \le 2 \left(
      \frac{\sqrt{\cond_2(A)}-1}{\sqrt{\cond_2(A)}+1}\right)^k
    \norm{e^{(0)}}_A.
  \end{gather}
\end{Corollary}

\begin{proof}
  We make no further assumtions on the spectrum of $\mata$ except that
  all eigenvalues are in the interval
  $[\lambda_{\min},\lambda_{\max}]$. Thus, we estimate the factors $\rho_k$ in \slideref{Corollary}{cg-optimality-spectrum} by
  \begin{gather}
    \rho_k \le \min_{\substack{p\in\P_k\\p(0) = 1}}
    \max_{\lambda\in[\lambda_{\min},\lambda_{\max}]} \abs{p(\lambda)}.
  \end{gather}
  
  We use the optimality property of Chebyshev polynomials stated in
  \slideref{Corollary}{chebyshev-minimal-2} and note that the
  optimality stated there is exactly what we need. Thus,
  $p = \widehat\pchebyshev_k(\lambda)$ solves the minimization
  problem and there holds
  \begin{align}
    \rho_k
    &\le \left(\pchebyshev_k\left(
      \frac{\lambda_{\max}+\lambda_{\min}}{\lambda_{\max}-\lambda_{\min}}
      \right)\right)^{-1}\\
    & \le \left(\frac{\sqrt{\lambda_{\max}}-\sqrt{\lambda_{\min}}}{\sqrt{\lambda_{\max}}+\sqrt{\lambda_{\min}}}
      \right)^k
  \end{align}
  Dividing both sides of the fraction by $\sqrt{\lambda_{\min}}$ yields the result.
\end{proof}

\begin{remark}
  Note that the estimate in \slideref{Corollary}{cg-condition-number}
  is not a classical contraction estimate which allows us to deduce
  the error reduction from step $k$ to $k+1$. Indeed, the factor 2
  before the quotient renders this exercise impossible, and the
  estimate for the steepest descent method is our rescue.

  Instead, the estimate shows that with growing dimension of the
  Krylov space, eventually the factor 2 is compensated by the first
  $k$ powers of the quotient and then the additional powers guarantee
  faster convergence.
\end{remark}


\begin{Theorem}{gmres-optimality}
  For the residual $\vr^{(k)} = \vb-\mata\vx^{(k)}$ after $k$ steps of
  the GMRES method, there holds
  \begin{gather}
    \norm{\vr^{(k)}}_2 = \min_{\substack{p\in\P_k\\p(0) = 1}}
    \norm{p(\mata)\vr^{(0)}}_2.
  \end{gather}
  If the matrix $\mata$ is diagonalizable, then there holds
    \begin{gather}
    \norm{\vr^{(k)}}_2 \le \rho_k \cond_2(\matv) \norm{\vr^{(0)}}_2,
  \end{gather}
  where $\matv$ is the matrix of eigenvectors and
  \begin{gather}
    \rho_k = \min_{\substack{p\in\P_k\\p(0) = 1}} \max_{\lambda\in\sigma(\mata)} \abs{p(\lambda)}.
  \end{gather}
\end{Theorem}

\begin{proof}
  See \cite[Proposition 6.32]{Saad00}.
\end{proof}

\begin{Corollary}{gmres-estimate}
  Let $\mata$ be diagonalizable such that its spectrum is contained in
  an ellipse in the complex plane with center $c$, focal distance $d$
  and semimajor axis $a$. Then,
  \begin{gather}
     \rho_k \approx \left(
      \frac{a+\sqrt{a^2-d^2}}{c+\sqrt{c^2-d^2}}
      \right)^k.
  \end{gather}
\end{Corollary}

\begin{proof}
  See \cite[Corollary 6.33]{Saad00}.
\end{proof}

\begin{remark}
  This theorem is analogue to \slideref{Theorem}{cg-optimality} and
  \slideref{Corollary}{cg-optimality-spectrum} and the proofs are
  identical, based on \slideref{Lemma}{krylov-polynomial} and
  \slideref{Theorem}{projection-oblique-optimal}.

  Note that the step from minimizing over $p(\mata)$ to $p(\lambda)$
  requires that the matrix $\mata$ is diagonalizable and produces
  optimal results only if it is normal. While this condition is not
  strictly necessary, there are examples of arbitrarily bad
  convergence. Indeed, for every nonincreasing sequence of $n-1$
  numbers, you can find a sequence such that the norms of residuals of
  the GMRES method will follow that
  sequence~\cite{GreenbaumPtakStrakos96}.
\end{remark}

\begin{Theorem}{gmres-pos-def}
  If $\mata$ is positive definite, then the GMRES method as well as
  the restarted GMRES method converge.
\end{Theorem}

\begin{proof}
  See \cite[Theorem 6.30]{Saad00}.
\end{proof}

\begin{remark}
  The main practical difference between the conjugate gradient and
  GMRES methods is the short recurrence. While running hundreds of
  steps of cg is not an issue, GMRES becomes infeasible beyond some 30
  steps due to the extensive storage and the explicit
  orthogonalization. There has been research into finding solutions
  for this problem, but having both optimization and short recurrence
  is impossible in general (in your homework you saw an example of
  nonsymmetric matrices where it is possible).
\end{remark}

\input{gmres}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bi-Lanczos and BiCG-stab}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  The problem of short recurrence for nonsymmetric matrices has been
  addressed by biorthogonalization methods. They go back to
  \slideref{Lemma}{projection-basis} in the non-orthogonal case and
  build two biorthogonal sequences.
\end{intro}

\begin{Algorithm*}{bi-lanczos}{Lanczos Biorthogonalization}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1,\vw_1\in\R^n, \scal(\vv_1,\vw_1) = 1$, $\vv_0=\vw_0=0$, $\beta_1=\delta_1=0$.
    \For{$j=1,\dots,m$}
    \State $\alpha_{j} \gets \scal(\mata \vv_j,\vw_j)$
    \State $\hat\vv_{j+1} \gets \mata\vv_j - \alpha_j\vv_j - \beta_j \vv_{j-1}$
    \State $\hat\vw_{j+1} \gets \mata^T\vw_j - \alpha_j\vw_j - \delta_j \vw_{j-1}$
    \State $\delta_{j+1} \gets \sqrt{\abs{\scal(\hat\vv_{j+1},\hat\vw_{j+1})}}$
    \If{$\delta_{j+1}=0$} \textbf{stop}\EndIf
    \State $\beta_{j+1} \gets \nicefrac{\scal(\hat\vv_{j+1},\hat\vw_{j+1})}{\delta_{j+1}}$
    \State $\vv_{j+1} = \nicefrac{\hat\vv_{j+1}}{\beta_{j+1}}$
    \State $\vw_{j+1} = \nicefrac{\vw_{j+1}}{\delta_{j+1}}$
    \EndFor
  \end{algorithmic}  
\end{Algorithm*}

\begin{Theorem}{bi-lanczos-projection}
  If the Lanczos biorthogonalization does not break down before step
  $m$, then, the sequences $\{\vv_k\}_{k=1,\dots,m}$ and
  $\{\vw_k\}_{k=1,\dots,m}$ are bases for $\krylov_m(\mata,\vv_1)$ and
  $\krylov_m(\mata^T,\vw_1)$, respectively. They form a biorthogonal system and there holds
  \begin{align}
    \mata\matv_m &= \matv_m \matt_m + \delta_{m+1}\vv_{m+1\ve_m^T},\\
    \mata^T\matw_m &= \matw_m \matt_m^T + \beta_{m+1}\vw_{m+1\ve_m^T},\\
    \matw_m^T \mata \matv_m &= \matt_m,
  \end{align}
  where
  \begin{gather}\small
    \matt_m =
    \begin{pmatrix}
      \alpha_1 & \beta_2\\
      \delta_2 & \alpha_2 & \beta_2\\
      &\ddots & \ddots & \ddots\\
      &&\delta_{m-1} & \alpha_{m-1}&\beta_{m-1}\\
      &&&\delta_m&\alpha_m
    \end{pmatrix}.
  \end{gather}
\end{Theorem}

\begin{remark}
  The norms of the vectors $\vv_k$ and $\vw_k$ generated by Lanczos
  biorthogonalization are not controlled and may grow or shrink in an
  undesired way. This situation can be improved, since upon closer
  inspection of lines 8 and 9, biorthogonality is maintained as long
  as the coefficients $\beta_j$ and $\delta_j$ are chosen such that
  \begin{gather}
    \beta_{j+1}\delta_{j+1} = \scal(\hat\vv_{j+1},\hat\vw_{j+1}).
  \end{gather}
  
  Nevertheless, there is no mechanism which prevents this inner
  product from being zero before the Krylov spaces become
  invariant. Thus, Lanczos biorthogonalization can suffer from unlucky
  breakdowns.
\end{remark}

\begin{remark}  
  All biorthogonalization methods have some disadvantages in common:
  they do not minimize the error or the residual in each step. There
  might also be near breakdowns, where a denominator is almost zero,
  which must be detected, since after such an incident, the future
  convergence might suffer.

  Often, the necessity to multiply with $\mata^T$ is considered a downside.
  
  This has spurred the development of a whole bunch of algorithms,
  from BiCG to QMR to BiCG-stab (already involving 8 auxiliary vectors) to
  BiCG-stab($m$), which combines $m$ GMRES steps win an outer
  BiCG-stab algorithm.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Preconditioning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  If matrices are extremely ill-conditioned, convergence of cg or
  GMRES will still be slow, even if considerable faster than steepest
  descent or minimal residual. No general purpose algorithms with a
  better dependence on the condition number have been found.

  Thus, the only way out lies in a positive answer to the question:
  given a linear system $\mata\vx = \vb$, can we modify the system
  into one with better convergence properties?
\end{intro}

\begin{Definition}{preconditioned-system}
  We say that a linear system $\mata\vx=\vb$ is preconditioned from
  the left by the matrix $\matb^{-1}$, if we solve instead
  \begin{gather}
    \matb^{-1}\mata\vx = \matb^{-1}\vb.
  \end{gather}
  It is preconditioned from the right, if we solve
  \begin{gather}
    \mata\matb^{-1}\vy = \vb,\qquad \vx = \matb^{-1}\vy.
  \end{gather}
\end{Definition}

\begin{Definition}{preconditioner}
  A \define{preconditioner} is a matrix $\matb^{-1}$ such that the
  condition number of
  \begin{gather}
    \matb^{-1}\mata\qquad\text{or}\qquad \mata\matb^{-1}
  \end{gather}
  is much better than the one of $\mata$. Furthermore, the evaluation
  of $\matb^{-1}$ is at least much cheaper than the one of
  $\mata^{-1}$, ideally not much more expensive than the product
  $\mata\vx$.
\end{Definition}

\begin{Lemma}{preconditioner-symmetry}
  Let $\mata$ and $\matb$ be symmetric, positive definite matrices. Then, $\matb^{-1}\mata$ is self-adjoint with respect to the $\matb$-inner product, i.~e.
  \begin{gather}
    \scal(\matb^{-1}\mata\vx,\vx)_{\matb}
    = \scal(\mata\vx,\vx)_2
    = \scal(\vx,\mata\vx)_2
    = \scal(\vx,\matb\matb^{-1}\mata\vx)_2
    =\scal(\vx,\matb^{-1}\mata\vx)_{\matb}.
  \end{gather}
\end{Lemma}

\begin{remark}
  The term ``orthogonality'' in the Lanczos procedure can be in
  reference to any inner product, if orthogonalization is performed
  with that inner product. In that case, the property ``symmetric'' of
  the matrix must be replaced by ``self-adjoint with respect to the inner product''

  We can now construct a preconditioned conjugate gradient method by
  changing the inner products and introducing the residual of the
  preconditioned equation as a new auxiliary vector
  \begin{gather}
    \vz_{j} = \matb^{-1} \vr_{j} = \matb^{-1}\vb - \matb^{-1}\mata\vx_{j}.
  \end{gather}
  Then, we obtain
  \begin{align}
    \scal(\vz_j,\vz_j)_{\matb} &= \scal(\matb\matb^{-1}\vr_j,\vz_j)_2 = \scal(\vr_j,\vz_j)_2,\\
    \scal(\matb^{-1}\mata\vp_j,\vp_j)_{\matb} &= \scal(\mata\vp_j,\vp_j)_2 = \scal(\vp_j,\vp_j)_{\mata}.
  \end{align}
  Thus, the algorithm can be implemented without application of the
  matrix $\matb$, which may not be easy to implement.
\end{remark}

\begin{Algorithm*}{pcg}{Preconditioned Conjugate Gradient}
  \begin{algorithmic}[1]
    \State $\vr_0 = \vb-\mata \vx_0$, $\vz_0 = \matb^{-1}\vr_0$
    \State $\vp_0 \gets \vz_0$
    \For{$j=0,1,\dots$}
    \State $\alpha_j \gets \frac{\scal(\vr_j,\vz_j)}{\scal(\mata\vp_j,\vp_j)}$
    \State $\vx_{j+1} \gets \vx_j + \alpha_j \vp_j$
    \State $\vr_{j+1} \gets \vr_j - \alpha_j \mata \vp_j$
    \State $\vz_{j+1} \gets \matb^{-1} \vr_{j+1}$
    \State $\beta_j =\frac{\scal(\vr_{j+1},\vz_{j+1})}{\scal(\vr_j,\vz_j)}$
    \State $\vp_{j+1} = \vz_{j+1} + \beta_j \vp_j$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{todo}
  Middle of the stash conflict
\end{todo}

\begin{Algorithm*}{gmres}{Generalized Minimal Residual}
  Given $\vx^{(0)}\in\R^n$, the ``iterate'' $\vx^{(m)}$ of the
  \define{GMRES} method is computed by the following steps.
  \begin{enumerate}
  \item Compute the Arnoldi basis $\matv_{m+1}$ of
    $\krylov_{m+1}(\mata,\vr^{(0)})$ and the matrix $\overline\matH_m\in\R^{m+1\times m}$.
  \item Solve the least squares problem
    \begin{gather}
      \norm{\beta\ve_1 - \overline\matH_m \vy}_2 \stackrel{!}{=} \min
    \end{gather}
    in $\R^m$ with $\beta = \norm{\vr^{(0)}}_2$.
  \item Let
    \begin{gather}
      \vx^{(m)} = \vx^{(0)} + \matv_{m}\vy
    \end{gather}
  \end{enumerate}
\end{Algorithm*}

\begin{remark}
  The GMRES methof is not really an iterative method. indeed, the intermediate iterates $\vx^{(k)}$ may not even be computed, since the norm of the residual can be computed by \slideref{}{}
\end{remark}

\begin{todo}
  End of stash conflict
\end{todo}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
