\usepackage{algpseudocode}
\lstset{language=Python,numbers=left,resetmargins=true,xleftmargin=8pt,basicstyle=\small,numberstyle=\scriptsize}
\usetikzlibrary{svg.path}
\usetikzlibrary{matrix,fit}
\pgfdeclarelayer{bg}
\pgfsetlayers{bg,main}
\excludecomment{solution}
\input{mixed/fig/tikzsettings}

\title{Numerical Linear Algebra}
\author{Guido Kanschat}
\date{\today}

\def\esp#1{V_{#1}}

\begin{document}
\maketitle
\tableofcontents
\chapter{QR Factorization}
\label{chap:qr}
\input{ortho}

\chapter{Dense Algebraic Eigenvalue Problems}
\label{chap:dense-eigen}

\begin{intro}
  We refer to problems in linear algebra which allow storing the
  complete matrix as \define{dense linear algebra}. They are typically
  characterized by dimensions into the hundreds, possibly thousands,
  and any entry in the matrix may have a nonzero value. Such matrices
  are typically stored as a rectangular or quadratic array of numbers,
  and we can perform manipulations based on the matrix entry.

  In contrast, we will turn to \define{sparse linear algebra} in the
  later chapters, where dimensions got up to millions and billions
  ($10^9$). Since currently no computer on earth can store a matrix
  with $10^{18}$ entries, those matrices will be characterized by the
  fact that each row only contains very few nonzero entries, or that
  the matrix is not stored, but only available algorithmically in the
  form of a function performing the action $\vx\mapsto \mata
  \vx$. Thus, access to and manipulation of matrix entries is not
  possible, and we have to focus on methods only using the properties
  of the matrix as a linear mapping.
\end{intro}

\section{Mathematical background}
\subsection{Definition of Eigenvalue Problems}
\input{def-evp}
\section{Well-posedness of the EVP and bounds on eigenvalues}
\input{conditioning}

\section{Vector iterations}
\input{vector-iterations}

\section{The QR iteration}
\input{qr}

\section{Methods in real arithmetic}
\input{real-qr}

\section{Computing eigenvectors}
%\input{orthopoly}

\chapter{Solving Large Sparse Linear Systems}
\label{chap:sparse}

\section{Sparse matrices}

\input{sparse}

\section{Basic iterative methods}
\input{iterations}

\section{Krylov-space methods}
\input{Krylov}

\chapter{Large Sparse Eigenvalue Problems}
\label{chap:sparse-eigen}

\begin{intro}
  In this chapter we study algorithms for finding a small number
  $n_{\text{ev}}$ of eigenvalues and corresponding eigenvectors of a
  large matrix $\mata\in\Rnn$, where $n$ is large, for instance
  $n>10^6$. Like in the previous chapter, this is achieved by
  projecting the problem into smaller subspaces, where we can solve
  the eigenvalue problem using the methods of the first chapter.

  Thus, we start by transferring the concept of Galerkin approximation
  from \slideref{Definition}{galerkin-method} and then use again the
  Arnoldi and Lanczos process to generate subspaces.
\end{intro}

\section{Projection methods}
\input{ev-projection}

\section{The Lanczos method}
\input{ev-lanczos}

\section{The Arnoldi method}
\input{ev-arnoldi}


\appendix
\chapter{Basics from Linear Algebra}
\section{Vectors, bases, and matrices}

\begin{Definition}{parallel}
  We call two vectors $\vv,\vw$ in a vector space $V$ \define{parallel}, if
  there is a scalar factor $\alpha$ such that $\alpha \vv=\vw$. We write
  \begin{gather}
    \vv\parallel\vw.
  \end{gather}
  Mot that for normed vectors, we have $\alpha = \pm1$ if $V=\R^n$ and
  $\alpha = e^{i\phi}$ if $V=\C^n$.
\end{Definition}

\subsection{Matrix notation for bases}
\input{bases}
\subsection{Similarity transformations}
\input{similarity}
\section{Inner products and orthogonality}
\input{inner}
\section{Projections}
\input{projections}
\section{The determinant}
\input{determinant}

\chapter{Basics from Numerical Analysis}


\section{Matrix norms and spectral radius}

\begin{Definition*}{matrix-norm}{Matrix norm}
  The space of matrices $\Cnn$ is a vector space of dimension $n^2$ and thus any vector space norm is a norm on this space. For an actual matrix norm, we require compatibility with matrix multiplication, namely
  \begin{gather}
    \norm{\mata\matb} \le \norm{\mata}\norm{\matb},
  \end{gather}
  for any matrices where this makes sense. A matrix norm is called \define{operator norm} or \define{induced norm}, if $\mata\colon V\to W$ and
  \begin{gather}
    \norm\mata = \norm{\mata}_{V\to W} = \sup_{\vv\in V}\frac{\norm{\mata\vv}_W}{\norm{\vv}_V}.
  \end{gather}
\end{Definition*}

\begin{Definition}{spectral-radius}
  The \define{spectral radius} of a matrix $\mata\in\Cnn$ is the
  maximal absolute value of its eigenvalues, that is
  \begin{gather}
    \rho(\mata) = \max_{\lambda\in\sigma(\mata)} \abs{\lambda}.
  \end{gather}
\end{Definition}

\begin{Lemma*}{spectral-radius}{Properties of the spectral radius}
  For any matrix norm $\norm\cdot$ and for any matrix $\mata\in\Cnn$ there holds
  \begin{gather}
    \rho(\mata) = \lim_{k\to\infty} \norm{\mata^k}^{1/k}.
  \end{gather}

  The sequence $\vx^{(k)} = A^k\vx$ converges to zero for all
  $\vx\in\C^n$ if and only if $\rho(\mata)<1$.

  For any $\epsilon>0$
  there is a matrix norm $\norm\cdot$ such that
  \begin{gather}
    \norm{\mata} \le (1+\epsilon) \rho(\mata) \qquad \forall \mata\in\Cnn.
  \end{gather}
\end{Lemma*}

\begin{Definition}{matrix-condition}
  The \define{condition number} of a matrix $\mata\in\Cnn$ for a given
  matrix norm is
  \begin{gather*}
    \cond\mata = \norm{\mata}\norm{\mata^{-1}}.
  \end{gather*}
  In particular, we define the \define{spectral condition number}
 $\cond_2\mata = \norm{\mata}_2\norm{\mata^{-1}}_2$.
\end{Definition}

\begin{Example}{matrix-norms}
  The operator norm induced by the maximum norm is the \define{row sum norm}
  \begin{gather}
    \norm{\mata}_\infty = \max_i \sum_j \abs{a_{ij}}.
  \end{gather}
  The operator norm induced by the 1-norm is the \define{column sum norm}
  \begin{gather}
    \norm{\mata}_\infty = \max_j \sum_i \abs{a_{ij}}.
  \end{gather}
  The operator norm induced by the \putindex{Euclidean norm} is the \define{spectral norm}
  \begin{gather}
    \norm{\mata}_2 = \lambda_{\max}(\mata^*\mata).
  \end{gather}
\end{Example}

\section{Chebyshev polynomials}
\input{chebyshev}

\bibliographystyle{alpha}
\bibliography{all}
\printindex

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
