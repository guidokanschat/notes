\begin{intro}
  The facts in this chapter can be found in many textbooks on
  numerical linear algebra, for instance in~\cite[Chapter
  5]{GolubVanLoan83}. In particular, the QR factorization in section
  5.2, Housholder transformations and Givens rotations in 5.1 there.
\end{intro}

\section{The Gram-Schmidt Algorithm}

\begin{Algorithm*}{gram-schmidt}{Gram-Schmidt Orthogonalization}
  Let $\vx_1, \dots,\vx_k\in\C^n$ with $k\le n$ be linearly independent.

  For $j=1,\dots,k$
  \begin{align}
    \label{eq:ortho:gs:1}
    \vw_j &= \vx_j - \sum_{i=1}^{j-1} \scal(\vx_j,\vv_i)\vv_i\\
    \label{eq:ortho:gs:2}
    \vv_j &= \frac{\vw_j}{\norm{\vw_j}_2}.
  \end{align}
\end{Algorithm*}

\begin{Theorem}{gram-schmidt}
  If the input vectors $\vx_1, \dots,\vx_k\in\C^n$ are linearly
  independent, then the Gram-Schmidt orthogonalization generates an
  orthonormal set $\vv_1, \dots,\vv_k\in\C^n$. Furthermore, for
  $j=1,\dots,k$ there holds
  \begin{gather}
    \spann{\vv_1, \dots,\vv_j} = \spann{\vx_1, \dots,\vx_j}.
  \end{gather}
\end{Theorem}

\begin{Algorithm}{gram-schmidt-implementation}
  \slideref{Algorithm}{gram-schmidt} can be implemented in two mathematically equivalent ways:

  \hrulefill
  \vspace*{2mm}

  \begin{minipage}{.49\textwidth}
    \begin{algorithmic}[1]
      \For{$j=1,\dots,k$}
      \State $\vy = 0$
      \For{$i=1,\dots,j-1$}
      \State $r \gets \scal(\vx_j,\vv_i)$
      \State $\vy \gets \vy + r \vv_i$
      \EndFor
      \State $\vv_j = \frac{\vx_j-\vy}{\norm{\vx_j-\vy}_2}$
      \EndFor
    \end{algorithmic}
  \end{minipage}
  \begin{minipage}{.49\textwidth}
    \begin{algorithmic}[1]
      \For{$j=1,\dots,k$}
      \State $\vw \gets \vx_j$
      \For{$i=1,\dots,j-1$}
      \State $r \gets \scal(\vw,\vv_i)$
      \State $\vw \gets \vw- r \vv_i$
      \EndFor
      \State $\vv_j = \frac{\vw}{\norm{\vw}_2}$
      \EndFor
    \end{algorithmic}
  \end{minipage}
\end{Algorithm}


\begin{Problem}{gram-schmidt-implementation}
  \begin{enumerate}
  \item Implement both methods in
    \slideref{Algorithm}{gram-schmidt-implementation}.
  \item Apply them to the
    vectors $\vx_1,\dots,\vx_n\in\R^n$, where $n=10$ and
  \begin{gather*}
    \vx_j = \left(\frac1j,\frac1{j+1},\dots,\frac1{j+n-1}\right)^T.
  \end{gather*}
\item Compute the \define{Gramian matrix} $\matg$ with entries
  $g_{ij} = \scal(\vv_j,\vv_j)$ of the resulting vectors.
\item Discuss the result. Why are the off-diagonal entries not zero? Why are the algorithms different?
\item Rewrite the algorithms by repeating rows 2--7 with $\vv_j$ instead of $\vx_j$ (\define{reorthogonalization}). What changes?
  \end{enumerate}
\end{Problem}

\begin{remark}
  The previous problem shows, that the Gram-Schmidt algorithm is
  highly susceptible to round-off errors. This is particularly true to
  the implementation on the left. The one on the right, often called
  \define{modified Gram-Schmidt algorithm}\index{Gram-Schmidt
    Orthogonalization!modified}, albeit it is the more natural
  implementation of the same \slideref{Algorithm}{gram-schmidt}.

  The vectors in \slideref{Problem}{gram-schmidt-implementation} form
  the so-called \define{Hilbert matrix}, which is notoriously
  ill-conditioned. Nevertheless, the example shows that either
  orthogonalization is very hard or we have to search for a better
  algorithm.
\end{remark}

\begin{intro}
  If we give names $r_{ij} = \scal(\vx_j,\vv_j)$ for $i<j$ to the
  coefficients of the Gram-Schmidt algorithm and let
  $r_{jj} = \norm{\vw_j}_2$ in equation~\eqref{eq:ortho:gs:2}, then
  equation~\eqref{eq:ortho:gs:2} becomes
  \begin{gather*}
    \vx_j = r_{jj} \vv_j + \sum_{i=1}^{j-1} r_{ij} \vv_i
    = \sum_{i=1}^{j} r_{ij} \vv_i.
  \end{gather*}
  Moreover, with the matrix notation
  $\matx = (\vx_1,\dots,\vx_k)$ and $\matv=(\vv_1,\dots,\vv_k)$, we
  obtain the equation
  \begin{gather*}
    \matx = \matv \matr,
  \end{gather*}
  where $\matr \in \R^{k\times k}$ is the upper triangular matrix with
  entries $r_{ij}$ as defined above. This gives rise to the definition
  of the QR factorization of a matrix.
\end{intro}

\begin{Definition}{qr-decomposition}
  The \define{QR factorization} of a matrix $\mata\in\C^{m\times n}$
  with $m\ge n$ is the product
  \begin{gather}
    \mata = \matq\matr,
  \end{gather}
  such that $\matq \in\C^{m\times n}$ is unitary and
  $\matr\in \C^{n\times n}$ is upper triangular.
\end{Definition}

\begin{intro}
  The existence of a QR factorization of an invertible matrix $\mata$
  follows constructively from \slideref{Theorem}{gram-schmidt}. For a
  singular matrix, the column vectors are linearly dependent, which
  will result in $r_{jj}=0$ for some index $j$.
\begin{todo} % Indent on todo removed since it failed to compile otherwise
    QR for singular matrices?
\end{todo}
  The following lemma rephrases the same statement we have already
  learned about the Gram-Schmidt orthogonalization.
\end{intro}

\begin{Lemma}{qr-columns}
  Let $\mata = \matq\matr$. Then, the column vectors of $\mata$ and
  $\matq$ admit the relation
  \begin{gather}
    \va_j = \sum_{i=1}^j r_{ij} \vq_i.
  \end{gather}
  If $r_{ii}\neq 0$ for $i=1,\dots,j$, this relation is uniquely
  invertible. In particular,
  \begin{gather}
    \spann{\vq_1,\dots,\vq_j}
    =
    \spann{\va_1,\dots,\va_j}.
  \end{gather}
\end{Lemma}

\begin{Theorem}{qr-existence}
  Every matrix $\mata\in\C^{m\times n}$ with $m\ge n$ of full rank
  admits a QR factorization. The columns of the matrix $\matq$ are
  uniquely determined up to unit factors. The factorization is unique
  under the condition that for all $j$ there holds $r_{jj} > 0$.
\end{Theorem}

\begin{intro}
  The QR factorization can be used to solve linear systems. It will
  also be an important component of eigenvalue solvers in
  chapter~\ref{chap:dense-eigen}.

  It is thus important to find ways to compute QR factorizations in a
  numerically stable way. The remainder of this chapter is devoted to
  this question. The key will be to replace the projections of the
  Gram-Schmidt algorithm by orthogonal transformations, namely
  reflections and rotations.

  The construction of such methods starts with the idea that if I find
  an orthogonal transformation of the matrix $\mata$ such that
  $\matq^*\mata=\matr$ is upper triangular, then
  $\mata=\matq\matr$. What is left, is the construction of
  $\matq^*$. Note that by \slideref{Theorem}{qr-existence}, all
  possible matrices $\matq$ of such factorizations are essentially
  equal (columns are equal upto scalar factors of size unity).

  While these methods are often also subjects of introductory courses
  on numerical methods, we discuss them here in detail since we also
  need the complex versions.
\end{intro}

\begin{intro}
  The matrix $\matq$ of a QR factorization can be obtained by an
  algorithm progressing over the columns of $\mata$. It progresses as
  follows: first, find a matrix $\matq_1$ such that $\matq_1^*\mata$
  has the structure
  \begin{gather}
    \mata_1 = \matq_1^* \mata =
    \begin{pmatrix}
      * & * & * & \dots & * & * \\
      0 & * & * & \dots & * & * \\
      0 & * & * & \dots & * & * \\
      0 & * & * & \dots & * & * \\
      \vdots & \vdots & \vdots & \cdots & \vdots & \vdots \\
      0 & * & * & \dots & * & *
    \end{pmatrix}.
  \end{gather}
  Hence, in the matrix $\mata_1$ all elements of the first column have
  been eliminated. The stars indicate unknown values usually not
  identical with the entries of $\mata$.

  In the next step, find a matrix $\matq_2$ such that
  \begin{gather}
    \mata_2 = \matq_2^* \mata_1
    =
    \begin{pmatrix}
      1 & 0 \\ 0 & \widetilde\matq_2^*
    \end{pmatrix}
    \mata_1
    =
    \begin{pmatrix}
      x & x & x & \cdots & x & x \\
      0 & * & * & \cdots & * & * \\
      0 & 0 & * & \cdots & * & * \\
      0 & 0 & * & \cdots & * & * \\
      \vdots & \vdots & \vdots & \cdots & \vdots & \vdots \\
      0 & 0 & * & \cdots & * & * \\
    \end{pmatrix}.
  \end{gather}
  Since the upper left block of $\matq_2^*$ is the
  identity, the multiplication from the left does not change the first
  row of $\mata_1$, such that the values indicated with $x$ are equal
  to the corresponding entries of $\mata_1$. This algorithm continues with
  \begin{gather}
    \matq_3^* \mata_2 =
    \begin{pmatrix}
      1&&\\
      &1&\\
      &&\widetilde\matq_3^*
    \end{pmatrix}
    \mata_2
        =
    \begin{pmatrix}
      x & x & x & \dots & x & x \\
      0 & x & x & \dots & x & x \\
      0 & 0 & * & \dots & * & * \\
      0 & 0 & 0 & \dots & * & * \\
      \vdots & \vdots & \vdots & \cdots & \vdots & \vdots \\
      0 & 0 & 0 & \dots & * & * \\
    \end{pmatrix}.
    ,
  \end{gather}
  until
  \begin{gather}
    \matq_{n-1}\mata_{n-2}
    =
    \begin{pmatrix}
      \id_{n-2}&\\
      &\widetilde\matq_{n-1}^*
    \end{pmatrix}\mata_{n-2}
    =
    \begin{pmatrix}
      x & x & x & \dots & x & x \\
      0 & x & x & \dots & x & x \\
      0 & 0 & x & \dots & x & x \\
      0 & 0 & 0 & \dots & x & x \\
      \vdots & \vdots & \vdots & \cdots & \vdots & \vdots \\
      0 & 0 & 0 & \dots & * & * \\
      0 & 0 & 0 & \dots & 0 & * \\
    \end{pmatrix},
  \end{gather}
  where $\widetilde\matq_{n-1}$ is a 2-by-2 matrix.
\end{intro}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Householder reflection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Definition}{householder-transformation}
  The \define{Householder transformation}
  associated with a vector $\vw\in\C^n$ is
  \begin{gather}
    \matq_{\vw} = \id - 2\frac{\vw\vw^*}{\vw^*\vw}
  \end{gather}
  It is also called \define{Householder matrix} or, particularly in
  the real case, \define{Householder reflection}.
\end{Definition}

\begin{Lemma}{householder-symmetry}
  For any vector $\vw\in\C^n$ the Householder transformation
  $\matq_{\vw}$ is Hermitian and orthogonal, that is,
  \begin{gather}
    \matq_{\vw}^{-1} = \matq_{\vw}^* = \matq_{\vw}.
  \end{gather}
\end{Lemma}

\begin{proof}
  The complex symmetry follows immediately from the rule for the
  adjoints of matrix products. Furthermore,
  \begin{multline}
    \matq_{\vw}^2 = \left(\id - 2\frac{\vw\vw^*}{\vw^*\vw}\right)^2
    = \id - 4\frac{\vw\vw^*}{\vw^*\vw} + 4\frac{\vw\vw^*\vw\vw^*}{\vw^*\vw\vw^*\vw}
    \\
    = \id - 4\frac{\vw\vw^*}{\vw^*\vw} + 4\frac{\vw\vw^*}{\vw^*\vw} = \id.
  \end{multline}
\end{proof}

\begin{Lemma}{householder-qr}
  For any vector $\vy\in\C^n$ there are vectors $\vw_\phi\in\C^n$ such
  that $\matq_{\vw_\phi} \vy$ is a multiple of $\ve_1$.

  The vector of choice for numerical stability is
  \begin{gather}
    \vw = \vy + e^{i\phi} \norm{\vy}_2\ve_1,
  \end{gather}
  where $\phi$ is the phase of $y_1$.
\end{Lemma}

\begin{proof}
  The statement of the lemma says that for a suitable vector $\vw$ there is a complex number
  $\alpha$ such that $\matq_{\vw} \vy = \alpha \ve_1$. Since
  $\matq_{\vw}$ preserves the Euclidean norm, we already know
  \begin{gather}
    \abs{\alpha} = \norm{\vy}_2.
  \end{gather}
  We now construct the vector $\vw$. First, we want to achieve
  \begin{gather}
    \label{eq:ortho:householder:1}
    \alpha\ve_1 = \matq_{\vw} \vy
    = \vy - 2 \frac{\vw\vw^*\vy}{\vw^*\vw}
    = \vy - 2 \frac{\vw^*\vy}{\vw^*\vw}\vw.
  \end{gather}
  Thus, $\vw$ must be in the span of $\vy-\alpha \ve_1$. Since we divide by
  its norm, its length does not matter and we let for some angle $\phi$
  \begin{gather}
    \vw_\phi =
    \begin{pmatrix}
      y_1 - e^{i\phi} \norm{\vy}_2\\y_2\\\vdots\\y_n
    \end{pmatrix}.
  \end{gather}
  If we compare the leftmost and rightmost terms in~\eqref{eq:ortho:householder:1}, we see that the quotient of inner products must be equal to $\nicefrac12$ auch that $y_2,\dots,y_n$ are canceled. Since
  \begin{align}
    \vw^*\vy &= \norm{\vy}_2^2 - y_1 e^{-i\phi} \norm{\vy}_2
    \\
    \vw^*\vw &= 2\norm{\vy}_2^2 - (y_1^*e^{i\phi} + y_1 e^{-i\phi})\norm{\vy}_2,
  \end{align}
  this can only hold if $y_1 e^{-i\phi}$ is real or equivalently, if $\phi$ is the phase of the complex number $y_1$ or of $-y_1$.

  Since the computation of the first component of $\vw_\phi$ is prone to loss of significance, we choose $\phi$ as the phase of $-y_1$. In the real case, this simplifies to $e^{i\phi} = -1$.
\end{proof}
\begin{todo}
  GM: \\
  To avoid loss of significance in the computation of the first component of \(\vw_\phi\), we choose \(\phi\) as the phase of
  \(-y_1\). In the real case, this simplifies to \(e^{i \phi} = -1\).
\end{todo}

\begin{Algorithm}{householder-multiplication}
  Computation of $\matq_{\vw}\vx$:

  \hrulefill
  \vspace*{2mm}
  \begin{algorithmic}[1]
    \Require $\vx\in\C^n$, Householder vector $\vw\in \C^n$, $\beta = \nicefrac2{\norm{w}^2}$
    \State $\alpha \gets \beta \vw^*\vx$
    \State $\vx \gets \vx - \alpha \vw$
  \end{algorithmic}
   \hrulefill
  %\vspace*{2mm}

   This algorithm requires $2n+1$ multiplications and $2n$ additions
   compared to $n^2$ for a general matrix vector product. The
   ``matrix'' $\matq_{\vw}$ can be stored using $n+1$ floating point
   numbers.
\end{Algorithm}
\begin{todo}
  GM: The matrix can be stored in \(n\) floating point values, by storing \(w\). Storage of \(\beta\) is not required.
\end{todo}

\begin{intro}

\end{intro}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Givens rotation}

\begin{Definition}{givens}
  The real \textbf{Givens-Rotation}\index{Givens rotation!real} $\givens_{jk}$ for $j<k$ with angle $\theta$ is the matrix
  \begin{gather}
      \givens_{jk} =
    \begin{pmatrix}
      \id \\
      &c&\cdots&s\\
      &\vdots&\id &\vdots\\
      &-s&\cdots&c\\
      &&&&\id
    \end{pmatrix}
    \in\Rnn.
  \end{gather}
    where $c = \cos\theta$ and $s = \sin\theta$.
  The corresponding mapping $\givens_{jk}\colon x\mapsto y$ is defined by
  \begin{gather}
    y_i =
    \begin{cases}
      c x_j + s x_k & i=j\\
      -s x_j + c x_k & i=k\\
      x_i &\text{else}
    \end{cases}.
  \end{gather}
\end{Definition}

\begin{todo}
  GM
\begin{Definition}{givens}
  The real \textbf{Givens-Rotation}\index{Givens rotation!real} $\givens_{jk}$ for $j<k$ with angle $\theta$ is the matrix
  \begin{gather}
      \givens_{jk} =
    \begin{pmatrix}
      \id \\
      &c&\cdots&-s\\
      &\vdots&\id &\vdots\\
      &s&\cdots&c\\
      &&&&\id
    \end{pmatrix}
    \in\Rnn.
  \end{gather}
    where $c = \cos\theta$ and $s = \sin\theta$.
  The corresponding mapping $\givens_{jk}\colon x\mapsto y$ is defined by
  \begin{gather}
    y_i =
    \begin{cases}
      c x_j - s x_k & i=j\\
      s x_j + c x_k & i=k\\
      x_i &\text{else}
    \end{cases}.
  \end{gather}
\end{Definition}
\end{todo}

\begin{remark}
  The entries $c$ and $s$ of the rotation matrix are in rows and
  columns $j$ and $k$.  The identity matrices $\id$ are of
  corresponding dimensions.

  Applying $\givens_{jk}$ to a matrix $\mata$ from the left modifies rows  $j$ and $k$ of $\mata$. Thus, it is a row operation similar to Gauss elimination.

  The action of $\givens_{jk}$ on a vector corresponds to the rotation
  in the plane spanned by $\ve_j$ und $\ve_k$. Thus, it is sufficient to investigated $2\times2$-matrices.

  Note that the notation $\givens_{jk}$ misses the angle $\theta$. This
  is due to the fact that it is always determined such that $y_k=0$.
\end{remark}

\begin{Notation}{givens-specific}
  In the more general case, that a Givens rotation $\givens_{jk}^T$ is not chosen to eliminate $x_k$ using $x_j$, but more generally eliminating a number $b$ using a number $a$, we write more specifically
  \begin{gather}
    \givens_{jk}[a,b].
  \end{gather}
\end{Notation}

\begin{todo}
  GM:
  Note that this computation is only correct in the real case.
\end{todo}
\begin{Lemma}{givens-computation}
  Givens rotation $\givens_{jk}^T$ eliminates the second component of the vector
  $(x_j,x_k)^T$ by choosing
  \begin{gather}
    r = \sqrt{x_j^2+x_k^2},\qquad
    c = \frac{x_j}r,\quad s = \frac{x_k}r.
  \end{gather}
  We obtain
  \begin{gather}
    \begin{pmatrix}
      c & s \\ -s & c
    \end{pmatrix}^T
    \begin{pmatrix}
      x_j\\x_k
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
    .
  \end{gather}
\end{Lemma}
\begin{todo}
  GM
\begin{Lemma}{givens-computation}
  Givens rotation $\givens_{jk}^T$ eliminates the second component of the vector
  $(x_j,x_k)^T \in \R^2$ by choosing
  \begin{gather}
    r = \sqrt{x_j^2+x_k^2},\qquad
    c = \frac{x_j}r,\quad s = \frac{x_k}r.
  \end{gather}
  We obtain
  \begin{gather}
    \begin{pmatrix}
      c & -s \\ s & c
    \end{pmatrix}^T
    \begin{pmatrix}
      x_j\\x_k
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
    .
  \end{gather}
\end{Lemma}
\end{todo}


\begin{remark}
  Computation of $r$ in the previous lemma is prone to numerical
  overflow due to computation of $x_j^2$ or $x_k^2$, even if $r$
  itself is within the numerical range. For the implementation, we can
  use the function \lstinline!hypot!. It computes the hypothenuse of a
  right-angled triangle without overflow.
\end{remark}

\begin{Definition}{givens-complex}
  The complex \textbf{Givens-Rotation}\index{Givens rotation!complex} $\givens_{jk}$ for $j<k$ with angles $\varphi,\theta$ is the matrix
  \begin{gather}
      \givens_{jk} =
    \begin{pmatrix}
      \id \\
      &c&\cdots&s\\
      &\vdots&\id &\vdots\\
      &-\overline s&\cdots&c\\
      &&&&\id
    \end{pmatrix}
    \in\Cnn.
  \end{gather}
    where $c = \cos\theta$ and $s = e^{i\varphi}\sin\theta$.
\end{Definition}
\begin{todo}
  GM
\begin{Definition}{givens-complex}
  The complex \textbf{Givens-Rotation}\index{Givens rotation!complex} $\givens_{jk}$ for $j<k$ with angles $\varphi,\theta$ is the matrix
  \begin{gather}
      \givens_{jk} =
    \begin{pmatrix}
      \id \\
      &c&\cdots&-s\\
      &\vdots&\id &\vdots\\
      &\overline s&\cdots&c\\
      &&&&\id
    \end{pmatrix}
    \in\Cnn.
  \end{gather}
    where $c = \cos\theta$ and $s = e^{i\varphi}\sin\theta$.
\end{Definition}
\end{todo}
\begin{todo}
  GM:
  Added Lemma for complex Givens computation
\end{todo}
\begin{Lemma}{givens-computation-complex}
  Givens rotation $\givens_{jk}^T$ eliminates the second component of the vector
  $(x_j,x_k)^T$ by choosing
  \begin{gather}
    r = \sqrt{x_j^2+x_k^2},\qquad
    c = \frac{|x_j|}r,\quad s = \frac{x_j}{|x_j|}\frac{x_k}r.
  \end{gather}
  We obtain
  \begin{gather}
    \begin{pmatrix}
      c & -s \\ \overline{s} & c
    \end{pmatrix}^*
    \begin{pmatrix}
      x_j\\x_k
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
    .
  \end{gather}
\end{Lemma}

\begin{todo}
  GM:
  Ich habe nicht verstanden, wie man es so implementiert.
\end{todo}
\begin{todo}
  GM:
  The next two blocks need to be adjusted as well, if the Givens rotation has been defined with the sign on the wrong
  offdiagonal entry
\end{todo}
\begin{Lemma}{givens-computation-complex}
  Let $u,v\in \C$ such that $u=u_1+iu_2$ and $v=v_1+iv_2$. Then, the second component of $(u,v)^T$ can be eliminated, such that
  \begin{gather}
    \begin{pmatrix}
      c & s\\-\overline s&c
    \end{pmatrix}^*
    \begin{pmatrix}
      u\\v
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
  \end{gather}
  by three real Givens rotations
  \begin{gather}
    \begin{pmatrix}
      c_\alpha & s_\alpha\\-s_\alpha & c_\alpha
    \end{pmatrix}^T
    \begin{pmatrix}
      u_1\\u_2
    \end{pmatrix}
    =
    \begin{pmatrix}
      r_u\\0
    \end{pmatrix}
    ,\quad
    \begin{pmatrix}
      c_\beta & s_\beta\\-s_\beta & c_\beta
    \end{pmatrix}^T
    \begin{pmatrix}
      v_1\\v_2
    \end{pmatrix}
    =
    \begin{pmatrix}
      r_v\\0
    \end{pmatrix},
    \\
    \begin{pmatrix}
      c_\theta & s_\theta \\-s_\theta  & c_\theta
    \end{pmatrix}^T
    \begin{pmatrix}
      r_u\\r_v
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
    ,
  \end{gather}
  choosing $\varphi = \beta-\alpha$ and $c=c_\theta$,
  $s=s_\theta e^{i\phi}$.
\end{Lemma}

\begin{proof}
  Note that the first two rotations map a complex number to a real number, hence
  \begin{gather}
    u = r_u e^{-i\alpha}, \qquad v = r_v e^{-i\beta}.
  \end{gather}
  Let now $\varphi = \beta-\alpha$, then,
  \begin{gather}
    \begin{aligned}
      c u - s v
      &= c_\theta e^{-i\alpha} r_u - s_\theta e^{i\phi} e^{-i\beta} r_v
      &&= e^{-i\alpha}\bigl(c_\theta r_u - s_\theta r_v\bigr) = r,
      \\
      c v + \overline s u
      &= c_\theta e^{-i\beta} r_v + s_\theta e^{-i\phi} e^{-i\alpha} r_u
      &&= e^{-i\beta}\bigl(c_\theta r_v + s_\theta r_u\bigr) = 0.
    \end{aligned}
  \end{gather}
\end{proof}

\begin{todo}
  GM:
  Not correct for complex rotation
\end{todo}
\begin{Algorithm}{givens-multiplication}
  The multiplication
  \begin{gather}
    \givens_{jk}\mata\qquad\qquad\qquad\qquad \mata\givens_{jk}
  \end{gather}
  affects only two rows/columns of $\mata\in\C^{m\times n}$ and can be
  implemented with $6n/6m$ multiplications and additions.

  \hrulefill
  \vspace*{2mm}

  \begin{minipage}{.49\textwidth}
    \begin{algorithmic}[1]
      \Require $c,s$ Givens rotation
      \For{$i=1,\dots,n$}
      \State $\alpha \gets a_{ji}$
      \State $\beta \gets a_{ki}$
      \State $a_{ji} \gets c\alpha-s\beta$
      \State $a_{ki} \gets s\alpha+c\beta$
      \EndFor
    \end{algorithmic}
  \end{minipage}
  \begin{minipage}{.49\textwidth}
    \begin{algorithmic}[1]
      \Require $c,s$ Givens rotation
      \For{$i=1,\dots,n$}
      \State $\alpha \gets a_{ij}$
      \State $\beta \gets a_{ik}$
      \State $a_{ij} \gets c\alpha-s\beta$
      \State $a_{ik} \gets s\alpha+c\beta$
      \EndFor
    \end{algorithmic}
  \end{minipage}
\end{Algorithm}

\begin{todo}
  GM
\begin{Algorithm}{givens-multiplication}
  The multiplication
  \begin{gather}
    \givens_{jk}^*\mata\qquad\qquad\qquad\qquad \mata\givens_{jk}
  \end{gather}
  affects only two rows/columns of $\mata\in\C^{m\times n}$ and can be
  implemented with $6n/6m$ multiplications and additions.

  \hrulefill
  \vspace*{2mm}

  \begin{minipage}{.49\textwidth}
    \begin{algorithmic}[1]
      \Require $c,s$ Givens rotation
      \For{$i=1,\dots,n$}
      \State $\alpha \gets a_{ji}$
      \State $\beta \gets a_{ki}$
      \State $a_{ji} \gets c\alpha+s\beta$
      \State $a_{ki} \gets -\overline{s}\alpha+c\beta$
      \EndFor
    \end{algorithmic}
  \end{minipage}
  \begin{minipage}{.49\textwidth}
    \begin{algorithmic}[1]
      \Require $c,s$ Givens rotation
      \For{$i=1,\dots,n$}
      \State $\alpha \gets a_{ij}$
      \State $\beta \gets a_{ik}$
      \State $a_{ij} \gets c\alpha+\overline{s}\beta$
      \State $a_{ik} \gets -s\alpha+c\beta$
      \EndFor
    \end{algorithmic}
  \end{minipage}
\end{Algorithm}
\end{todo}

\begin{remark}
  There is a compressed way of storing $c$ and $s$ of a Givens rotation in a single number, see~\cite[5.1.11]{GolubVanLoan83}.
\end{remark}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
