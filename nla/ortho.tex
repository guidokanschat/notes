\begin{intro}
  The facts in this chapter can be found in many textbooks on
  numerical linear algebra, for instance in~\cite[Chapter
  5]{GolubVanLoan83}. In particular, the QR factorization in section
  5.2, Housholder transformations and Givens rotations in 5.1 there.
\end{intro}

\section{The Gram-Schmidt Algorithm}

\begin{Algorithm*}{gram-schmidt}{Gram-Schmidt Orthogonalization}
  Let $\vx_1, \dots,\vx_k\in\C^n$ with $k\le n$ be linearly independent.

  For $j=1,\dots,k$
  \begin{align}
    \label{eq:ortho:gs:1}
    \vw_j &= \vx_j - \sum_{i=1}^{j-1} \scal(\vx_j,\vv_i)\vv_i\\
    \label{eq:ortho:gs:2}
    \vv_j &= \frac{\vw_j}{\norm{\vw_j}_2}.
  \end{align}
\end{Algorithm*}

\begin{Theorem}{gram-schmidt}
  If the input vectors $\vx_1, \dots,\vx_k\in\C^n$ are linearly
  independent, then the Gram-Schmidt orthogonalization generates an
  orthonormal set $\vv_1, \dots,\vv_k\in\C^n$. Furthermore, for
  $j=1,\dots,k$ there holds
  \begin{gather}
    \spann{\vv_1, \dots,\vv_j} = \spann{\vx_1, \dots,\vx_j}.
  \end{gather}
\end{Theorem}

\begin{Algorithm}{gram-schmidt-implementation}
  \slideref{Algorithm}{gram-schmidt} can be implemented in two mathematically equivalent ways:

  \hrulefill
  \vspace*{2mm}

  \begin{minipage}{.49\textwidth}
    \begin{algorithmic}[1]
      \For{$j=1,\dots,k$}
      \State $\vy = 0$
      \For{$i=1,\dots,j-1$}
      \State $r \gets \scal(\vx_j,\vv_i)$
      \State $\vy \gets \vy + r \vv_i$
      \EndFor
      \State $\vv_j = \frac{\vx_j-\vy}{\norm{\vx_j-\vy}_2}$
      \EndFor
    \end{algorithmic}
  \end{minipage}
  \begin{minipage}{.49\textwidth}
    \begin{algorithmic}[1]
      \For{$j=1,\dots,k$}
      \State $\vw \gets \vx_j$
      \For{$i=1,\dots,j-1$}
      \State $r \gets \scal(\vw,\vv_i)$
      \State $\vw \gets \vw- r \vv_i$
      \EndFor
      \State $\vv_j = \frac{\vw}{\norm{\vw}_2}$
      \EndFor
    \end{algorithmic}
  \end{minipage}
\end{Algorithm}


\begin{Problem}{gram-schmidt-implementation}
  \begin{enumerate}
  \item Implement both methods in
    \slideref{Algorithm}{gram-schmidt-implementation}.
  \item Apply them to the
    vectors $\vx_1,\dots,\vx_n\in\R^n$, where $n=10$ and
  \begin{gather*}
    \vx_j = \left(\frac1j,\frac1{j+1},\dots,\frac1{j+n-1}\right)^T.
  \end{gather*}
\item Compute the \define{Gramian matrix} $\matg$ with entries
  $g_{ij} = \scal(\vv_j,\vv_j)$ of the resulting vectors.
\item Discuss the result. Why are the off-diagonal entries not zero? Why are the algorithms different?
\item Rewrite the algorithms by repeating rows 2--7 with $\vv_j$ instead of $\vx_j$ (\define{reorthogonalization}). What changes?
  \end{enumerate}
\end{Problem}

\begin{remark}
  The previous problem shows, that the Gram-Schmidt algorithm is
  highly susceptible to round-off errors. This is particularly true to
  the implementation on the left. The one on the right, often called
  \define{modified Gram-Schmidt algorithm}\index{Gram-Schmidt
    Orthogonalization!modified}, albeit it is the more natural
  implementation of the same \slideref{Algorithm}{gram-schmidt}.

  The vectors in \slideref{Problem}{gram-schmidt-implementation} form
  the so-called \define{Hilbert matrix}, which is notoriously
  ill-conditioned. Nevertheless, the example shows that either
  orthogonalization is very hard or we have to search for a better
  algorithm.
\end{remark}

\begin{intro}
  If we give names $r_{ij} = \scal(\vx_j,\vv_j)$ for $i<j$ to the
  coefficients of the Gram-Schmidt algorithm and let
  $r_{jj} = \norm{\vw_j}_2$ in equation~\eqref{eq:ortho:gs:2}, then
  equation~\eqref{eq:ortho:gs:2} becomes
  \begin{gather*}
    \vx_j = r_{jj} \vv_j + \sum_{i=1}^{j-1} r_{ij} \vv_i
    = \sum_{i=1}^{j} r_{ij} \vv_i.
  \end{gather*}
  Moreover, with the matrix notation
  $\matx = (\vx_1,\dots,\vx_k)$ and $\matv=(\vv_1,\dots,\vv_k)$, we
  obtain the equation
  \begin{gather*}
    \matx = \matv \matr,
  \end{gather*}
  where $\matr \in \R^{k\times k}$ is the upper triangular matrix with
  entries $r_{ij}$ as defined above. This gives rise to the definition
  of the QR factorization of a matrix.
\end{intro}

\begin{Definition}{qr-decomposition}
  The \define{QR factorization} of a matrix $\mata\in\C^{m\times n}$
  with $m\ge n$ is the product
  \begin{gather}
    \mata = \matq\matr,
  \end{gather}
  such that $\matq \in\C^{m\times n}$ is unitary and
  $\matr\in \C^{n\times n}$ is upper triangular.
\end{Definition}

\begin{intro}
  The existence of a QR factorization of an invertible matrix $\mata$
  follows constructively from \slideref{Theorem}{gram-schmidt}. For a
  singular matrix, the column vectors are linearly dependent, which
  will result in $r_{jj}=0$ for some index $j$.
\begin{todo} % Indent on todo removed since it failed to compile otherwise
    QR for singular matrices?
\end{todo}
  The following lemma rephrases the same statement we have already
  learned about the Gram-Schmidt orthogonalization.
\end{intro}

\begin{Lemma}{qr-columns}
  Let $\mata = \matq\matr$. Then, the column vectors of $\mata$ and
  $\matq$ admit the relation
  \begin{gather}
    \va_j = \sum_{i=1}^j r_{ij} \vq_i.
  \end{gather}
  If $r_{ii}\neq 0$ for $i=1,\dots,j$, this relation is uniquely
  invertible. In particular,
  \begin{gather}
    \spann{\vq_1,\dots,\vq_j}
    =
    \spann{\va_1,\dots,\va_j}.
  \end{gather}
\end{Lemma}

\begin{Theorem}{qr-existence}
  Every matrix $\mata\in\C^{m\times n}$ with $m\ge n$ of full rank
  admits a QR factorization. The columns of the matrix $\matq$ are
  uniquely determined up to unit factors. The factorization is unique
  under the condition that for all $j$ there holds $r_{jj} > 0$.
\end{Theorem}

\begin{intro}
  The QR factorization can be used to solve linear systems. It will
  also be an important component of eigenvalue solvers in
  chapter~\ref{chap:dense-eigen}.

  It is thus important to find ways to compute QR factorizations in a
  numerically stable way. The remainder of this chapter is devoted to
  this question. The key will be to replace the projections of the
  Gram-Schmidt algorithm by orthogonal transformations, namely
  reflections and rotations.

  The construction of such methods starts with the idea that if I find
  an orthogonal transformation of the matrix $\mata$ such that
  $\matq^*\mata=\matr$ is upper triangular, then
  $\mata=\matq\matr$. What is left, is the construction of
  $\matq^*$. Note that by \slideref{Theorem}{qr-existence}, all
  possible matrices $\matq$ of such factorizations are essentially
  equal (columns are equal upto scalar factors of size unity).

  While these methods are often also subjects of introductory courses
  on numerical methods, we discuss them here in detail since we also
  need the complex versions.
\end{intro}

\begin{intro}
  \label{intro:ortho:1}
  The matrix $\matq$ of a QR factorization can be obtained by an
  algorithm progressing over the columns of $\mata$. It progresses as
  follows: first, find a matrix $\matq_1$ such that $\matq_1^*\mata$
  has the structure
  \begin{gather}
    \mata_1 = \matq_1^* \mata =
    \begin{pmatrix}
      * & * & * & \dots & * & * \\
      0 & * & * & \dots & * & * \\
      0 & * & * & \dots & * & * \\
      0 & * & * & \dots & * & * \\
      \vdots & \vdots & \vdots & \cdots & \vdots & \vdots \\
      0 & * & * & \dots & * & *
    \end{pmatrix}.
  \end{gather}
  Hence, in the matrix $\mata_1$ all elements of the first column have
  been eliminated. The stars indicate unknown values usually not
  identical with the entries of $\mata$.

  In the next step, find a matrix $\matq_2$ such that
  \begin{gather}
    \mata_2 = \matq_2^* \mata_1
    =
    \begin{pmatrix}
      1 & 0 \\ 0 & \widetilde\matq_2^*
    \end{pmatrix}
    \mata_1
    =
    \begin{pmatrix}
      x & x & x & \cdots & x & x \\
      0 & * & * & \cdots & * & * \\
      0 & 0 & * & \cdots & * & * \\
      0 & 0 & * & \cdots & * & * \\
      \vdots & \vdots & \vdots & \cdots & \vdots & \vdots \\
      0 & 0 & * & \cdots & * & * \\
    \end{pmatrix}.
  \end{gather}
  Since the upper left block of $\matq_2^*$ is the
  identity, the multiplication from the left does not change the first
  row of $\mata_1$, such that the values indicated with $x$ are equal
  to the corresponding entries of $\mata_1$. This algorithm continues with
  \begin{gather}
    \matq_3^* \mata_2 =
    \begin{pmatrix}
      1&&\\
      &1&\\
      &&\widetilde\matq_3^*
    \end{pmatrix}
    \mata_2
        =
    \begin{pmatrix}
      x & x & x & \dots & x & x \\
      0 & x & x & \dots & x & x \\
      0 & 0 & * & \dots & * & * \\
      0 & 0 & 0 & \dots & * & * \\
      \vdots & \vdots & \vdots & \cdots & \vdots & \vdots \\
      0 & 0 & 0 & \dots & * & * \\
    \end{pmatrix}.
    ,
  \end{gather}
  until
  \begin{gather}
    \matq_{n-1}^*\mata_{n-2}
    =
    \begin{pmatrix}
      \id_{n-2}&\\
      &\widetilde\matq_{n-1}^*
    \end{pmatrix}\mata_{n-2}
    =
    \begin{pmatrix}
      x & x & x & \dots & x & x \\
      0 & x & x & \dots & x & x \\
      0 & 0 & x & \dots & x & x \\
      0 & 0 & 0 & \dots & x & x \\
      \vdots & \vdots & \vdots & \cdots & \vdots & \vdots \\
      0 & 0 & 0 & \dots & * & * \\
      0 & 0 & 0 & \dots & 0 & * \\
    \end{pmatrix} = \matr,
  \end{gather}
  where $\widetilde\matq_{n-1}$ is a 2-by-2 matrix.
\end{intro}

\begin{intro}
  \label{sec:ortho:qr:q-product}
  In the previous section, we computed $\matr$ as
  \begin{gather}
    \matr = \matq_{n-1}^* \cdots \matq_{2}^*\matq_{1}^*\mata,
  \end{gather}
  Thus, we get the product representation
  \begin{gather}
    \matq = \matq_{1} \matq_{2} \cdots \matq_{n-1}.
  \end{gather}
  Depending on the represention of the factors in this product, this
  product can be either computed as a matrix, or it can be ecaluated
  factor by factor.
\end{intro}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Householder reflection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Definition}{householder-transformation}
  The \define{Householder transformation}
  associated with a vector $\vw\in\C^n$ is
  \begin{gather}
    \matq_{\vw} = \id - 2\frac{\vw\vw^*}{\vw^*\vw}
  \end{gather}
  It is also called \define{Householder matrix} or, particularly in
  the real case, \define{Householder reflection}, since it reflects
  the vector $\vx$ at the subspace perpendicular to $\vw$.
\end{Definition}

\begin{Lemma}{householder-symmetry}
  For any vector $\vw\in\C^n$ the Householder transformation
  $\matq_{\vw}$ is Hermitian and orthogonal, that is,
  \begin{gather}
    \matq_{\vw}^{-1} = \matq_{\vw}^* = \matq_{\vw}.
  \end{gather}
\end{Lemma}

\begin{proof}
  The complex symmetry follows immediately from the rule for the
  adjoints of matrix products. Furthermore,
  \begin{multline}
    \matq_{\vw}^2 = \left(\id - 2\frac{\vw\vw^*}{\vw^*\vw}\right)^2
    = \id - 4\frac{\vw\vw^*}{\vw^*\vw} + 4\frac{\vw\vw^*\vw\vw^*}{\vw^*\vw\vw^*\vw}
    \\
    = \id - 4\frac{\vw\vw^*}{\vw^*\vw} + 4\frac{\vw\vw^*}{\vw^*\vw} = \id.
  \end{multline}
\end{proof}

\begin{Lemma}{householder-qr}
  For any vector $\vx\in\C^n$ there are vectors $\vw_\phi\in\C^n$ such
  that $\matq_{\vw_\phi} \vx$ is a multiple of $\ve_1$.

  The vector of choice for numerical stability is
  \begin{gather}
    \label{eq:ortho:householder:1}
    \vw = \vx + e^{i\phi} \norm{\vx}_2\ve_1,
  \end{gather}
  where $\phi$ is the phase of $x_1$. We call this vector \define{Householder vector} for $\vx$.
\end{Lemma}

\begin{proof}
  The statement of the lemma says that for a suitable vector $\vw$ there is a complex number
  $\alpha$ such that $\matq_{\vw} \vx = \alpha \ve_1$. Since
  $\matq_{\vw}$ preserves the Euclidean norm, we already know
  \begin{gather}
    \abs{\alpha} = \norm{\vx}_2.
  \end{gather}
  We now construct the vector $\vw$. First, we want to achieve
  \begin{gather}
    \label{eq:ortho:householder:1}
    \alpha\ve_1 = \matq_{\vw} \vx
    = \vx - 2 \frac{\vw\vw^*\vx}{\vw^*\vw}
    = \vx - 2 \frac{\vw^*\vx}{\vw^*\vw}\vw.
  \end{gather}
  Thus, $\vw$ must be in the span of $\vx-\alpha \ve_1$. Since we divide by
  its norm, its length does not matter and we let for some angle $\phi$
  \begin{gather}
    \vw_\phi =
    \begin{pmatrix}
      x_1 - e^{i\phi} \norm{\vx}_2\\x_2\\\vdots\\x_n
    \end{pmatrix}.
  \end{gather}
  If we compare the leftmost and rightmost terms
  in~\eqref{eq:ortho:householder:1}, we see that the quotient of inner
  products must be equal to $\nicefrac12$ such that $x_2,\dots,x_n$
  are canceled. Since
  \begin{align}
    \vw^*\vx &= \norm{\vx}_2^2 - x_1 e^{-i\phi} \norm{\vx}_2
    \\
    \vw^*\vw &= 2\norm{\vx}_2^2 - (x_1^*e^{i\phi} + x_1 e^{-i\phi})\norm{\vx}_2,
  \end{align}
  this can only hold if $x_1 e^{-i\phi}$ is real or equivalently, if
  $\phi$ is the phase of the complex number $x_1$ or of $-x_1$.

  Since the computation of the first component of $\vw_\phi$ is prone
  to loss of significance, we choose $\phi$ as the phase of $-x_1$. In
  the real case, this simplifies to $e^{i\phi} = -1$.
\end{proof}

\begin{todo}
  GM: \\
  To avoid loss of significance in the computation of the first component of \(\vw_\phi\), we choose \(\phi\) as the phase of
  \(-y_1\). In the real case, this simplifies to \(e^{i \phi} = -1\).
\end{todo}

\begin{intro}
  \label{intro:ortho:householder-storage}
  The Householder vector $\vw$ is determined up to its length, which
  means we have the freedom to normalize it to some criterion when we
  compute it. A common normalization is $\vw_1 = 1$, such that this
  value does not have to be stored. In its position, we can store the
  normalization factor $\beta = \nicefrac2{\norm{\vw}^2}$.

  Let $\vv$ be the vector computed according
  to~\eqref{eq:ortho:householder:1} without normalization. Then,
  \begin{gather}
    \label{eq:ortho:householder:2a}
    v_1 = x_1 \left(1+ \frac{\norm{\vx}}{\abs{x_1}}\right) =: \alpha.
  \end{gather}
  Hence,
  \begin{gather}
    \vw = \frac1{x_1 \left(1+ \frac{\norm{\vx}}{\abs{x_1}}\right)}
    \begin{pmatrix}
      v_1\\x_2\\\vdots\\x_n
    \end{pmatrix}
    =
    \begin{pmatrix}
      1 \\\nicefrac{x_2}{\alpha}\\\vdots\\\nicefrac{x_n}{\alpha}
    \end{pmatrix}
    .
  \end{gather}
  Since the ``1'' in the first row is known, there is no need to store
  it. This will be important for the implementation of the QR
  factorization below. The Householder reflection will thus be stored in the format
  \begin{gather}
    \label{eq:ortho:householder:2b}
    \vw(2:) = \left(\tfrac{x_2}{\alpha},\dots,\tfrac{x_n}{\alpha}\right)^T, \qquad \beta = \tfrac2{\norm{\vw}^2}.
  \end{gather}
  A functioning code will also have to distinguish some special
  cases. The first is, where $\norm{\vx(2:n)}=0$. In this case, we
  could indeed let $w = (1,0,\dots,0)^T$ and reflect the vector at
  itself. Thus, $\beta = 2$. A more sophisticated programmer might
  notice that no operation is necessary at all and therefore,
  $\matq_{\vw}$ should be the identity. But the identity is not a
  reflection and thus, the code for applying $\matq_{\vw}$ must handle
  this case. We can indicate it by letting $\beta=0$.

  Second, if $x_1=0$, we cannot compute $\alpha$ according to
  \eqref{eq:ortho:householder:2a}. This is not a problem though, since
  we can revert to the formula in\eqref{eq:ortho:householder:1}.

  Note that in all those cases, the vector $\vw$ in
  \eqref{eq:ortho:householder:2b} contains all information of
  $\matq_{\vw}$, but instead of $n^2$ numbers, we only store
  $n$. Therefore, we should \textbf{never} store a Householder
  reflection as an $n\times n$ field of numbers.
\end{intro}

\begin{Problem}{householder-compute}
  Write a function \textsc{householder\_compute} which takes a vector
  $\vx$, replaces $\vx(2:)$ by the Householder vector $\vw(2:0)$
  according to the discussion in
  section~\ref{intro:ortho:householder-storage}, and returns $\beta$.
\end{Problem}

\begin{Algorithm}{householder-multiplication}
  Computation of $\matq_{\vw}\vx$. The result is updated in the vector
  $\vx$. $\vw$ is in the format of \eqref{eq:ortho:householder:2b}.

  \hrulefill
  \vspace*{2mm}
  \begin{algorithmic}[1]
    \Function{householder}{$\vx,\beta,\vw$}
    \If{$\beta \neq 0$}
    \State $\alpha \gets \beta \bigl( x_1 + \vw(2:)^*\vx(2:)\bigr)$
    \State $x_1 \gets x_1 - \alpha$
    \State $\vx(2:) \gets \vx(2:) - \alpha \vw(2:)$
    \EndIf
    \EndFunction
  \end{algorithmic}
   \hrulefill
  %\vspace*{2mm}

   This algorithm requires $2n+1$ multiplications and $2n$ additions
   compared to $n^2$ for a general matrix vector product.
\end{Algorithm}
\begin{todo}
  GM: The matrix can be stored in \(n\) floating point values, by storing \(w\). Storage of \(\beta\) is not required.
\end{todo}

\begin{Problem}{householder-product}
  According to section~\ref{sec:ortho:qr:q-product}, the matrix
  $\matq$ of the QR factorization is a product of Householder
  matrices. Is it better to compute $\matq$ as a matrix or is it
  better to store and apply its factors separately? To answer this
  question, consider the follwoing steps for an $n\times n$ matrix:
  \begin{enumerate}
  \item How much storage do you need for the factors $\matq_1$ to $\matq_{n-1}$?
  \item How many operations does it require to apply the product
    $\matq_1\cdots\matq_{n-1}$ to a vector?
  \item How many operations do you need to compute $\matq$ as a matrix?
  \item Compare the two options.
  \end{enumerate}
  Note in these calculations, that the Householder vector $\vw_k$ used
  for $\matq_k$ only requires $n-k$ entries.
\end{Problem}

\begin{intro}
  When applying the Householder transformation to the first column of
  the matrix $\mata$, we already know that the subdiagonal entries of
  the first column of the result will be zero. Therefore, we can make
  use of this memory to store the Householder vector
  $\vw(2:)$ right when we compute it. $\beta$ we have to store separately.

  Then, we can apply $\matq_1$ to all other columns of $\mata$,
  effectively computing $\matq\mata$. This scheme can be continued
  towards the right and bottom of the matrix. In the end, the upper
  triangle of $\mata$ contains $\matr$ and the strict lower triangle
  contains the entries of the Householder vectors
  $\vw_j$. Grahphically, the algorithm proceeds like this, where
  $w_{j;i}$ is the $i$-th entry of the Householder vector $\vw_j$:
  \begin{multline}
    \begin{pmatrix}
      a_{11} & \cdots &a_{1n}\\
      \vdots&&\vdots\\
      a_{n1}&\cdots&a_{nn}
    \end{pmatrix}
    \to
    \begin{pmatrix}
      r_{11} &r_{12} &r_{13} &r_{14} \\
      w_{1;2} & * & * & * \\
      w_{1;3} & * & *& * \\
      w_{1;4} & * & *& * \\
    \end{pmatrix}
    \\
    \to
    \begin{pmatrix}
      r_{11} &r_{12} &r_{13} &r_{14} \\
      w_{1;2} & r_{22} & r_{23} & r_{24} \\
      w_{1;3} & w_{2;2} & * & * \\
      w_{1;4} & w_{2;3} & *& * \\
    \end{pmatrix}
    \to
    \begin{pmatrix}
      r_{11} &r_{12} &r_{13} &r_{14} \\
      w_{1;2} & r_{22} & r_{23} & r_{24} \\
      w_{1;3} & w_{2;2} & r_{33} & r_{34} \\
      w_{1;4} & w_{2;3} & w_{3;2} & r_{44} \\
    \end{pmatrix}
  \end{multline}
\end{intro}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Givens rotation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Definition}{givens}
  The real \textbf{Givens-Rotation}\index{Givens rotation!real}
  $\givens_{jk}$ for $j<k$ with angle $\theta$ is the matrix
  \begin{gather}
      \givens_{jk} =
    \begin{pmatrix}
      \id \\
      &c&\cdots&s\\
      &\vdots&\id &\vdots\\
      &-s&\cdots&c\\
      &&&&\id
    \end{pmatrix}
    \in\Rnn.
  \end{gather}
    where $c = \cos\theta$ and $s = \sin\theta$.
  The corresponding mapping $\givens_{jk}\colon x\mapsto y$ is defined by
  \begin{gather}
    y_i =
    \begin{cases}
      c x_j + s x_k & i=j\\
      -s x_j + c x_k & i=k\\
      x_i &\text{else}
    \end{cases}.
  \end{gather}
\end{Definition}

\begin{remark}
  The entries $c$ and $s$ of the rotation matrix are in rows and
  columns $j$ and $k$.  The identity matrices $\id$ are of
  corresponding dimensions.

  Applying $\givens_{jk}$ or $\givens_{jk}^*$ to a matrix $\mata$ from
  the left modifies rows $j$ and $k$ of $\mata$. Thus, it is a row
  operation similar to Gauss elimination.

  The action of $\givens_{jk}$ on a vector corresponds to the rotation
  in the plane spanned by $\ve_j$ und $\ve_k$. Thus, it is sufficient
  to investigate $2\times2$-matrices.

  Note that the notation $\givens_{jk}$ misses the angle $\theta$. This
  is due to the fact that it is always determined such that $y_k=0$.
\end{remark}

\begin{Notation}{givens-specific}
  In the more general case, that a Givens rotation $\givens_{jk}^T$ is
  not chosen to eliminate $x_k$ using $x_j$, but more generally
  eliminating a number $b$ using a number $a$, we write more
  specifically
  \begin{gather}
    \givens_{jk}[a,b].
  \end{gather}
\end{Notation}

\begin{Lemma}{givens-computation}
  The real Givens rotation $\givens_{jk}^T$ eliminates the second
  component of the vector $(x_j,x_k)^T$ by choosing
  \begin{gather}
    r = \sqrt{x_j^2+x_k^2},\qquad
    c = \frac{x_j}r,\quad s = -\frac{x_k}r.
  \end{gather}
  We obtain
  \begin{gather}
    \begin{pmatrix}
      c & s \\ -s & c
    \end{pmatrix}^T
    \begin{pmatrix}
      x_j\\x_k
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
    .
  \end{gather}
  \begin{todo}
  Furthermore, we have
  \begin{gather}
    c = -\frac{x_j}r,\quad s = -\frac{x_k}r
    \qquad \Rightarrow \qquad
    \begin{pmatrix}
      c & s \\ -s & c
    \end{pmatrix}^T
    \begin{pmatrix}
      x_j\\x_k
    \end{pmatrix}
    =
    \begin{pmatrix}
      -r\\0
    \end{pmatrix}
    .    
  \end{gather}    
  \end{todo}
\end{Lemma}

\begin{remark}
  Computation of $r$ in the previous lemma is prone to numerical
  overflow due to computation of $x_j^2$ or $x_k^2$, even if $r$
  itself is within the numerical range. For the implementation, we can
  use the function \lstinline!hypot!. It computes the hypothenuse of a
  right-angled triangle without overflow.

  An alternative implementation avoids the overflow, but changes the
  sign in some cases.
\end{remark}

\begin{Algorithm}{givens-computation}
  \begin{algorithmic}[1]
    \Function{compute\_givens}{$a$,$b$}
    \If{$b=0$}
    \State $c=1$, $s=0$
    \Else
    \If{$\abs{b}>\abs{a}$}
    \State $\tau=-\frac ab$, $s=\frac1{\sqrt{1+\tau^2}}$, $c=\tau s$
    \Else
    \State $\tau=-\frac ba$, $c=\frac1{\sqrt{1+\tau^2}}$, $s=\tau c$
    \EndIf
    \EndIf
    \State \Return $(c,s)$
    \EndFunction
  \end{algorithmic}
\end{Algorithm}

\begin{Definition}{givens-complex}
  The complex \textbf{Givens-Rotation}\index{Givens rotation!complex} $\givens_{jk}$ for $j<k$ with angles $\varphi,\theta$ is the matrix
  \begin{gather}
      \givens_{jk} =
    \begin{pmatrix}
      \id \\
      &c&\cdots&s\\
      &\vdots&\id &\vdots\\
      &-\overline s&\cdots&c\\
      &&&&\id
    \end{pmatrix}
    \in\Cnn.
  \end{gather}
    where $c = \cos\theta$ and $s = e^{i\varphi}\sin\theta$.
\end{Definition}

\begin{Lemma}{givens-computation-complex}
  Let $u,v\in \C$ such that $u=u_1+iu_2$ and $v=v_1+iv_2$. Then, the second component of $(u,v)^T$ can be eliminated, such that
  \begin{gather}
    \begin{pmatrix}
      c & s\\-\overline s&c
    \end{pmatrix}^*
    \begin{pmatrix}
      u\\v
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
  \end{gather}
  by three real Givens rotations
  \begin{gather}
    \begin{pmatrix}
      c_\alpha & s_\alpha\\-s_\alpha & c_\alpha
    \end{pmatrix}^T
    \begin{pmatrix}
      u_1\\u_2
    \end{pmatrix}
    =
    \begin{pmatrix}
      r_u\\0
    \end{pmatrix}
    ,\quad
    \begin{pmatrix}
      c_\beta & s_\beta\\-s_\beta & c_\beta
    \end{pmatrix}^T
    \begin{pmatrix}
      v_1\\v_2
    \end{pmatrix}
    =
    \begin{pmatrix}
      r_v\\0
    \end{pmatrix},
    \\
    \begin{pmatrix}
      c_\theta & s_\theta \\-s_\theta  & c_\theta
    \end{pmatrix}^T
    \begin{pmatrix}
      r_u\\r_v
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
    ,
  \end{gather}
  choosing $\varphi = \beta-\alpha$ and $c=c_\theta$,
  $s=s_\theta e^{i\phi}$.
\end{Lemma}

\begin{proof}
  Note that the first two rotations map a complex number to a real number, hence
  \begin{gather}
    u = r_u e^{-i\alpha}, \qquad v = r_v e^{-i\beta}.
  \end{gather}
  Let now $\varphi = \beta-\alpha$, then,
  \begin{gather}
    \begin{aligned}
      c u - s v
      &= c_\theta e^{-i\alpha} r_u - s_\theta e^{i\phi} e^{-i\beta} r_v
      &&= e^{-i\alpha}\bigl(c_\theta r_u - s_\theta r_v\bigr) = r,
      \\
      c v + \overline s u
      &= c_\theta e^{-i\beta} r_v + s_\theta e^{-i\phi} e^{-i\alpha} r_u
      &&= e^{-i\beta}\bigl(c_\theta r_v + s_\theta r_u\bigr) = 0.
    \end{aligned}
  \end{gather}
\end{proof}

\begin{Lemma}{givens-computation-complex}
  The complex Givens rotation $\givens_{jk}^*$ eliminates the second
  component of the vector $(x_j,x_k)^T$ by choosing
  \begin{gather}
    r = \sqrt{\abs{x_j}^2+\abs{x_k}^2},\qquad
    c = \frac{|x_j|}r,\quad s = - \frac{x_j}{|x_j|}\frac{\overline{x}_k}r.
  \end{gather}
  We obtain
  \begin{gather}
    \begin{pmatrix}
      c & s \\ -\overline{s} & c
    \end{pmatrix}^*
    \begin{pmatrix}
      x_j\\x_k
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
    .
  \end{gather}
\end{Lemma}

\begin{Algorithm}{givens-multiplication}
  The multiplication
  \begin{gather}
    \givens_{jk}^*\mata\qquad\qquad\qquad\qquad \mata\givens_{jk}
  \end{gather}
  affects only two rows/columns of $\mata\in\C^{m\times n}$ and can be
  implemented with $6n/6m$ multiplications and additions.

  \hrulefill
  \vspace*{2mm}

  \begin{minipage}{.49\textwidth}
    \begin{algorithmic}[1]
      \Require $c,s$ Givens rotation
      \For{$i=1,\dots,n$}
      \State $\alpha \gets a_{ji}$
      \State $\beta \gets a_{ki}$
      \State $a_{ji} \gets c\alpha-s\beta$
      \State $a_{ki} \gets \overline{s}\alpha+c\beta$
      \EndFor
    \end{algorithmic}
  \end{minipage}
  \begin{minipage}{.49\textwidth}
    \begin{algorithmic}[1]
      \Require $c,s$ Givens rotation
      \For{$i=1,\dots,n$}
      \State $\alpha \gets a_{ij}$
      \State $\beta \gets a_{ik}$
      \State $a_{ij} \gets c\alpha-\overline{s}\beta$
      \State $a_{ik} \gets s\alpha+c\beta$
      \EndFor
    \end{algorithmic}
  \end{minipage}
\end{Algorithm}

\begin{remark}
  There is a compressed way of storing $c$ and $s$ of a Givens
  rotation in a single number, see~\cite[5.1.11]{GolubVanLoan83}.
\end{remark}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
