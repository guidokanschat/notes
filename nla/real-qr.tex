\begin{intro}
  The multiplication of two complex numbers involves four
  multiplications of real numbers. Hence, if $\mata$ is a real matrix,
  we should avoid computing in complex arithmetic.

  This section consists of two parts. First, we study algorithms for
  Hermitian matrices, which are reduced to real tridiagonal
  matrices. Here, we obtain a very fast algorithm with a guaranteed
  convergence result.

  The second part is about nonsymmetric real matrices, where we face
  the challenge that eigenvalues and eigenvectors may be complex.
\end{intro}

\subsection{The real symmetric eigenvalue problem}

\begin{Lemma}{qr-tridiagonal}
  Let $\matt\in\Rnn$ be a real, symmetric, tridiagonal matrix and
  $\matq\matr=\matt$ its QR factorization. Then, $\tilde\matt=\matr\matq$ is also
  symmetric and tridiagonal. Furthermore, $\matr$ is zero except for
  its main and the first two upper diagonals.

  The same holds for the shifted version with $\sigma\in\R$,
  \begin{gather}
    \matq\matr = \matt-\sigma\id,\qquad \tilde\matt = \matr\matq+\sigma\id.
  \end{gather}
\end{Lemma}

\begin{proof}
  See homework.
\end{proof}

\begin{Remark}{tridiagonal-storage}
  While the mathematical structure of a tridiagonal matrix is that of
  a matrix with its multiplication properties and QR factorization and
  so on, its storage structure consists of two vectors, a diagonal
  vector $\va\in\R^n$ and the upper and lower diagonal
  $\vb\in\R^{n-1}$. Any efficient implementation of the tridiagonal QR
  iteration must use such a reduced storage structure.
\end{Remark}

\begin{Lemma*}{perfect-shift-sym}{Perfect shift}
  Let $\matt\in\Rnn$ be an unreduced, symmetric, tridiagonal matrix,
  $\sigma\in\sigma(\matt)$, and $\matq\matr=\matt-\sigma\id$ the
  shifted QR factorization. Then, $r_{nn}=0$. Furthermore, the last
  column of $\tilde\matt = \matr\matq+\sigma\id$ is equal to
  $\sigma\ve_n$.
\end{Lemma*}

%  \begin{todo}
%\begin{proof}
%  If $\matt$ is unreduced, the first $n-1$ columns of
%  $\matt-\sigma\id$ are linearly independent for any $\sigma$.
%\end{proof}
%  \end{todo}

\begin{Remark*}{real-symmetric-qr}{QR-Iteration for real, symmetric matrices}
  In this case, many things simplify
  \begin{enumerate}
  \item Hessenberg form is tridiagonal
  \item The Schur canonical form is
    \begin{gather}
      \mata = \matq^T\matd\matq
    \end{gather}
    with a real, diagonal matrix $\matd$
  \item QR factorization uses $\bigo(n)$ operations and $\matr$
    consists only of the main diagonal and one upper diagonal.
  \end{enumerate}
  Accumulating the matrix $\matq$ still needs $\bigo(n^2)$ operations and should be avoided!
\end{Remark*}

\begin{Algorithm*}{qr-explicit-shift}{QR iteration with explicit shift}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$ symmetric
    \State $\matt_0 = \matq_0^*\mata\matq_0$\Comment{tridiagonal}
    \For {$k=1,\ldots$ until convergence}
    \State $\matq_k\matr_k = \matt_{k-1} - \sigma_k\id$\Comment{QR factorization}
    \State $\matt_{k} = \matr_k\matq_k + \sigma_k\id$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{wilkinson-shift}
  Let
  \begin{gather}
    \matt =
    \begin{pmatrix}
      a_1&b_1\\
      b_1&\ddots&\ddots\\
      &\ddots&a_{n-1}&b_{n-1}\\
      &&b_{n-1}&a_n
    \end{pmatrix}.
  \end{gather}
  Then, the \putindex{Wilkinson shift} $\sigma$ can be computed as
  \begin{gather}
    \sigma = a_n + d - \operatorname{sign}(d) \sqrt{d^2+b_{n-1}^2},
    \qquad d=\frac{a_{n-1}-a_n}2.
  \end{gather}
\end{Lemma}

\begin{Algorithm*}{implicit-symmetric-shift}{Symmetric QR step with implicit shift}
  \begin{algorithmic}[1]
    \Require $\matt\in\Rnn$ symmetric, unreduced, tridiagonal; $\sigma\in\R$
    \State Compute $\matg_{12} = \matg_{12}[t_{11}-\sigma,t_{21}]$\Comment{Givens rotation}
    \State $\matt \gets \matg_{12}^T\matt\matg_{12}$
    \For {$k=2,\dots,n-1$} \Comment{Bulge chasing}
    \State $\matg_{k,k+1} = \matg_{k,k+1}[t_{k,k-1},t_{k+1,k-1}]$
    \State $\matt \gets \matg_{k,k+1}^T\matt\matg_{k,k+1}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Remark}{tridiagonal-bulge}
  Note that there is no storage space for the ``bulge element'' if the
  matrix is stored in the format of two vectors.

  Therefore, the bulge chasing algorithm must store the bulge element
  separately and keep track of its position.
\end{Remark}

% \begin{todo}
%\begin{example}
%    Graphical representation of bulge chasing
%\end{example}
%  \end{todo}

\begin{Theorem}{implicit-symmetric-shift}
  Given $\matt\in\Rnn$ symmetric, unreduced, and tridiagonal. Let
  \begin{gather}
    \matt^{(e)} = \matq^*\matt\matq,
    \qquad
    \matt^{(i)} = \matz^*\matt\matz,
  \end{gather}
  where $\matt_e$ is computed by the QR step with explicit shift and
  $\matz=\matg_1\matg_2\dots\matg_{n-1}$ is the matrix of the QR step
  with implicit shift. Then, there holds
  \begin{xalignat}2
    \vz_1&=\vq_1,\\
    \vz_i&=\pm \vq_i,& i&=2,\dots,n,\\
    \abs{t_{i,i-1}^{(i)}}&=\abs{t_{i,i-1}^{(e)}},& i&=2,\dots,n.
  \end{xalignat}
\end{Theorem}

\begin{proof}
  Without loss of generality, we can assume that the QR factorization
  in the explicit shift step is computed by Givens rotations.  Then,
  the matrices $\matq$ and $\matz$ are both defined as a product of
  Givens rotations $\matg_{12}\matg_{23}\dots\matg_{k-1,k}$, where the
  first column is defined by the first rotation only. And the first
  rotation matrix is the same for both algorithms, such that
  $\vz_1 = \vq_1$. For the remaining results, we can use the
  \putindex{Implicit Q Theorem}.
\end{proof}

\begin{Theorem}{wilkinson-convergence}
  The QR iteration with Wilkinson shift and deflation converges for
  every symmetric, tridiagonal matrix $\mata$.

  Asymptotically, the convergence is quadratic, that is,
  \begin{gather}
    \abs{h_{nn}^{(k+1)} - \lambda_n} \le c \abs{h_{nn}^{(k)} - \lambda_n}^2.
  \end{gather}

  In many cases, it is even cubic.
\end{Theorem}

\begin{proof}
  The proof is straight from~\cite{Wilkinson68}.  Applying Givens
  rotations $G_{n-1,n}\dots G_{12}$ to the matrix
  $\overline \mata = \mata^{(k)} - \sigma \id$ with diagonal
  $\overline a_j = a_{jj}-\sigma$ and off-diagonal
  $b_j = a_{j-1,j} = a_{j,j-1}$, we obtain
  \begin{gather}
    \matr^{(k)} =
    \begin{pmatrix}
      p_1 & q_1 & r_1 \\
      0 &p_2 & q_2 & r_2 \\
      & 0 & p_3 & q_3 &r_3 \\
      && 0 & p_4 & q_4 & r_4 \\
      &&& \ddots & \ddots & \ddots &\ddots \\
      &&&& 0 & p_{n-1} & q_{n-1} \\
      &&&&& 0 & x_n
    \end{pmatrix},
  \end{gather}
  where with entries generated inductively from $x_1 = \overline a_1$,
  $y_1= \overline b_2$ by
  \begin{xalignat}2
    \label{eq:real:wilkinson:1}
    p_{j-1} &= \sqrt{x_{j-1} +b_{j}}\\
    \label{eq:real:wilkinson:2}
    c_{j} &= \tfrac{x_{j-1}}{p_{j-1}}
    & s_{j} &= \tfrac{-b_{j}}{p_{j-1}}
    \\
    \label{eq:real:wilkinson:3}
    x_j &= c_j \overline a_j - s_j y_{j-1}
    & y_j &= c_j b_{j+1}.
  \end{xalignat}
  Here, $c_j$ and $s_j$ are the cosine and sine from
  $\givens_{j-1,j}$. The elements $q_j$ and $r_j$ do not contribute to
  the argument and are therefore not computed here.

  Computing now the product
  $\mata^{(k+1)} = \matr\givens_{12}^*\dots\givens_{n-1,n}^* +
  \sigma\id$ with entries $\tilde a_{j}$ and $\tilde b_j$, we obtain
  \begin{gather}
    \mata^{(k+1)} =
    \begin{pmatrix}
      \tilde a_1 & \tilde b_2 \\
      \tilde b_2 & \tilde a_2 & \tilde b_3 \\
      & \tilde b_3 & \tilde a_3 & \tilde b_4 \\
      && \tilde b_4 & \tilde a_4 & \tilde b_5 \\
      &&& \ddots & \ddots &\ddots \\
      &&&& \tilde b_{n-1} & \tilde a_{n-1}& \tilde b_n \\
      &&&&&  \tilde b_n & \tilde a_n
    \end{pmatrix},
  \end{gather}
  where
  \begin{xalignat}2
    \label{eq:real:wilkinson:5}
    \tilde a_n &= c_n x_n + \sigma
    & \tilde b_n &= s_n x_n\\
    \label{eq:real:wilkinson:6}
    \tilde b_j &= s_j p_j & j &= 2,\dots,n-1.
  \end{xalignat}

  Since $\sigma$ is a root of t he characteristic polynomial of the
  lower right block, we have for the entries of $\mata^{(k)}-\sigma\id$
  \begin{gather}
    \label{eq:real:wilkinson:4}
    \overline a_{n-1}\overline a_n - b_n^2 = 0,
    \qquad\text{or}\qquad
    \overline a_n = \frac{b_n^2}{\overline a_{n-1}}
  \end{gather}
  Furthermore, since $\sigma$ is the closest eigenvalue to $a_n$, we have
  \begin{gather}
    \label{eq:real:wilkinson:10}
    \abs{\overline a_{n-1}} \ge \abs{b_n}.
  \end{gather}
  We now show that the product $\abs{b_{n-1}^{(k)}b_{n}^{(k)}}$ is
  monotonically decreasing. To this end, we observe
  \begin{align}
    x_n
    &= c_n \overline a_n - s_n y_{n-1}\\
    &= c_n\frac{b_n^2}{\overline a_{n-1}} - s_n c_{n-1} b_n\\
    &= \frac{b_n}{\overline a_{n-1}}
      \left(c_n b_n - s_n c_{n-1} \overline a_{n-1}\right)\\
    &= \frac{b_n}{\overline a_{n-1}}
      \left(s_n x_{n-1} - s_n c_{n-1} \overline a_{n-1}\right)\\
    &= \frac{b_n}{\overline a_{n-1}}
      \left(s_n c_{n-1} \overline a_{n-1} - s_n s_{n-1}y_{n-2}
      - s_n c_{n-1} \overline a_{n-1}\right)\\
    &= - \frac{b_n}{\overline a_{n-1}} s_n s_{n-1}y_{n-2}\\
    &= - \frac{b_n}{\overline a_{n-1}} s_n s_{n-1} c_{n-2} b_{n-1}.
  \end{align}
  Here, we have used the definition of $x_n$ and $x_{n-1}$
  in~\eqref{eq:real:wilkinson:3} in the first and firth line,
  respectively. The definition of $y_{n-1}$ and $y_{n-2}$ in
  equation~\eqref{eq:real:wilkinson:3} was used in the second and
  seventh line, respectively.  In the second line, we also used
  equation~\eqref{eq:real:wilkinson:4}. In the fourth line, we resolve
  both equations in~\eqref{eq:real:wilkinson:2} for $x_{n-1}$ and
  $b_n$ to obtain
  \begin{gather}
    c_n e_n = s_n x_{n-1}.
  \end{gather}

Now that we are looking at the iteration, all variables will be at
step $k$ unless explicitly noted. By~\eqref{eq:real:wilkinson:5},
\begin{gather}
  b_n^{k+1} = s_n x_n
  = - \frac{b_n}{\overline a_{n-1}} s_n^2 s_{n-1} c_{n-2} b_{n-1}.
\end{gather}
Since all other factors are bounded by one, we obtain
\begin{gather}
  \label{eq:real:wilkinson:7}
  \abs*{b_n^{(k+1)}} \le \abs*{b_{n-1}^{(k)}}.
\end{gather}
By~\eqref{eq:real:wilkinson:6} and the definition of $s_{n}$
in~\eqref{eq:real:wilkinson:2}, we have
\begin{gather}
  b_{n-1}^{(k+1)} = s_{n-1} p_{n-1} = \frac{s_{n-1}b_n}{s_n}.
\end{gather}
Hence,
\begin{align}
  \abs*{b_{n-1}^{(k+1)}b_{n}^{(k+1)}}
  &= \abs*{\frac{s_{n-1}b_{n}}{s_n}
    \frac{b_nc_{n-2}s_n^2s_{n-1}b_{n-1}}{\overline a_{n-1}}}\\
  \label{eq:real:wilkinson:8}
  &= \abs*{b_{n-1}^{(k)}b_{n}^{(k)}}\abs*{\frac{b_n}{\overline a_{n-1}}}
    \abs*{c_{n-2}s_{n-1}^2s_n}\\
  \label{eq:real:wilkinson:9}
    &\le \abs*{b_{n-1}^{(k)}b_{n}^{(k)}}.
\end{align}
Thus, the sequence $\abs{b_{n-1}^{(k)}b_{n}^{(k)}}$ is monotonically
decreasing and, since it is bounded from below, convergent.
Next, we rule out the case where the limit is nonzero. Let
us make this assumption and derive a contradiction. We have
\begin{gather}
  \lim_{k\to\infty}
  \frac{\abs*{b_{n-1}^{(k+1)}b_{n}^{(k+1)}}}%
  {\abs*{b_{n-1}^{(k)}b_{n}^{(k+1)}}}
  = 1,
\end{gather}
which by~\eqref{eq:real:wilkinson:8} implies
\begin{gather}
  \abs*{\frac{b_n}{\overline a_{n-1}}}
  \abs*{c_{n-2}}\abs*{s_{n-1}^2}\abs*{s_n} \to 1.
\end{gather}

Since each of these terms is bounded by one, they must converge to one
individually,
\begin{gather}
  \abs*{\frac{b_n}{\overline a_{n-1}}} \to 1,\qquad
  \abs*{c_{n-2}} \to 1,\qquad
  \abs*{s_{n-1}^2} \to 1,\qquad
  \abs*{s_n} \to 1.
\end{gather}
Since the two sines converge to one, the corresponding cosines converge
\begin{gather}
  \abs*{c_{n-1}^2} \to 0,\qquad \abs*{c_n} \to 0.
\end{gather}
Furthermore,
\begin{gather}
  \abs*{s_n} = \frac{b_n^2}{b_n^2+x_n^2} \to 1
\end{gather}
implies $\abs{x_{n-1}} \to 0$ and
\begin{gather}
  0 = \lim x_{n-1}
  = \lim \Bigl(c_{n-1}\overline a_{n-1} - c_{n-2}s_{n-1}b_{n-1}\Bigr)
  = 0 - 1\cdot \lim b_{n-1},
\end{gather}
yields the contradiction through $b_{n-1} \to 0$.

Hence, the limit is zero and we observe that for any given $\epsilon$
there will be an index $k$ such that $\abs{b_{n-1}^{(k)}b_{n}^{(k)}}$,
which in turn implies that either $\abs{b_{n}^{(k)}}<\epsilon$ or
$\abs{b_{n-1}^{(k)}}<\epsilon$. In the second case, we
apply~\eqref{eq:real:wilkinson:6} to deduce that
$\abs{b_{n}^{(k+1)}}<\epsilon$. In both cases, we reach a regime where
the proof of quadratic convergence below will apply.

Now we tend to the second part of the theorem, namely asymptotic
quadratic convergence. To this end, we assume that the iteration has yielded a matrix
\begin{gather}
  \mata^{(k)} =
  \begin{pmatrix}
    \matb &
    \begin{matrix}
      0\\ \epsilon
    \end{matrix}
    \\
    \begin{matrix}
      0 & \epsilon
    \end{matrix}
    & \frac{\epsilon^2}{\overline a_{n-1}}
  \end{pmatrix}
  =
  \begin{pmatrix}
    \matb&\\&\frac{\epsilon^2}{\overline a_{n-1}}
  \end{pmatrix}
  +
  \begin{pmatrix}
    \\
    &&\epsilon\\
    &\epsilon
  \end{pmatrix}
  .
\end{gather}
Hence, we can use the Bauer-Fike theorem and its
~\slideref{Corollary}{conditioning-eigenvalues-normal} to estimate the
eigenvalue $\lambda_1,\dots,\lambda_n$ of $\mata^{(k)}$ which are also
the eigenvalues of $\mata$. Let $\mu_1,\dots,\mu_{n-1}$ be the
eigenvalues of $\matb$. Then, there is an ordering such that
\begin{gather}
  \begin{aligned}
  \abs*{\lambda_i-\mu_i} & \le \epsilon & i&=1,\dots,n-1\\
  \abs*{\lambda_n - \tfrac{\epsilon^2}{\overline a_{n-1}}} &\le\epsilon.
  \end{aligned}
\end{gather}
All eigenvalues of an unreduced Hessenberg matrix are simple, hence
there is a gap between $\lambda_n$ and all other eigenvalues of
$\mata$, say $\abs{\lambda_i-\lambda_n}\ge \delta$.  Since
\begin{gather}
  \mu_i = \mu_i = \lambda_i+\lambda_i-\lambda_n+\lambda_n
  -\tfrac{\epsilon^2}{\overline a_{n-1}}+\tfrac{\epsilon^2}{\overline a_{n-1}},
\end{gather}
there holds
\begin{align}
  \abs*{\mu_i}
  &\ge \abs*{\lambda_i-\lambda_n} - \abs*{\mu_i-\lambda_i}
    - \abs*{\lambda_n - \tfrac{\epsilon^2}{\overline a_{n-1}}}
    - \abs*{\tfrac{\epsilon^2}{\overline a_{n-1}}}\\
  &\ge \delta - \epsilon - \epsilon -\epsilon = \delta-3\epsilon.
\end{align}
We apply QR factorization to $\mata^{(k)}$ to obtain
$\mata^{(k+1)}$. Before the last givens rotation, the matrix is
\begin{gather}
  \begin{pmatrix}
    \ddots&\ddots&\\
    * & x_{n-1} & \epsilon c_{n-1}\\
    & \epsilon & \tfrac{\epsilon^2}{\overline a_{n-1}}
  \end{pmatrix},
\end{gather}
where by \slideref{Lemma}{tridiagonal-rnn} $\abs{x_{n-1}} \ge \delta-3\epsilon$. Since
\begin{gather}
  \abs{s_n} = \frac{\epsilon}{\sqrt{x_{n-1}^2+\epsilon^2}} \le \frac{\epsilon}{\delta-3\epsilon},
\end{gather}
we conclude
\begin{align}
  \abs*{b_n^{k+1}}
  &= \left(\frac{\epsilon^2}{\overline a_{n-1}} - \epsilon c_{n-1}s_n \right) s_n\\
  &\le \frac{\epsilon^2}{\overline a_{n-1}} \frac{\epsilon}{\delta-3\epsilon}
    + \frac{\epsilon^3}{(\delta-3\epsilon)^2}.
\end{align}
The second term behaves like $\epsilon^3$ as $\epsilon\to0$. For the
first term, we can use~\eqref{eq:real:wilkinson:10} to bound
$\abs{\overline a_{n-1}}\ge\epsilon$ and thus get a term behaving like
$\epsilon^2$, which is the statement of the theorem. On the other
hand, this is not, what is observed when the algorithm is used. The
reason for this discrepancy lies in the fact that the
estimate~\eqref{eq:real:wilkinson:10} is too pessimistic and that the
Wilkinson shift approximates $a_n$, not $a_{n-1}$. But then,
$\abs{\overline a_{n-1}}$ is bounded from below independent of
$\epsilon$. Unfortunately, this behavior cannot be guaranteed.
\end{proof}

\begin{Lemma}{tridiagonal-rnn}
  Let $\mata\in\Rnn$ be symmetric and tridiagonal. If
  $\matq\matr=\mata$ is its QR factorization, then there holds
  \begin{gather}
    \abs{r_{nn}} \ge \min_{\lambda\in\sigma(\mata)} \abs{\lambda}.
  \end{gather}
\end{Lemma}

\begin{proof}
  There holds
  \begin{gather}
    \abs*{r_{nn}} = \norm*{\matr^*\ve_n}_2 = \norm*{\mata\matq^*\ve_n}_2.
  \end{gather}
  Since by \slideref{Theorem}{minmax}
  \begin{gather}
    \min_{\lambda\in\sigma(\mata)}\abs*{\lambda}
    = \min_{\norm{\vx}=1} \norm{\mata\vx},
  \end{gather}
  we immediately conclude the result.
\end{proof}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The eigenvalue problem for nonsymmetric real matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Theorem*}{real-schur-form}{The real Schur form}
  For every matrix $\mata\in \Rnn$ there is an orthogonal matrix
  $\matq\in\Rnn$ and a matrix $\matr\in\Rnn$ such that
  \begin{gather}
    \mata = \matq\matr\matq^*,
    \qquad
    \matr =
    \begin{pmatrix}
      R_{11} &* & *&*\\
      &R_{22}&*&*\\
      &&\ddots&*\\
      &&& R_{jj}
    \end{pmatrix},
  \end{gather}
  where the diagonal blocks are either of dimension one containing the
  real eigenvalues or of dimension 2 for complex conjugate eigenvalue
  pairs. The latter correspond to scaled rotation matrices with the
  according eigenvalue pair.
\end{Theorem*}

\begin{intro}{double-shift}
  When applying the QR step with Wilkinson shift, the shift parameter
  might be complex, thus leading to a bad approximation and
  consequently to slow convergence. Therefore, we have to circumvent
  this situation and find a working method in real arithmetic.
%  Using double shifts, the QR-iteration can be made to converge to the
%  real Schur form using double shifts in real arithmetic. This method
%  is also known as the \define{Francis QR step}~\cite[Algorithm
%  7.5-1]{GolubVanLoan83}.
\end{intro}

\begin{Algorithm*}{double-shift-step}{Explicit double-shift QR step}
  \begin{algorithmic}[1]
    \Require $\matH\in\Rnn$ Hessenberg form, $\sigma_1,\sigma_2\in\C$ shifts
    \State $\matq_1\matr_1 \gets \matH - \sigma_1\id$ \Comment{QR factorization}
    \State $\matH_1 \gets \matr_1\matq_1 + \sigma_1\id$
    \State $\matq_2\matr_2 \gets \matH_1 - \sigma_2\id$ \Comment{QR factorization}
    \State $\matH_2 \gets \matr_2\matq_2 + \sigma_2\id$
  \end{algorithmic}
\end{Algorithm*}

\begin{Remark}{explicit-double-shift-no}
  The explicit double-shift QR step is prone to introduce imaginary
  parts into the matrices due to roundoff errors.

  Never implement the explicit double step! We only introduced it to
  develop the algorithm.
\end{Remark}

\begin{Lemma}{double-shift-matrix}
  Let $\sigma_1,\sigma_2\in\C$ be the eigenvalues of the $2\times2$-matrix
  \begin{gather}
    \matg =
    \begin{pmatrix}
      h_{n-1,n-1}&h_{n-1,n}\\h_{n,n-1}&h_{n,n}
    \end{pmatrix}.
  \end{gather}
  Then, the unitary matrices $\matq_1,\matq_2$ of the double shift
  algorithm with shifts $\sigma_1,\sigma_2$ can be chosen such that
  $\matz = \matq_1\matq_2$ and thus $\matH_2 = \matz^T\matH\matz$ are
  real matrices in exact arithmetic.
\end{Lemma}

\begin{proof}
  First, we realize (homework) that
  \begin{gather}
    \matq_1\matq_2\matr_2\matr_1 = \matm = (\matH-\sigma_1\id)(\matH-\sigma_2\id).
  \end{gather}
  Hence,
  \begin{gather}
    \matm = \matH^2-s\matH+t\id,
  \end{gather}
  where
  \begin{gather}
    \begin{aligned}
      s &= \sigma_1+\sigma_2 = \operatorname{tr}(\matg)&\in&\R,\\
      t &= \sigma_1\sigma_2 = \det(\matg)&\in&\R.
    \end{aligned}
  \end{gather}
  Thus, $\matm\in\Rnn$. Since there is a real QR factorization of
  $\matm$, we can choose $\matz = \matq_1\matq_2\in\Rnn$. Thus, we
  conclude
  \begin{gather}
    \matH_2 = \matq_2^*\matH_1\matq_2 = (\matq_1\matq_2)^*\matH(\matq_1\matq_2) = \matz^T\matH\matz.
  \end{gather}
\end{proof}

\begin{remark}
  The explicit double step has several drawbacks. First, the algorithm
  must choose $\matq_1$ and $\matq_2$ such that their product is
  real. But then, roundoff errors will cause imaginary contributions
  in the result of the double step.

  We could also explicitly compute $\matm$ and then $\matz$ by real QR
  factorization. But here, we need a matrix vector product with
  $\bigo(n^3)$ operations.
\end{remark}

\begin{Theorem}{implicit-double-shift}{Implicit double-shift}
  The following QR step is essentially equivalent to the explicit double-shift:
  \begin{enumerate}
  \item Compute the first column of $\vm_1$ of $\matm$.
  \item Compute a Householder matrix $\matq_0$ such that $\matq_0\vm_1$ is a multiple of $\ve_1$.
  \item Compute Householder matrices $\matq_1,\dots,\matq_{n-2}$ such
    that for $\matp = \matq_0\dots\matq_{n-2}$ there holds
    \begin{enumerate}
    \item $\matp^T\matH\matp$ is a Hessenberg matrix
    \item The first columns of $\matp$ and of $\matz = \matq_1\matq_2$
      of the explicit shift algorithm coincide.
    \end{enumerate}
  \end{enumerate}
\end{Theorem}

\begin{Algorithm*}{francis-qr-step}{Francis QR Step}
  \begin{algorithmic}[1]
    \Require \( \matH \in \Rnn\) unreduced Hessenberg Form
    \State \( s \gets \operatorname{tr}(\matH_{n-1:n, n-1:n})\)
    \State \( t \gets \det(\matH_{n-1:n, n-1:n})\)
    \State \(\vw \gets \left(\matH^2 - s \matH + t \id\right)_{:,1}\)
    \State \(\matq \gets \mathtt{Householder\_Matrix}(\vw)\)
    \State \( \matH \gets \matq^\mathsf{T} \matH \matq\)
    \For{\(i = 1, \ldots , n-2\)}
      \State \(\matq \gets \mathtt{Householder\_Matrix}(\matH_{i+1:n,i})\)
      \State \( \matH \gets \matq^\mathsf{T} \matH \matq\)
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{remark}
  The matrix $\matm$ in the double shift algorithm is not Hessenberg,
  but it has two nonzero lower diagonals. Thus, we use Householder
  reflection for $\matq_0$ and eliminate $h_{21}$ and $h_{31}$. The
  resulting matrix will have an additional entry $h_{31}$.

  The subsequent Householder reflections $\matq_k$ are used to
  eliminate the additional lower diagonal entry $h_{k+2,k}$ in the
  same fashion as the ``bulge chasing'' for the symmetric implicit
  shift. This method is often named \define{Francis QR step} after its
  inventor.
\end{remark}

\begin{Algorithm*}{deflation-francis}{Deflation for Francis QR iteration}
  In each step of the Francis QR iteration, first set
  \begin{gather}
    h_{i,i-1} = 0, \qquad \text{where}\quad
    \abs{h_{i,i-1}} \le \text{tol} (\abs{h_{i,i}}+\abs{h_{i-1,i-1}}).
  \end{gather}

  Then, partition the matrix
  $\matH$ as
  \begin{gather}
    \matH =
    \begin{pmatrix}
      \matH_{11} & \matH_{12} & \matH_{13} \\
      & \matH_{22} & \matH_{23}\\
      && \matH_{33}
    \end{pmatrix},
  \end{gather}
  where $\matH_{22}\in\R^{q\times q}$ and
  $\matH_{22}\in\R^{p\times p}$ are chosen maximal such that
  $\matH_{33}$ is upper \putindex{quasi-triangular} and $\matH_{22}$
  is unreduced.

  The Francis QR step is then applied to $\matH_{22}$ only. If the
  eigenvectors are not computed, even the transformations of
  $\matH_{12}$ and $\matH_{23}$ can be eliminated.
\end{Algorithm*}

\begin{remark}
  Both the symmetric and the unsymmetric algorithms become more
  efficient, if eigenvectors are not computed by accumulating the
  necessary transformations.

  A way around this is the application of the shifted inverse
  iteration with the approximated eigenvalues, which typically
  converges in one step for well-conditioned problems.
\end{remark}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
