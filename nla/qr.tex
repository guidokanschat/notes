\subsection{Derivation from orthogonal subspace iteration}

\begin{todo}
  rename vectors in OSI
\end{todo}

\begin{intro}
  \label{par:qr:intro}
  In this section, we call the orthogonal vectors of the orthogonal subspace iteration $\matu^{(k)}$ to distinguish them from the new orthogonal basis constructed here.
  
  If the \putindex{orthogonal subspace iteration} converges, then
  there is an unitary matrix $\matu$ such that
  $\lim_{k \to \infty} \matu_k = \matu$.  From the two assignments of
  the algorithm, we get
  \begin{gather}
    \lim_{k \to \infty} \matu_k^* \mata \matu_k = \lim_{k \to \infty} \matu_k^* \maty_k = \lim_{k \to \infty}
    \matu_k^*\matu_{k+1}\matr_{k+1} = \matr,
  \end{gather}
  where $\matr\leftarrow\matr_k$. Hence, the matrices
  \begin{gather}
    \label{eq:qr:intro:1}
    \mata_k = \matu_k^*\mata\matu_k
  \end{gather}
  converge to an upper triangular matrix with the converged
  eigenvalues on the diagonal. Our next goal ist the modification of
  the orthogonal subspace iteration, such that we compute the sequence
  $\mata_k$ directly, without the intermediate $\maty_k$. To this end,
  we require $m=n$, that is, we compute the whole spectrum of $\mata$.
\end{intro}

\begin{Lemma}{qr-iteration-derivation}
  The matrices $\mata_k = \matu_k^*\mata\matu_k$ obtained from the
  orthogonal subspace iteration follow the recursion formula
  \begin{gather}
    \label{eq:qr:iteration-derivation:2}
    \mata_k = \matq_k^*\mata_{k-1}\matq_k,
  \end{gather}
  where $\matq_k = \matu_{k-1}^*\matu_k$ for $k=1,\dots,n-1$. There holds
  \begin{gather}
    \label{eq:qr:iteration-derivation:1}
    \matu_k=\matu_0\matq_1\dots\matq_k.
  \end{gather}
  Furthermore, $\mata_k$ can be
  computed from the QR factorization $\mata_{k-1}= \matq_k\matr_k$ as
  \begin{gather}
    \mata_k = \matr_k\matq_k.
  \end{gather}
\end{Lemma}

\begin{proof}
  By the definition of $\mata_{k-1}$ and
  \slideref{Algorithm}{subspace-iteration}, we have
  \begin{gather*}
    \mata_{k-1} = \matu_{k-1}^*\mata\matu_{k-1}
    = \matu_{k-1}^* \maty_{k-1}
    = \left(\matu_{k-1}^*\matu_{k}\right)\matr_k = \matq_k\matr_k,
  \end{gather*}
  where we see that on the right we have obtained the QR factorization of $\mata_{k-1}$.
  On the other hand,
  \begin{align*}
    \mata_k
    &= \matu_k^*\mata\matu_k\\
    &= \matu_k^*\mata\matu_{k-1}\matu_{k-1}^*\matu_k\\
    &= \matu_k^*\maty_{k-1} \matu_{k-1}^*\matu_k\\
    &= \matr_k \left(\matu_{k-1}^*\matu_k\right) = \matr_k\matq_k.
  \end{align*}
  Thus, we see that $\mata_k$ is obtained by multiplying the QR
  factorization of $\mata_{k-1}$ in reverse order.  We have already
  replaced $\matq_k = \matu_{k-1}^*\matu_k$ above. Mutliplying from
  the left by $\matu_{k-1}$ and using $\matu_0=\matq_0=\id$ yields the
  formula for $\matu_k$ by induction.

  Now, let $\matq_0=\id$ such that
  $\mata_0 = \matq_0^*\mata\matq_0=\mata$. Then, for $k\ge 1$ we
  obtain the recursion
  \begin{gather}
    \mata_{k} = \matr_k\matq_k = \matq_k^* \matq_k \matr_k \matq_k = \matq_k^* \mata_{k-1} \matq_k.
  \end{gather}
\end{proof}

\begin{Algorithm*}{qr-iteration}{QR iteration}
  \begin{algorithmic}[1]
    \Require $\mata_0\in\Cnn$.
    \For{$k=1,\dots$ until convergence}
    \State $\matq_k\matr_k \gets \mata_{k-1}$ \Comment{QR factorization}
    \State $\mata_{k} = \matr_k\matq_k$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{qr-Schur}
  If convergent, the QR iteration converges to the Schur canonical
  form of the matrix $\mata$ with eigenvalues sorted according to
  their modulus.
\end{Lemma}

\begin{proof}
  See~\ref{par:qr:intro}.
\end{proof}

\begin{Lemma}{qr-1}
  The matrices $\mata_k$ of the QR-iteration with $\mata_0 = \mata$
  have the following properties:
  \begin{enumerate}
  \item If $\matq_0=\id$, $\mata_{k} = \matq_k^*\mata_{k-1}\matq_k = \matq_k^*\dots\matq_0^*\mata\matq_0\dots\matq_k$.
  \item $\mata^k=\matq_1\dots\matq_k\matr_k\dots\matr_1$.
  \item If $\mata$ is normal, so is $\mata_k$ for any $k$.
  \item If $\mata$ is complex symmetric, so is $\mata_k$ for any $k$.
  \end{enumerate}
\end{Lemma}

\begin{proof}
  The first relation follows from~\eqref{eq:qr:intro:1}
  and~\eqref{eq:qr:iteration-derivation:1} letting $\matu_0=\id$,
  which corresponds to $\mata_0 = \matu_0^*\mata\matu_0 = \matq_0^*\mata\matq_0 = \mata$.
  
  The second, we prove by induction, where
  $\mata = \mata_0 = \matq_1\matr_1$ follows directly from the first
  step of the algorithm. For the induction step, we abbreviate
  \begin{gather}
    \matu_k = \matq_1\dots\matq_k,\qquad \mats_k = \matr_k\dots\matr_1.
  \end{gather}
  Assuming $\mata^k = \matu_k\mats_k$, we obtain
  \begin{gather}
    \mata^{k+1} = \mata\matu_k\mats_k = \matu_k\mata_{k+1}\mats_k
    = \matu_k\matq_{k+1}\matr_{k+1}\mats_k = \matu_{k+1}\mats_{k+1}.
  \end{gather}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hessenberg matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{intro}
  In each step of the QR-iteration, a QR factorization of the matrix
  is needed, which requires $\bigo(n^3)$ operations. Thus, the
  complexity of the iteration is highly unfavorable. The following
  discussion will provide us with means to reduce the complexity of
  the QR factorization to $\bigo(n^2)$, in the symmetric case even to
  $\bigo(n)$.
\end{intro}

\begin{Definition}{hessenberg}
  A matrix is in \define{Hessenberg form} or is a \define{Hessenberg
    matrix}, if all its entries below the first subdiagonal are zero. Visually,
  \begin{gather}
    H =
    \begin{pNiceMatrix}
      *&\Cdots&\Cdots&\Cdots&*\\
      *&\Ddots&&&\Vdots\\
      &\Ddots&\Ddots&&\Vdots\\
      &&\Ddots&\Ddots&\Vdots\\
      &&&*&*
    \end{pNiceMatrix}
  \end{gather}
  A symmetric or Hermitian Hessenberg matrix is \define{tridiagonal}.
\end{Definition}

\begin{Algorithm*}{Hessenberg-qr-1}{Explicit Hessenberg QR step}
  \index{Hessenberg QR!explicit}
  \begin{algorithmic}[1]
    \Require $\matH\in\Cnn$ in Hessenberg form
    \For{$k=1,\dots,n-1$}
    \Comment{factorization $\matq\matr = \matH$, $\matr$ stored in $\matH$}
    \State $\givens_{k,k+1} \gets$ Givens rotation for $h_{kk},h_{k+1,k}$
    \State $\matH\gets \givens^*_{k,k+1}\matH$
    \EndFor
    \For{$k=1,\dots,n-1$}
    \Comment{$\matH = \matr\matq$}
    \State $\matH\gets \matH\givens_{k,k+1}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{example}
  At the example of a 5-by-5-matrix, we show how this algorithm works.
  \begin{enumerate}
  \item Apply a Givens rotation from the left which eliminates the value $Y$ from the matrix. It affects the two top rows.
    \begin{gather*}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{1-2}{2-5}
          \Body
          \cellcolor{green}x&\Block{1-4}{}*&*&*&*\\
          \cellcolor{green}y&\Block{1-4}{}*&*&*&*\\
          &\Block{1-4}{}*&*&*&*\\
          &&\Block{1-3}{}*&*&*\\
          &&&\Block{1-2}{}*&*
        \end{NiceArray}\right)
      \underrightarrow{\quad\matg_{12}^*\matH\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \Block{1-5}{}*&*&*&*&*\\
          &\Block{1-4}{}*&*&*&*\\
          &\Block{1-4}{}*&*&*&*\\
          &&\Block{1-3}{}*&*&*\\
          &&&\Block{1-2}{}*&*
        \end{NiceArray}\right)    
    \end{gather*}

  \item Do the same with the following row
    \begin{gather*}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{2-3}{3-5}
          \Body
          \Block{1-5}{}*&*&*&*&*\\
          &\cellcolor{green}x&\Block{1-3}{}*&*&*\\
          &\cellcolor{green}y&\Block{1-3}{}*&*&*\\
          &&\Block{1-3}{}*&*&*\\
          &&&\Block{1-2}{}*&*
        \end{NiceArray}\right)
      \underrightarrow{\quad\matg_{23}^*\matH\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \Block{1-5}{}*&*&*&*&*\\
          &\Block{1-4}{}*&*&*&*\\
          &&\Block{1-3}{}*&*&*\\
          &&\Block{1-3}{}*&*&*\\
          &&&\Block{1-2}{}*&*
        \end{NiceArray}\right)    
    \end{gather*}
    to the last pair
    \begin{gather*}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{4-5}{5-5}
          \Body
          \Block{1-5}{}*&*&*&*&*\\
          &\Block{1-4}{}*&*&*&*\\
          &&\Block{1-3}{}*&*&*\\
          &&&\cellcolor{green}x&*\\
          &&&\cellcolor{green}y&*\\
        \end{NiceArray}\right)
      \underrightarrow{\quad\matg_{45}^*\matH\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \Block{1-5}{}*&*&*&*&*\\
          &\Block{1-4}{}*&*&*&*\\
          &&\Block{1-3}{}*&*&*\\
          &&&\Block{1-2}{}*&*\\
          &&&&*
        \end{NiceArray}\right)    
    \end{gather*}
    Note that columns with two zero entries remain unchanged and will not have to be processed.
  \item Now the matrix is upper triangular and the transformation was
    \begin{gather*}
      \matq^* = \givens_{45}^*\givens_{34}^*\givens_{23}^*\givens_{12}^*.
    \end{gather*}

  \item When we go back, we apply Givens rotations from the right, thus affecting columns of the matrices.
    \begin{multline*}\small
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{1-1}{1-1}
          \rectanglecolor{yellow}{1-2}{2-2}
          \Body
          *&\Block{2-1}{}*&\Block{3-1}{}*&\Block{4-1}{}*&\Block{5-1}{}*\\
          &*&*&*&*\\
          &&*&*&*\\
          &&&*&*\\
          &&&&*
        \end{NiceArray}\right)
      \underrightarrow{\quad\matH\matg_{12}\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{1-2}{2-2}
          \rectanglecolor{yellow}{1-3}{3-3}
          \Body
          \Block{2-1}{}*&\Block{2-1}{}*&\Block{3-1}{}*&\Block{4-1}{}*&\Block{5-1}{}*\\
          *&*&*&*&*\\
          &&*&*&*\\
          &&&*&*\\
          &&&&*
        \end{NiceArray}\right)
      \underrightarrow{\quad\matH\matg_{23}\quad}
      \cdots\\\small
      \underrightarrow{\quad\matH\matg_{34}\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{1-4}{4-4}
          \rectanglecolor{yellow}{1-5}{5-5}
          \Body
          \Block{2-1}{}*&\Block{3-1}{}*&\Block{4-1}{}*&\Block{4-1}{}*&\Block{5-1}{}*\\
          *&*&*&*&*\\
          &*&*&*&*\\
          &&*&*&*\\
          &&&&*
        \end{NiceArray}\right)
      \underrightarrow{\quad\matH\matg_{45}\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \Block{2-1}{}*&\Block{3-1}{}*&\Block{4-1}{}*&\Block{5-1}{}*&\Block{5-1}{}*\\
          *&*&*&*&*\\
          &*&*&*&*\\
          &&*&*&*\\
          &&&*&*
        \end{NiceArray}\right)
    \end{multline*}    
  \end{enumerate}
\end{example}

\begin{remark}
  While the previous algorithmdoes the job, it has some
  disadvantages. For once, the Givens rotations accumulated in the
  first part must be stored to be applied in the second. Then, if
  applied to a tridiagonal matrix, the result of the first step has no
  subdiagonal, but two superdiagonals. And finally, if applied to a
  symmetric matrix, all intermediate results are nonsymmetric.

  These drawbacks have led to a second algorithm, where each Givens
  rotation is applied from the left and the right at the same time and
  $\matr$ is never explicitly computed.

  After introducing this algorithm, we will obviously have to make
  sure that its result is useful.
\end{remark}

\begin{Algorithm*}{Hessenberg-qr-2}{Implicit Hessenberg QR step}
  \begin{algorithmic}[1]
    \Require $\matH\in\Cnn$ in Hessenberg form
    \State $\givens_{1,2} \gets$ Givens rotation for $h_{11},h_{21}$
    \State $\matH \gets \givens^*_{1,2}\matH \givens_{1,2}$
    \For{$k=2,\dots,n-1$}
    \State $\givens_{k,k+1}\gets \givens_{k,k+1}[h_{k,k-1},h_{k+1,k-1}]$ Givens rotation
    \State $\matH \gets \givens^*_{k,k+1}\matH \givens_{k,k+1}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{example}
  At the example of a 5-by-5-matrix, we show how this algorithm works.
  \begin{enumerate}
  \item Apply a Givens rotation from the left which eliminates the value $Y$ from the matrix. It affects the two top rows. By
    applying the Givens rotation from the right an additional non zero entry below the subdiagonal is created.
    \begin{gather*}\small
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{1-2}{2-5}
          \Body
          \cellcolor{green}x&\Block{1-4}{}*&*&*&*\\
          \cellcolor{green}y&\Block{1-4}{}*&*&*&*\\
          &\Block{1-4}{}*&*&*&*\\
          &&\Block{1-3}{}*&*&*\\
          &&&\Block{1-2}{}*&*
        \end{NiceArray}\right)
      \to%\underrightarrow{\quad\matg_{12}^*\matH\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{1-1}{3-2}
          \Body
          *&\Block{3-1}{}*&\Block{4-1}{}*&\Block{5-1}{}*&\Block{5-1}{}*\\
          &*&*&*&*\\
          &*&*&*&*\\
          &&*&*&*\\
          &&&*&*
        \end{NiceArray}\right)
      \to%\underrightarrow{\quad\matg_{12}^*\matH\matg_{12}\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \Block{3-1}{}*&\Block{3-1}{}*&\Block{4-1}{}*&\Block{5-1}{}*&\Block{5-1}{}*\\
          *&*&*&*&*\\
          \cellcolor{red}*&*&*&*&*\\
          &&*&*&*\\
          &&&*&*
        \end{NiceArray}\right)    
    \end{gather*}
\item Apply a Givens rotation from the left to restore Hessenberg form of the first column.
    \begin{gather*}\small
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{2-2}{3-5}
          \Body
          \Block{1-5}{}*&*&*&*&*\\
          \cellcolor{green}x&\Block{1-4}{}*&*&*&*\\
          \cellcolor{green}y&\Block{1-4}{}*&*&*&*\\
          &&\Block{1-3}{}*&*&*\\
          &&&\Block{1-2}{}*&*
        \end{NiceArray}\right)
      \to%\underrightarrow{\quad\matg_{12}^*\matH\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{1-2}{4-3}
          \Body
          \Block{2-1}{}*&\Block{3-1}{}*&\Block{4-1}{}*&\Block{5-1}{}*&\Block{5-1}{}*\\
          *&*&*&*&*\\
          &*&*&*&*\\
          &&*&*&*\\
          &&&*&*
        \end{NiceArray}\right)    
      \to%\underrightarrow{\quad\matH\matg_{12}\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \Block{2-1}{}*&\Block{4-1}{}*&\Block{4-1}{}*&\Block{5-1}{}*&\Block{5-1}{}*\\
          *&*&*&*&*\\
          &*&*&*&*\\
          &\cellcolor{red}*&*&*&*\\
          &&&*&*
        \end{NiceArray}\right)    
    \end{gather*}
    
  \item Do the same with the second column.
    
    \begin{gather*}\small
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{3-3}{4-5}
          \Body
          \Block{1-5}{}*&*&*&*&*\\
          \Block{1-5}{}*&*&*&*&*\\
          &\cellcolor{green}x&\Block{1-3}{}*&*&*\\
          &\cellcolor{green}y&\Block{1-3}{}*&*&*\\
          &&&\Block{1-2}{}*&*
        \end{NiceArray}\right)
      \to%\underrightarrow{\quad\matg_{23}^*\matH\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{1-3}{5-4}
          \Body
          \Block{2-1}{}*&\Block{3-1}{}*&\Block{4-1}{}*&\Block{5-1}{}*&\Block{5-1}{}*\\
          *&*&*&*&*\\
          &*&*&*&*\\
          &&*&*&*\\
          &&&*&*
        \end{NiceArray}\right)
      \to%\underrightarrow{\quad\matH\matg_{23}\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \Block{2-1}{}*&\Block{3-1}{}*&\Block{5-1}{}*&\Block{5-1}{}*&\Block{5-1}{}*\\
          *&*&*&*&*\\
          &*&*&*&*\\
          &&*&*&*\\
          &&\cellcolor{red}*&*&*
        \end{NiceArray}\right)    
    \end{gather*}
  
  \item Finally with the last row
    
    \begin{gather*}\small
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{4-4}{5-5}
          \Body
          \Block{1-5}{}*&*&*&*&*\\
          \Block{1-5}{}*&*&*&*&*\\
          &\Block{1-4}{}*&*&*&*\\
          &&\cellcolor{green}x&\Block{1-2}{}*&*\\
          &&\cellcolor{green}y&\Block{1-2}{}*&*
        \end{NiceArray}\right)
      \to%\underrightarrow{\quad\matg_{23}^*\matH\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \CodeBefore
          \rectanglecolor{yellow}{1-4}{5-5}
          \Body
          \Block{2-1}{}*&\Block{3-1}{}*&\Block{4-1}{}*&\Block{5-1}{}*&\Block{5-1}{}*\\
          *&*&*&*&*\\
          &*&*&*&*\\
          &&*&*&*\\
          &&&*&*
        \end{NiceArray}\right)
      \to%\underrightarrow{\quad\matH\matg_{23}\quad}
      \left(\begin{NiceArray}[margin,hvlines,corners=SW]{ccccc}
          \Block{2-1}{}*&\Block{3-1}{}*&\Block{4-1}{}*&\Block{5-1}{}*&\Block{5-1}{}*\\
          *&*&*&*&*\\
          &*&*&*&*\\
          &&*&*&*\\
          &&&*&*
        \end{NiceArray}\right)    
    \end{gather*}

    Note that columns with two zero entries remain unchanged and will not have to be processed.
  \end{enumerate}
\end{example}

\begin{Remark}{bulge-chasing}
  This algorithm is called \define{bulge chasing} with the following
  image in mind. After the application of the first rotation, there is
  a bulge protruding down from the Hessenberg form in the furst
  column. This bulge is then ``chased'' down row by row until it
  leaves the matrix at the bottom.
    \begin{gather}\arraycolsep0pt
    \begin{pNiceMatrix}
      *&\Cdots&\Cdots&\Cdots&\Cdots&*\\[-5pt]
      *&\Ddots&&&&\Vdots\\[-5pt]
      &\Ddots&\Ddots&&&\Vdots\\[-5pt]
      &*&\Ddots&\Ddots&&\Vdots\\[-5pt]
      &&&\Ddots&\Ddots&\Vdots\\[-5pt]
      &&&&*&*
    \end{pNiceMatrix}
    \to
    \begin{pNiceMatrix}
      *&\Cdots&\Cdots&\Cdots&\Cdots&*\\[-5pt]
      *&\Ddots&&&&\Vdots\\[-5pt]
      &\Ddots&\Ddots&&&\Vdots\\[-5pt]
      &&\Ddots&\Ddots&&\Vdots\\[-5pt]
      &&*&\Ddots&\Ddots&\Vdots\\[-5pt]
      &&&&*&*
    \end{pNiceMatrix}
  \end{gather}
\end{Remark}

\begin{Definition}{hessenberg-unreduced}
  A Hessenberg matrix is called \define{unreduced} if all entries on
  the first subdiagonal are nonzero. It is called \define{reduced}
  otherwise.
\end{Definition}

\begin{Theorem*}{implicit-Q}{Implicit Q Theorem}
  Let $\mata\in\Cnn$ arbitrary, let $\matq,\matv\in\Cnn$ unitary such that
  \begin{gather}
    \matq^*\mata\matq = \matH,\qquad \matv^*\mata\matv = \matg,
  \end{gather}
  where $\matH$ and $\matg$ are Hessenberg matrices.
  Let $k$ denote
  the smallest integer such that $h_{k+1,k} = 0$, or $k=n$ if $\matH$
  is unreduced. Assume $\vv_1 \parallel \vq_1$. Then,
  $\vv_j\parallel \vq_j$ and $\abs{h_{j+1,j}} = \abs{g_{j+1,j}}$
  for $j=1,\dots,k-1$. If $k<n$, then $g_{k+1,k} = 0$.
\end{Theorem*}

\begin{proof}
  We define the matrix $\matw = \matv^*\matq$, which is unitary as the product of unitary matrices. There holds
  \begin{gather}
    \matg\matw = \matv^*\mata\matv\matv^*\matq = \matv^*\mata\matq
    = \matv^*\matq\matq^*\mata\matq = \matw\matH.
  \end{gather}
  Spelling out column $j$ of this product, we obtain
  \begin{gather}
    \label{eq:qr:implicitq-1}
    \matg\vw_j = \sum_{k=1}^{j+1} h_{kj} \vw_{k}.
  \end{gather}
  We use this equality to show by induction over the columns that the
  entries $w_{ij}$ of $\matw$ are zero for $i>j$. In other words,
  $\matw$ is upper triangular.  For $j=1$, we obtain from the
  unitarity of $\matq$ and $\matv$ and the parallelity of $\vq_1$
  and $\vv_1$ that $\vw_1 = e^{i\phi} \ve_1$ for some argument $\phi$.

  Now let the statement be proven for all columns of $\matw$ up to
  column $j$. Then, from~\eqref{eq:qr:implicitq-1} we obtain
  \begin{gather}
    h_{j+1,j}\vw_{j+1} = \matg\vw_j - \sum_{\nu=1}^j h_{\nu j} \vw_{\nu}.
  \end{gather}
  For each vector in the sum, there holds $(\vw_{\nu})_i = 0$ for
  $i>j$. Since $\matg$ is Hessenberg and applied to $\matw_j$, the
  last possibly nonzero entry of the product is in position $j+1$,
  what we wanted to show.

  Since every unitary matrix is normal and due to
  \slideref{Problem}{normal-triangular-diagonal} every triangular
  normal matrix is diagonal, the matrix $(\vw_1,\dots,\vw_k)$ is
  diagonal with diaognal entries of the form $w_{jj} = e^{i\phi_j}$
  with some arguments $\phi_j$. Hence, $\vv_j = e^{-i\phi_j} \vq_j$
  for $j=1,\dots,k$ and thus $\vv_j$ is parallelto $\vq_j$.

  There holds for $j<k$
  \begin{gather}
    \abs{h_{j+1,j}} = \abs{\ve_{j+1}^*\matH\ve_j} = \abs{\vq_{j+1}^*\mata\vq_j}
    = \abs{\vv_{j+1}^*\mata\vv_j} = \abs{g_{j+1,j}}.
  \end{gather}

  If $k<n$, that is, $h_{k+1,k}=0$, it remains to show that
  \begin{multline}
    g_{k+1,k} = \ve_{k+1}^*\matg\ve_k = e^{i\phi_k}\ve_{k+1}^*\matg\matw\ve_k
    =  e^{i\phi_k}\ve_{k+1}^*\matw\matH\ve_k\\
    =  e^{i\phi_k}\ve_{k+1}^* \sum_{j=1}^k h_{jk} \matw\ve_j
    =  \sum_{j=1}^k e^{i\phi_k} h_{jk} \ve_{k+1}^* \ve_j = 0.
  \end{multline}
\end{proof}

\begin{Definition}{essentially-equal}
  The \putindex{Implicit Q Theorem} says that two Hessenberg forms of $\mata$ with the same initial reduction vector are \define{essentially equal} in the sense that they only differ by the diagonal scaling $\matg = \matd^{-1}\matH\matd$ where $\matd=\diag(d_1,\dots,d_n)$ matrix with $\abs{d_{i}} = 1$.
\end{Definition}

\begin{Corollary}{Hessenberg-qr-equivalence}
  The two versions of the Hessenberg QR step are essentially equal.
\end{Corollary}

\begin{Problem}{Hessenberg-qr-effort}
  \begin{enumerate}
  \item How many operations do the two versions of the Hessenberg QR step require?
  \item Show that if $\matH$ is Hermitian, the result of the
    Hessenberg QR step is Hermitian as well.
  \end{enumerate}
\end{Problem}

\begin{Corollary}{Hessenberg-qr}
  The complexity of each step of the implicit QR-iteration for Hessenberg matrices is $\bigo(n^2)$. For tridiagonal (complex) symmetric matrices, it is $\bigo(n)$.
\end{Corollary}

\begin{Theorem}{Hessenberg-householder}
  Every matrix $\mata\in\Cnn$ is unitarily similar to a Hessenberg matrix $\matH$, that is,
  \begin{gather}
    \matH = \matq^* \mata \matq.
  \end{gather}
  The matrix $\matq$ can be obtained by $n-2$ \putindex{Householder
    reflection}s.
\end{Theorem}

\begin{proof}
  The proof is constructive and relies on Householder
  transformations. We begin by partitioning the matrix $\mata$ as
  \begin{gather}
    \mata =
    \left(\begin{NiceArray}[margin,hvlines]{ccw{c}{4em}c}
        \Block{1-4}{}*&\Cdots&\Cdots&*\\
        \Block{4-1}{\vv_1}&\Block{4-3}<\huge>{*}&&\\
        &&&\\
        &&&\\
        &&&
      \end{NiceArray}\right)
  \end{gather}
  Now we find the Householder vector $\tilde\vw_1\in\C^{n-1}$ which transforms $\vv_1$ to a multiple of $\ve_1\in \C^{n-1}$ and let
  \begin{gather}
    \vw_1 =
    \begin{pmatrix}
      0\\ \tilde\vw_1
    \end{pmatrix},
    \qquad
    \matq_1 = \id - 2 \frac{\vw_1\vw_1^*}{\vw_1^*\vw_1}.
  \end{gather}
  Note that the multiplication $\matq_1\mata$ leaves the first row of
  $\mata$ unchanged, while $\mata\matq_1$ leaves the first column
  unchanged. Hence,
  \begin{gather}
    \matq_1 \mata =
    \left(\begin{NiceArray}[margin,hvlines]{ccw{c}{4em}c}
        \Block{1-4}{}*&\Cdots&\Cdots&*\\
        *&\Block{4-3}<\huge>{*}&&\\
        \Block{3-1}{0}&&&\\
        &&&\\
        &&&
      \end{NiceArray}\right), 
  \end{gather}
  and this structure does not change by multiplication with $\matq_1$
  from the right. Now partition
  \begin{gather}
    \matq_1\mata\matq_1 =
    \left(\begin{NiceArray}[margin,hvlines]{cccw{c}{4em}c}
        \Block{1-5}{}*&\Cdots&\Cdots&\Cdots&*\\
        *&\Block{1-4}{}*&\Cdots&\Cdots&*\\
        \Block{4-1}{0}&\Block{4-1}{\vv_2}&\Block{4-3}<\huge>{*}&&\\
        &&&&\\
        &&&&\\
        &&&&
      \end{NiceArray}\right),     
  \end{gather}
  and choose the Householder vector $\tilde\vw_2\in\C^{n-2}$ which
  maps $\vv_2$ to a multiple of $\ve_1\in\C^{n-2}$. Let
  \begin{gather}
    \vw_2 =
    \begin{pmatrix}
      0\\0\\ \tilde\vw_2
    \end{pmatrix},
    \qquad
    \matq_2 = \id - 2 \frac{\vw_2\vw_2^*}{\vw_2^*\vw_2},
  \end{gather}
  and observe that multiplication with $\matq_2$ from left and right leaves the first two rows and columns untouched, respectively. Hence,
  \begin{gather}
    \matq_2\matq_1\mata\matq_1\matq_2 =
    \left(\begin{NiceArray}[margin,hvlines]{cccw{c}{4em}c}
        \Block{1-5}{}*&\Cdots&\Cdots&\Cdots&*\\
        *&\Block{1-4}{}*&\Cdots&\Cdots&*\\
        \Block{4-1}{0}&*&\Block{4-3}<\huge>{*}&&\\
        &\Block{3-1}{0}&&&\\
        &&&&\\
        &&&&
      \end{NiceArray}\right).
  \end{gather}
  This algorithm can be continued until the third last entry in the
  last row is set to zero. Note that the operations from the left
  mimic the QR factorization, but start to operate one row below the
  diagonal.
\end{proof}

\begin{Problem}{Hermitian-tridiagonal}
  Show that every (complex) Hermitian matrix is unitarily similar
  to a symmetric tridiagonal matrix with real entries.
\end{Problem}

\begin{Algorithm*}{qr-method}{The Hessenberg QR-Method}
  Compute the spectrum of a matrix $\mata\in\Cnn$ by
  \begin{enumerate}
  \item Use $n-2$ Householder transformations to transform $\mata$ to
    Hessenberg form
    \begin{gather}
     \matH_0 = \matq^*\mata\matq.
   \end{gather}
 \item QR-iteration: perform the implicit Hessenberg QR step until convergence
 \item Store Householder vectors as well as $r$ and $c$ for each
   Givens rotation \textbf{only} if the eigenvectors are desired in the end.
  \end{enumerate}
\end{Algorithm*}

\begin{remark}
  In this algorithm, we focus on computing eigenvalues. The
  eigenvectors are neglected. The could be computed by storing the
  $n-2$ Householder vectors of the transformation to Hessenberg form
  and all parameters of the Givens rotations of the iteration. In the
  end, the vectors can be computed by applying all these unitary
  matrices in the right order to an identity matrix.

  Usually, this is not done, since it results in an inefficient
  algorithm. Furthermore, we will modify the algorithm to deal with
  reduced Hessenberg matrices, which may appear anytime in the
  iteration. Actually, we should point out here that the implicit QR
  step should not be continued in this case since the Implicit Q
  Theorem dos not apply anymore.
\end{remark}

\begin{Lemma}{Hessenberg-rank}
  The rank of an unreduced Hessenberg matrix of dimension $n$ is at
  least $n-1$. In particular, the geometric multiplicity of any
  eigenvalue of such a matrix is one.
\end{Lemma}

\begin{proof}
  If $\mata$ is in Hessenberg form and unreduced, the $k+1$-st entry
  of the $k$-th column vector $\va_k$ for $k\le n-1$ is nonzero, while
  the same entry of all previous column vectors is zero. Hence, the
  first $n-1$ column vectors are linearly independent and the rank of
  $\mata$ is at least $n-1$. Note that this argument does not apply to
  the last column.

  If $\mata$ is unreduced Hessenberg, so is $\mata-\lambda\id$, which
  proves the second statement.
\end{proof}

\subsection{Deflation and shifts}

\begin{intro}
  The goal of this section is the development and justification of a
  method which accelerates convergence of the QR-iteration and
  reducing the effort at the same time. It is based on shifts, like
  for the simple or inverse power method. But, shifts are much more
  powerful here, since we compute not only ``converging subspace'',
  but also its complement. The presentation follows
  mostly~\cite{GolubVanLoan83}.
\end{intro}

\begin{Theorem}{qr-reduction}
  Let the matrix $\matH^{(k)}\in\Cnn$ in the QR iteration be of the
  form
  \begin{gather}
    \matH^{(k)} =
    \begin{pmatrix}
      \matH_{11} & \mata_{12}\\0 & \matH_{22}
    \end{pmatrix}
  \end{gather}
  with Hessenberg matrices $\matH_{11}\in\C^{p\times p}$,
  $\matH_{22}\in \C^{n-p\times n-p}$ and an arbitrary matrix
  $\mata_{12}\in \C^{p\times n-p}$. Then, the matrix $\matq^{(k)}$
  decouples into two diagonal blocks and $\matH^{(k+1)}$ has the same
  form. Thus, the iteration decouples into two separate iterations. If
  $p=n-1$, then $h_{nn}$ approximates an eigenvalue.
\end{Theorem}

\begin{proof}
  We consider the algorithm in its explicit form.  The Givens
  transformation $\givens_{p,p+1}$ can be chosen as identity, since
  the subdiagonal entry in column $p$ is already zero. Hence, the
  product of all givens transformations is
  \begin{gather}
    \matq = \givens_{12}\dots\givens_{p-1,p}\givens_{p+1,p+2}\dots\givens_{n-1,n},
  \end{gather}
  where the first $p-1$ only act on the first $p$ rows/columns and the
  remaining ones on th last $n-p$. Therefore, $\matr\matq$ has the
  same block structure as $\matH^{(k)}$.
\end{proof}


\begin{Algorithm*}{qr-deflation}{Deflation}
  After each step of the shifted QR-iteration monitor the subdiagonal
  elements of $\matH^{(k)}$. Whenever
  \begin{gather}
    \abs{h_{j,j-1}} \le \eps \bigl(\abs{h_{j-1,j-1}}+\abs{h_{jj}}\bigr)
  \end{gather}
  set $h_{j,j-1}=0$.

  If this happens in the last row, consider $h_{nn}=\lambda_n$
  converged and proceed with a matrix of dimension $n-1\times n-1$.

  If this happens in the center of the matrix, proceed with both
  remaining diagonal blocks separately.

\end{Algorithm*}

\begin{Remark}{qr-deflation}
  Deflation changes the matrix in a ``nonorthogonal'' way and thus
  changes the eigenvalues. Their accuracy will be determined by the
  parameter $\eps$ in the end.
\end{Remark}

\begin{remark}
  The purpose of deflation is removing Schur vectors from the
  iteration. Thus, if one of the Schur vectors for a multiple
  eigenvalue has converged, the remaining iterations will deal with
  reduced multiplicity.

  Deflation by itself will not help us to deal with the
  requirement that all eigenvalues must have different modulus, but
  this is solved below in combination with shifts.
\end{remark}

\begin{Algorithm*}{shifted-qr-iteration}{QR iteration with shift}
  \begin{algorithmic}[1]
    \Require $\matH_0 \in\Cnn$, Hessenberg, unreduced
    \For {$k=1,\ldots$ until convergence}
    \State $\matq_k\matr_k = \matH_{k-1} - \sigma_k\id$\Comment{QR factorization}
    \State $\matH_{k} = \matr_k\matq_k + \sigma_k\id$
    \EndFor
  \end{algorithmic}
  There is an implicit form of the shifted QR step which follows
  exactly the version outlined for the unshifted case.
\end{Algorithm*}

\begin{Lemma}{shifted-qr-similarity}
  The matrices $\matH_k$ generated by the QR iteration with shifts
  admit the recurrence relation
  \begin{gather}
    \matH_k = \matq_k^*\matH_{k-1}\matq_k.
  \end{gather}
\end{Lemma}  

\begin{proof}
  The proof is almost identical to \slideref{Lemma}{qr-1}. There holds
  \begin{multline}
    \matH_k = \matr_k\matq_k + \sigma_k \id
    = \matq_k^*\matq_k\matr_k\matq_k + \sigma_k \id\\
    =\matq_k^*\left(\matH_{k-1}-\sigma_k\id\right)\matq_k + \sigma_k \id
    =\matq_k^*\matH_{k-1}\matq_k.
  \end{multline}
\end{proof}

\begin{Lemma*}{perfect-shift}{Perfect shift}
  Let $\matH\in\Cnn$ be an unreduced Hessenberg matrix with eigenvalue
  $\sigma$. Let $\matq\matr = \matH - \sigma\id$ be a QR factorization
  and $\widetilde\matH = \matr\matq+\sigma\id$. Then,
  $\tilde h_{n,n-1}=0$ and $\tilde h_{nn} =\sigma$.
\end{Lemma*}

\begin{proof}
  See also~\cite[Theorem 7.5.1]{GolubVanLoan83}.  Since $\matH$ is
  unreduced, its first $n-1$ columns are linearly independent. Hence,
  if $\matq\matr=\matH-\sigma\id$ is a QR factorization, then
  $r_{ii} \neq 0$ for $i=1,\dots,n-1$.

  Since $\matH-\sigma\id$ is singular, we conclude $r_{nn}=0$. Thus,
  the last row of $\matr\matq$ is zero and the statement holds.
\end{proof}

\begin{intro}
  Obviously, if we knew an eigenvalue, we could deflate right
  away. Thus, the next step in the development of the algorithm is the
  determination of a \define{shift strategy} which drives $h_{n,n-1}$
  to zero by approximating the last eigenvalue.

  Such a shift strategy selects a new shift parameter $\sigma_k$ in
  every step of the algorithm. The shift strategies differ in the
  approximation of the eigenvalue which is closest to $h_{nn}$.
\end{intro}

\begin{Example*}{rayleigh-shift}{Rayleigh shift}
  The Rayleigh quotient for the smallest eigenvalue by magnitude
  converges to $h_{nn}$, as
  \begin{gather}
    \ve_n^* H^{(k)} \ve_n = h_{nn}^{(k)}
  \end{gather}
  and $\vq_n$ is orthogonal to all eigenvectors for eigenvalues of
  greater magnitude. Therefore, using $\sigma_k = h_{nn}^{(k)}$ seems
  a good idea, and often is. But it is not reliable, as in the example
  \begin{gather}
    H =
    \begin{pmatrix}
      0 & 1 \\ 1 & 0
    \end{pmatrix}.
  \end{gather}
\end{Example*}

\begin{Definition*}{wilkinson-shift}{Wilkinson shift}
  Let
  \begin{gather}
    \matm =
    \begin{pmatrix}
      h_{n-1,n-1}^{(k)}&h_{n-1,n}^{(k)}\\h_{n,n-1}^{(k)}&h_{nn}^{(k)}
    \end{pmatrix}.
  \end{gather}
  Then, for $\sigma_k$ use the eigenvalue of $\matm$ which is closer
  to $h_{nn}^{(k)}$.
\end{Definition*}

\begin{Example}{wilkinson-failure}
  Consider the orthogonal matrix
  \begin{gather}
    \mata =
    \begin{pmatrix}
      0&0&1\\1&0&0\\0&1&0
    \end{pmatrix}.
  \end{gather}
  The lower right block has a single eigenvalue zero, such that the
  Wilkinson shift and the Rayleigh shift for this matrix are zero. The eigenvalues of $\mata$ are
  \begin{gather}
    \sigma(\mata) = \left\{1, -\tfrac12 \pm \sqrt{\tfrac34}i\right\},
  \end{gather}
  which all have the same modulus. Thus, the algorithm will not converge with either shift.
\end{Example}

\begin{remark}
  The structure of this example for the Wilkinson shift suggests, that
  for every simple shift strategy relying on submatrices, we will find
  a matrix which defeats it. This could be either because the
  iteration stagnates or since it runs in a loop. Hence, we have to
  break this situatuin, which can be achieved by applying a random
  shift.

  A notable exception from this rule are symmetric tridiagonal
  matrices, where we actually have a proof of convergence, see
  \slideref{Theorem}{wilkinson-convergence}.
\end{remark}

\begin{Algorithm}{exceptional-shift}
  If no deflation has ocurred for a given number of iteration steps of
  the shifted QR iteration, the chosen shift strategy has failed.

  In this case, perform a single step with a random shift parameter, a
  so-called \define{exceptional shift}.
\end{Algorithm}

\begin{remark}
  From the necessity to introduce exceptional shifts, we realize that
  a convergence result for the shifted QR iteration is hard to
  obtain. Nevertheless, from the result for the orthogonal subspace
  iteration, we have
  \begin{gather}
    h_{j+1,j}^{(k)} = \bigo \left(\abs*{\frac{\lambda_{j+1}-\sigma}{\lambda_j-\sigma}}^k\right).
  \end{gather}
  Hence, the situation is not hopeless and typically, the algorithm
  converges again after an exceptional shift.
\end{remark}

\begin{Algorithm*}{qr-step-deflation}{QR step with deflation}
  In each step of the QR iteration, first set
  \begin{gather}
    h_{i,i-1} = 0, \qquad \text{where}\quad
    \abs{h_{i,i-1}} \le \eps \left(\abs{h_{i,i}}+\abs{h_{i-1,i-1}}_{\vphantom{g}}\right).
  \end{gather}

  Then, partition the matrix
  $\matH$ as
  \begin{gather}
    \matH =
    \begin{pmatrix}
      \matH_{11} & \matH_{12} & \matH_{13} \\
      & \matH_{22} & \matH_{23}\\
      && \matH_{33}
    \end{pmatrix},
  \end{gather}
  where $\matH_{22}\in\R^{q\times q}$ and
  $\matH_{33}\in\R^{p\times p}$ are chosen maximal such that
  $\matH_{33}$ is upper \putindex{triangular} and $\matH_{22}$
  is unreduced.

  The shifted QR step is then applied to $\matH_{22}$ only.
\end{Algorithm*}

\begin{remark}
  The three diagonal blocks in the preceding algorithm represent different stages in the algorithm:
  \begin{itemize}
  \item $\matH_{33}$ is converged to an upper triangular matrix and
    hence corresponds to a converged Schur form with eigenvalues on
    the diagonal.
  \item $\matH_{22}$ is an unreduced Hessenberg matrix. The shifted QR
    step operates on this block with the goal of driving at least one
    subdiagonal element to zero.
  \item $\matH_{11}$ is the part of the matrix waiting to be handled
    by the algorithm later.
  \end{itemize}
  If a subdiagonal entry of $\matH_{22}$ is in the last row, this last
  row is transferred to the block $\matH_{33}$ in the next step, hence
  reducing the dimension of $\matH_{22}$ by one.

  If such a subdiagonal entry is in an earlier row, we split the
  matrix $\matH_{22}$ at this point. The lower diagonal block becomes
  the new $\matH_{22}$, while the upper one is becoming a part of
  $\matH_{11}$.

  We might save additional iterations by allowing two-by-two blocks on
  the diagonal of $\matH_{33}$. Their eigenvalues can still be
  computed easily by computing the roots of a quadratic polynomial.
  
  We do not compute eigenvectors with this algorithm, such that the
  transformations of $\matH_{13}$ and $\matH_{23}$ can be avoided.
\end{remark}

\begin{remark}
  When implementing \slideref{Algorithm}{qr-step-deflation}, we have
  to be able to run a QR step on a submatrix. Allocating new memory
  and copying the submatrix should be avoided since it comes at
  considerable cost.

  This means on the other hand, that the matrix $\matH_{22}$ will not
  be stored as a consecutive array of $q\times q$ numbers. Depending
  on whether the entries are sorted in row-major or column-major
  order, there will either be a gap between each consecutive element
  of a column or between the last element of one column and the first
  element of the next.

  Thus, the QR step operations must allow for a \define{stride}
  between rows or columns. This can be achieved either by storing the
  matrix as a sequence of column vectors, or by using strided versions
  of the algorithms.

  In FORTRAN, the function DAXPY of the basic linear algebra
  subprograms (BLAS) library has explicit parameters for this
  stride. The submatrix objects of the Armadillo library implement
  this in a transparent way.
\end{remark}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
