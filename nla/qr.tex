\subsection{Derivation from orthogonal subspace iteration}
\begin{todo}
  GM:
  Q converges because the QR decomposition is unique for matrices with l.i. columns correct? \\
  This does not work with the matrix polynomial.
\end{todo}
\begin{remark}
  If the iteration converges, then there is an orthogonal matrix $\matq$ such that $\lim_{k \to
  \infty} \matq_k = \matq$.
  From the two assignments of the algorithm, we get
  \begin{gather}
    \lim_{k \to \infty} \matq_k^* \mata \matq_k = \lim_{k \to \infty} \matq_k^* \maty_k = \lim_{k \to \infty}
    \matq_k^*\matq_{k+1}\matr_{k+1} = \matr,
  \end{gather}
  where $\matr\leftarrow\matr_k$, that is, the matrices
  $\mata_k = \matq_k^*\mata\matq_k$ converge to an upper triangular
  matrix with the converged eigenvalues on the diagonal. Our next goal
  ist the modification of the orthogonal subspace iteration, such that
  we compute the sequence $\mata_k$ directly, without the intermediate
  $\maty_k$. To this end, we assume $m=n$, that is, we compute the
  whole spectrum of $\mata$. By lines 2 and 3 of the algorithm, we have
  \begin{gather*}
    \mata_{k-1} = \matq_{k-1}^*\mata\matq_{k-1} = \matq_{k-1}^* \maty_{k-1} = \left(\matq_{k-1}^*\matq_{k}\right)\matr_k,
  \end{gather*}
  where we see that on the right we have obtained the QR factorization of $\mata_{k-1}$.
  On the other hand,
  \begin{align*}
    \mata_k
    &= \matq_k^*\mata\matq_k\\
    &= \matq_k^*\mata\matq_{k-1}\matq_{k-1}^*\matq_k\\
    &= \matq_k^*\maty_{k-1} \matq_{k-1}^*\matq_k\\
    &= \matr_k \left(\matq_{k-1}^*\matq_k\right).
  \end{align*}
  Thus, we see that $\mata_k$ is obtained by multiplying the QR
  factorization of $\mata_{k-1}$ in reverse order. Note that $\matq_k$
  in the following algorithm differs from $\matq_k$ in the previous!
\end{remark}

\begin{Algorithm*}{qr-iteration}{QR iteration}
  \begin{algorithmic}[1]
    \Require $\mata_0\in\Cnn$.
    \For{$k=1,\dots$ until convergence}
    \State $\matq_k\matr_k \gets \mata_{k-1}$ \Comment{QR factorization}
    \State $\mata_{k} = \matr_k\matq_k$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}
\begin{todo}
  GM:
\begin{Algorithm*}{qr-iteration}{QR iteration}
  \begin{algorithmic}[1]
    \Require $\mata_0\in\Cnn$.
    \For{$k=1,\dots$ until convergence}
    \State $\matq_k\matr_k \gets \mata_{k-1}$ \Comment{QR factorization}
    \State $\mata_{k} = \matq_k \mata_{k-1} \matq_k$ \Comment{$\mata_{k} = \matr_k\matq_k$}
    \EndFor
  \end{algorithmic}
\end{Algorithm*}
\end{todo}

%\subsection{Analysis}
\begin{Lemma}{qr-1}
  The matrices $\mata_k$ of the QR-iteration have the following properties:
  \begin{enumerate}
  \item $\mata_{k} = \matq_k^*\mata_{k-1}\matq_k = \matq_k^*\dots\matq_0^*A\matq_0\dots\matq_k$.
  \item If $\matq_0=\id$, then $\mata^k=\matq_1\dots\matq_k\matr_k\dots\matr_1$.
  \item If $\mata$ is normal, so is $\mata_k$ for any $k$.
  \item If $\mata$ is complex symmetric, so is $\mata_k$ for any $k$.
  \end{enumerate}
\end{Lemma}
\begin{todo}
  GM: I think the index in 2. is wrong, and I am unsure, if it is correct. Also I don't see why it is needed.
\end{todo}

\begin{proof}
  For the first relation, we compute
\begin{todo}
  GM:
  \begin{gather}
    \mata_{k} = \matr_k\matq_k = \matq_k^* \matq_k \matr_k \matq_k = \matq_k^* \mata_{k-1} \matq_k.
  \end{gather}
\end{todo}
  \begin{gather}
    \matq_k\mata_{k}\matq_k^* = \matq_k\matr_k\matq_k\matq_k^* = \mata_{k-1}.
  \end{gather}
  The second, we prove by induction, where
  $\mata = \mata_0 = \matq_1\matr_1$ follows directly from the first
  step of the algorithm. For the induction step, we abbreviate
  \begin{gather}
    \matp_k = \matq_1\dots\matq_k,\qquad \matu_k = \matr_k\dots\matr_1.
  \end{gather}
  Assuming $\mata^k = \matp_k\matu_k$, we obtain
  \begin{gather}
    \mata^{k+1} = \mata\matp_k\matu_k = \matp_k\mata_{k+1}\matu_k
    = \matp_k\matq_{k+1}\matr_{k+1}\matu_k = \matp_{k+1}\matu_{k+1}.
  \end{gather}
\end{proof}
\subsection{Implementation issues}
\begin{intro}
  In each step of the QR-iteration, a QR factorization of the matrix
  is needed, which requires $\bigo(n^3)$ operations. Thus, the
  complexity of the iteration is highly unfavorable. The following
  discussion will provide us with means to reduce the complexity of
  the QR factorization to $\bigo(n^2)$, in the symmetric case even to
  $\bigo(n)$.
\end{intro}

\begin{Definition}{hessenberg}
  A matrix is in \define{Hessenberg form} or is a \define{Hessenberg
    matrix}, if all its entries below the first subdiagonal are zero. Visually,
  \begin{gather}
    H =
    \begin{pmatrix}
      *&*&*&*&*&*\\
      *&*&*&*&*&*\\
      0&*&*&*&*&*\\
      0&0&*&*&*&*\\
      0&0&0&*&*&*\\
      0&0&0&0&*&*
    \end{pmatrix}
  \end{gather}
  A symmetric or Hermitian Hessenberg matrix is \define{tridiagonal}.
\end{Definition}

\begin{todo}
\begin{Algorithm*}{Hessenberg-qr}{Unoptimized Hessenberg QR step}
  \begin{algorithmic}[1]
    \Require $\matH\in\Cnn$ in Hessenberg form
    \For{$k=1,\dots,n-1$}
    \Comment{factorization $\matq\matr = \matH$, $\matr$ stored in $\matH$}
    \State $\givens_{k,k+1} \gets$ Givens rotation for $h_{kk},h_{k+1,k}$
    \State $\matH\gets \givens^*_{k,k+1}\matH$
    \EndFor
    \For{$k=1,\dots,n-1$}
    \Comment{$\matH = \matr\matq$}
    \State $\matH\gets \matH\givens_{k,k+1}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Definition}{hessenberg-unreduced}
  A Hessenberg matrix is called \define{unreduced} if all entries on
  the first subdiagonal are nonzero. It is called \define{reduced}
  otherwise.
\end{Definition}

\begin{Theorem*}{implicit-Q}{Implicit Q Theorem}
  Let $\mata\in\Cnn$ arbitrary, let $\matq,\matv\in\Cnn$ unitary such that
  \begin{gather}
    \matq^*\mata\matq = \matH,\qquad \matv^*\mata\matv = \matg,
  \end{gather}
  where $\matH$ and $\matg$ are Hessenberg matrices. Let $k$ denote
  the smallest integer such that $h_{k+1,k} = 0$, or $k=n$ if $\matH$
  is unreduced. Assume $\vv_1 = \vq_1$. Then,
  $\vv_j = e^{i_j\phi}\vq_j$ and $\abs{h_{j,j-1}} = \abs{g_{j,j-1}}$
  for $j=1,\dots,k$. if $k<n$, then $g_{k+1,k} = 0$.
\end{Theorem*}

\begin{proof}
  See \cite[Theorem 7.4-2]{GolubVanLoan83}.
\end{proof}

\begin{Definition}{essentially-equal}
  The \putindex{Implicit Q Theorem} says that two Hessenberg forms of $\mata$ with the same initial reduction vector are \define{essentially equal} in the sense that they only differ by the diagonal scaling $\matg = \matd^{-1}\matH\matd$ where $\matd=\diag(d_1,\dots,d_n)$ matrix with $\abs{d_{i}} = 1$.
\end{Definition}

\begin{Algorithm*}{Hessenberg-qr}{Hessenberg QR step}
  \begin{algorithmic}[1]
    \Require $\matH\in\Cnn$ in Hessenberg form
    \State $\givens_{0,1} \gets$ Givens rotation for $h_{0,0},h_{1,0}$
    \State $\matH \gets \givens^*_{k,k+1}\matH \givens_{k,k+1}$
    \For{$k=1,\dots,n-1$}
    \State $\givens_{k,k+1} \gets$ Givens rotation for $h_{k,k-1},h_{k+1,k-1}$
    \State $\matH \gets \givens^*_{k,k+1}\matH \givens_{k,k+1}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}
Maybe as an exercise prove, that the two Algorithms result in essentially-equal matrices?
\begin{example}
  At the example of a 4-by-4-matrix, we show how this algorithm works.
  \begin{enumerate}
  \item Apply a Givens rotation from the left which eliminates the value $Y$ from the matrix. It affects the two top rows. By
    applying the Givens rotation from the left an additional non zero entry below the subdiagonal is created.

    {\small\begin{tikzpicture}
      \input{fig/hessenberg-givens-1};
    \end{tikzpicture}}
  \item Apply a Givens rotation from the left to restore Hessenberg form of the first column.


    {\small\begin{tikzpicture}
        \input{fig/hessenberg-givens-2};
      \end{tikzpicture}}
  \item Do the same with the second column.

    {\small\begin{tikzpicture}
        \input{fig/hessenberg-givens-3};
      \end{tikzpicture}}

    Note that columns with two zero entries remain unchanged and will not have to be processed.
  \item The transformation matrix was
    \begin{gather*}
      \matq = \givens_{12}\givens_{23}\givens_{34}.
    \end{gather*}

  \end{enumerate}
\end{example}
\end{todo}

\begin{Algorithm*}{Hessenberg-qr}{Hessenberg QR step}
  \begin{algorithmic}[1]
    \Require $\matH\in\Cnn$ in Hessenberg form
    \For{$k=1,\dots,n-1$}
    \Comment{factorization $\matq\matr = \matH$, $\matr$ stored in $\matH$}
    \State $\givens_{k,k+1} \gets$ Givens rotation for $h_{kk},h_{k+1,k}$
    \State $\matH\gets \givens^*_{k,k+1}\matH$
    \EndFor
    \For{$k=1,\dots,n-1$}
    \Comment{$\matH = \matr\matq$}
    \State $\matH\gets \matH\givens_{k,k+1}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{example}
  At the example of a 4-by-4-matrix, we show how this algorithm works.
  \begin{enumerate}
  \item Apply a Givens rotation from the left which eliminates the value $Y$ from the matrix. It affects the two top rows.

    {\small\begin{tikzpicture}
      \input{fig/hessenberg-givens-1};
    \end{tikzpicture}}

  \item Do the same with the second and third rows.

    {\small\begin{tikzpicture}
        \input{fig/hessenberg-givens-2};
      \end{tikzpicture}}

    {\small\begin{tikzpicture}
        \input{fig/hessenberg-givens-3};
      \end{tikzpicture}}

    Note that columns with two zero entries remain unchanged and will not have to be processed.
  \item Now the matrix is upper triangular and the transformation was
    \begin{gather*}
      \matq^* = \givens_{34}^*\givens_{23}^*\givens_{12}^*.
    \end{gather*}

  \item When we go back, we apply Givens rotations from the right, thus affecting columns of the matrices.

    {\footnotesize\begin{tikzpicture}\input{fig/hessenberg-givens-4};\end{tikzpicture}
        \begin{tikzpicture}\input{fig/hessenberg-givens-5};\end{tikzpicture}
        \begin{tikzpicture}\input{fig/hessenberg-givens-6};\end{tikzpicture}
        \begin{tikzpicture}\input{fig/hessenberg-givens-6-result};\end{tikzpicture}}
  \end{enumerate}
\end{example}

\begin{todo}
  GM: Theorem does not make sense with this version of the QR
\end{todo}
\begin{Theorem}{Hessenberg-qr}
  The QR factorization of a Hessenberg matrix $\matH$ can be obtained
  by $n-1$ Givens rotations. The matrix $\matr\matq$ is again in
  Hessenberg form. For a (complex) symmetric matrix $\matH$, the
  matrix $\matr\matq$ is even tridiagonal and (complex) symmetric.
\end{Theorem}

\begin{Corollary}{Hessenberg-qr}
  The complexity of each step of a QR-iteration for Hessenberg matrices is $\bigo(n^2)$. For tridiagonal (complex) symmetric matrices, it is $\bigo(n)$.
\end{Corollary}

\begin{Theorem}{Hessenberg-householder}
  Every matrix $\mata\in\Cnn$ is unitarily similar to a Hessenberg matrix $\matH$, that is,
  \begin{gather}
    \matH = \matq^* \mata \matq.
  \end{gather}
  The matrix $\matq$ can be obtained by $n-2$ \putindex{Householder
    reflection}s.
\end{Theorem}

\begin{todo}
  GM: Step 2 needs to be rewriitten for the new algorithm
\end{todo}
\begin{Algorithm*}{qr-method}{The QR-Method}
  Compute the spectrum of a matrix $\mata\in\Cnn$ by
  \begin{enumerate}
  \item Use $n-2$ Householder transformations to transform $\mata$ to
    Hessenberg form
    \begin{gather}
     \matH = \matq^*\mata\matq.
   \end{gather}
 \item QR-iteration: let $\matH_{0}=\matH$ and perform until convergence
   \begin{align}
     \givens^{(k)}_{1,2}\times\dots\times\givens^{(k)}_{n-1,n} \matr &= \matH_k\\
     \matH_{k+1} &= \matr \givens^{(k)}_{1,2}\times\dots\times\givens^{(k)}_{n-1,n}.
   \end{align}
 \item Store Householder vectors as well as $r$ and $c$ for each
   Givens rotation if the eigenvectors are desired in the end.
  \end{enumerate}
\end{Algorithm*}

\begin{Theorem}{hessenberg-qr-convergence}
    Let $\matH\in\Cnn$ be a Hessenberg matrix with eigenvalues such that
  \begin{gather}
    \abs{\lambda_1} >
    \abs{\lambda_2}>\dots>\abs{\lambda_n}.
  \end{gather}
  Then, the sequences of the QR-iteration admit the following estimates:
  \begin{align}
    \dist(\esp{1,\dots,j},\ran{\matq^{(k)}}) &= \bigo \left(\abs*{\frac{\lambda_{j+1}}{\lambda_j}}^k\right),
    \\
    h_{j+1,j}^{(k)} &= \bigo \left(\abs*{\frac{\lambda_{j+1}}{\lambda_j}}^k\right)
                      .
  \end{align}
  Here, $h_{ij}^{(k)}$ are the entries of $\matH_k$.
\end{Theorem}

\begin{proof}
  See~\cite[Theorem 7.3-1]{GolubVanLoan83}.
\end{proof}

\subsection{Deflation and shifts}

\begin{intro}
  The goal of this section is the development and justification of a
  method which accelerates convergence of the QR-iteration and
  reducing the effort at the same time. It is based on shifts, like
  for the simple or inverse power method. But, shifts are much more
  powerful here, since we compute not only ``converging subspace'',
  but also its complement. The presentation follows
  mostly~\cite{GolubVanLoan83}.
\end{intro}

\begin{Theorem}{qr-reduction}
  Let the matrix $\matH^{(k)}\in\Cnn$ in the QR iteration be of the
  form
  \begin{gather}
    \matH^{(k)} =
    \begin{pmatrix}
      \matH_{11} & \mata_{12}\\0 & \matH_{22}
    \end{pmatrix}
  \end{gather}
  with Hessenberg matrices $\matH_{11}\in\C^{p\times p}$,
  $\matH_{22}\in \C^{n-p\times n-p}$ and an arbitrary matrix
  $\mata_{12}\in \C^{p\times n-p}$. Then, the matrix $\matq^{(k)}$
  decouples into two diagonal blocks and $\matH^{(k+1)}$ has the same
  form. Thus, the iteration decouples into two separate iterations. If
  $p=n-1$, then $h_{nn}$ approximates an eigenvalue.
\end{Theorem}

\begin{Algorithm*}{qr-deflation}{Deflation}
  After each step of the shifted QR-iteration monitor the subdiagonal
  elements of $\matH^{(k)}$. Whenever
  \begin{gather}
    \abs{h_{j,j-1}} \le \eps \bigl(\abs{h_{j-1,j-1}}+\abs{h_{jj}}\bigr)
  \end{gather}
  set $h_{j,j-1}=0$.

  If this happens in the last row, consider $h_{nn}=\lambda_n$
  converged and proceed with a matrix of dimension $n-1\times n-1$.

  If this happens in the center of the matrix, proceed with both
  remaining diagonal blocks separately.
\end{Algorithm*}

\begin{remark}
  The purpose of deflation is removing Schur vectors from the
  iteration. Thus, if one of the Schur vectors for a multiple
  eigenvalue has converged, the remaining iterations will deal with
  reduced multiplicity. Deflation will also help us to deal with the
  requirement that all eigenvalues must have different modulus, but
  this is solved below in combination with shifts.
\end{remark}

\begin{Definition}{hessenberg-unreduced}
  A Hessenberg matrix is called \define{unreduced} if all entries on
  the first subdiagonal are nonzero. It is called \define{reduced}
  otherwise.
\end{Definition}

\begin{Theorem*}{implicit-Q}{Implicit Q Theorem}
  Let $\mata\in\Cnn$ arbitrary, let $\matq,\matv\in\Cnn$ unitary such that
  \begin{gather}
    \matq^*\mata\matq = \matH,\qquad \matv^*\mata\matv = \matg,
  \end{gather}
  where $\matH$ and $\matg$ are Hessenberg matrices. Let $k$ denote
  the smallest integer such that $h_{k+1,k} = 0$, or $k=n$ if $\matH$
  is unreduced. Assume $\vv_1 = \vq_1$. Then,
  $\vv_j = e^{i_j\phi}\vq_j$ and $\abs{h_{j,j-1}} = \abs{g_{j,j-1}}$
  for $j=1,\dots,k$. if $k<n$, then $g_{k+1,k} = 0$.
\end{Theorem*}

\begin{proof}
  See \cite[Theorem 7.4-2]{GolubVanLoan83}.
\end{proof}

\begin{Definition}{essentially-equal}
  The \putindex{Implicit Q Theorem} says that two Hessenberg forms of $\mata$ with the same initial reduction vector are \define{essentially equal} in the sense that they only differ by the diagonal scaling $\matg = \matd^{-1}\matH\matd$ where $\matd=\diag(d_1,\dots,d_n)$ matrix with $\abs{d_{i}} = 1$.
\end{Definition}

\begin{todo}
  GM: The result, that the QR is the same as the Subspace iteration is not correct at this point anymore.
\begin{Algorithm*}{shifted-qr-iteration}{QR iteration with shift}
  \begin{algorithmic}[1]
    \Require $\mata\in\Cnn$
    \State $\matH_0 = \matq_0^*\mata\matq_0$\Comment{Hessenberg form}
    \For {$k=1,\ldots$ until convergence}
    \State $\matq_k\matr_k = \matH_{k-1} - \sigma_k\id$\Comment{QR factorization}
    \State $\matH_{k} = \matq_k^* \matH_{k-1} \matq_k $
    \EndFor
  \end{algorithmic}
\end{Algorithm*}
\end{todo}

\begin{Algorithm*}{shifted-qr-iteration}{QR iteration with shift}
  \begin{algorithmic}[1]
    \Require $\mata\in\Cnn$
    \State $\matH_0 = \matq_0^*\mata\matq_0$\Comment{Hessenberg form}
    \For {$k=1,\ldots$ until convergence}
    \State $\matq_k\matr_k = \matH_{k-1} - \sigma_k\id$\Comment{QR factorization}
    \State $\matH_{k} = \matr_k\matq_k + \sigma_k\id$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{shifted-qr-convergence}
  The shifted QR-iteration admits the estimate
  \begin{gather}
    h_{j+1,j}^{(k)} = \bigo \left(\abs*{\frac{\lambda_{j+1}-\sigma}{\lambda_j-\sigma}}^k\right)
  \end{gather}
\end{Lemma}

\begin{todo}
  Put perfect shift here, Theorem 7.5.1 from Golub/van Loan. It is needed for implicitly restarted Arnoldi.
\end{todo}

\begin{Example*}{rayleigh-shift}{Rayleigh shift}
  The Rayleigh quotient for the smallest eigenvalue by magnitude
  converges to $h_{nn}$, as
  \begin{gather}
    \ve_n^* H^{(k)} \ve_n = h_{nn}^{(k)}
  \end{gather}
  and $\vq_n$ is orthogonal to all eigenvectors for eigenvalues of
  greater magnitude. Therefore, using $\sigma_k = h_{nn}^{(k)}$ seems
  a good idea, and often is. But it is not reliable, as in the example
  \begin{gather}
    H =
    \begin{pmatrix}
      0 & 1 \\ 1 & 0
    \end{pmatrix}.
  \end{gather}
\end{Example*}

\begin{Definition*}{wilkinson-shift}{Wilkinson shift}
  Let
  \begin{gather}
    \matm =
    \begin{pmatrix}
      h_{n-1,n-1}^{(k)}&h_{n-1,n}^{(k)}\\h_{n,n-1}^{(k)}&h_{nn}^{(k)}
    \end{pmatrix}.
  \end{gather}
  Then, for $\sigma_k$ use the eigenvalue of $\matm$ which is closer
  to $h_{nn}^{(k)}$.
\end{Definition*}

\begin{Remark}{wilkinson-shift}
  The Wilkinson shift is reliable and the $h_{n,n-1}$ and $h_{nn}$
  converge to zero and the smallest eigenvalue by magnitude,
  respectively. They converge at least quadratically and cubically in
  the symmetric case~\cite[Section 8.2]{GolubVanLoan83}.
\end{Remark}

\begin{Lemma*}{perfect-shift}{Perfect shift}
  Let $\matH\in\Cnn$ be an unreduced Hessenberg matrix with eigenvalue
  $\sigma$. Let $\matq\matr = \matH - \sigma\id$ be a QR factorization
  and $\widetilde\matH = \matr\matq+\sigma\id$. Then,
  $\tilde h_{n,n-1}=0$ and $\tilde h_{nn} =\sigma$.
\end{Lemma*}

\begin{proof}
  See also~\cite[Theorem 7.5.1]{GolubVanLoan83}.  Since $\matH$ is
  unreduced, its first $n-1$ columns are linearly independent. Hence,
  if $\matq\matr=\matH-\sigma\id$ is a QR factorization, then
  $r_{ii} = 0$ for $i=1,\dots,n-1$.

  Since $\matH-\sigma\id$ is singular, we conclude $r_{nn}=0$. Thus,
  the last row of $\matr\matq$ is zero and the statement holds.
\end{proof}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
