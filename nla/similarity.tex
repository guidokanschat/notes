\begin{Definition}{similar-matrix}
  Two matrices $\mata,\matb\in\C^{n\times n}$ are called \define{similar}, if there is a nonsingular matrix $\matv\in\C^{n\times n}$, such that
  \begin{gather}
      \mata = \matv \matb \matv^{-1}.
  \end{gather}
  We call the mapping
  \begin{gather}
      \matb \mapsto \matv \matb \matv^{-1}
  \end{gather}
  a \define{similarity transformation}.
\end{Definition}

\begin{Lemma}{similarity-equivalence}
  Similarity is an equivalence relation, that is, it is reflexive,
  symmetric, and transitive.
\end{Lemma}

\begin{Lemma}{matrix-basis-change}
  Let $\phi\colon \C^n\to \C^n$ be a linear mapping represented with
  respect to the canonical basis by the matrix $\mata$, such that
  \begin{gather}
    \phi(\vx) = \mata \vx.
  \end{gather}
  Then, the matrix $\matb = \matv^{-1}\mata\matv$ represents $\phi$
  with respect to the basis $\matv$, namely, if $\vu = \matv \vy$ and
  $\phi(\vu) = \matv \vz$, then
  \begin{gather}
    \vz = \matb \vy.
  \end{gather}
\end{Lemma}

\begin{Lemma}{similarity-eigenvalues}
  The eigenvalues of a matrix are invariant under similarity transformations.
\end{Lemma}

\begin{proof}
  We use the definition of eigenvalues as roots of the characteristic
  polynomial and show that the characteristic polynomial does not
  change under similarity transformation. Let
  $\matb = \matv^{-1}\mata\matv$. Note that
  \begin{gather}
    \matb-\lambda\id = \matv^{-1}\mata\matv - \lambda \matv^{-1}\matv
    = \matv^{-1}(\mata-\lambda\id)\matv.
  \end{gather}
  Hence,
  \begin{gather}
    \chi_{\matb}(\lambda) = \det(\matv^{-1}{(\mata-\lambda\id)}\matv)
    = \det(\matv^{-1})\det(\mata-\lambda\id)\det(\matv)
    = \chi_{\mata}(\lambda).
  \end{gather}
\end{proof}

\begin{Theorem*}{Jordan-canonical-form}{Jordan canonical form}
  Every matrix $A\in\C^{n\times n}$ is similar to a matrix with block structure
  \begin{gather}
    \begin{pmatrix}
      J_1\\&J_2\\&&\ddots\\&&&J_k
    \end{pmatrix},
  \end{gather}
  where each block has the form
  \begin{gather}
    J_i = \begin{pmatrix}
      \lambda_i&1\\&\ddots&\ddots\\
      &&\lambda_i&1\\
      &&&\lambda_i
    \end{pmatrix}.
  \end{gather}
\end{Theorem*}


\begin{Definition}{diagonalizable}
  A matrix is called \define{diagonalizable}, if it is similar to a diagonal matrix, that is,
  \begin{gather}
    \matlambda = \matv^{-1} \mata \matv
  \end{gather}
  with a diagonal matrix $\matlambda$ of eigenvalues.
\end{Definition}

\begin{remark}
  Let us apply \slideref{Lemma}{matrix-basis-change} to this
  definition such that $\matlambda$ is the matrix representing $\mata$
  in the basis $\matv$.

  By our method of writing bases as matrices, the transformation
  matrix $\matv$ corresponds to a basis $\vv_i = \matv\ve_i$.

  Since $\matlambda$ is diagonal, we have
  \begin{gather}
    \matlambda\ve_i = \lambda_e\ve_i.
  \end{gather}
  \slideref{Lemma}{matrix-basis-change} now tells us that
  \begin{gather}
    \mata\vv_i = \lambda_i\matv\ve_i=\lambda_i\vv_i.    
  \end{gather}
  Hence, the columns of $\matv$ are indeed eigenvectors of $\mata$.
\end{remark}

\begin{Theorem}{matrix-functions}
  Let $\mata\in \C^{n\times n}$ be diagonalizable such that
  \begin{gather}
    \mata = \matv \matlambda \matv^{-1},
    \qquad \matlambda = \diag(\lambda_1,\dots,\lambda_n).
  \end{gather}
  Then, for any analytic function $f\colon \C \to \C$, the matrix
  $f(\mata)$ is well defined by
  \begin{gather}
    f(\mata) = \matv f(\matlambda) \matv^{-1},
    \qquad f(\matlambda) = \diag\bigl(f(\lambda_1),\dots,f(\lambda_n)\bigr),
  \end{gather}
  if all eigenvalues are in the domain of convergence of the Tayor
  series of $f$.
\end{Theorem}

\begin{proof}
  First, we observe that
  \begin{gather}
    \mata^2 = \matv \matlambda \matv^{-1} \matv \matlambda \matv^{-1}
    = \matv \matlambda^2 \matv^{-1}.
  \end{gather}
  The square of the diagonal matrix $\matlambda$ is easily computed.
  By induction $\mata^k =  \matv \matlambda^k \matv^{-1}$.

  Let now $f(x) = \sum a_k x^k$ be the Taylor series of $f$. Then,
  \begin{gather}
    \begin{split}
      f(\mata)
      &= \sum_{k=0}^\infty a_k  \matv \matlambda^k \matv^{-1}\\
      &= \matv \left(\sum_{k=0}^\infty a_k \matlambda^k\right) \matv^{-1}\\
      &= \matv \diag\left(
        \sum_{k=0}^\infty a_k \lambda_1^k,\dots,
        \sum_{k=0}^\infty a_k\lambda_n^k
      \right)  \matv^{-1}.
    \end{split}
  \end{gather}
  All limits are well defined since we assume that all eigenvalues are
  in the domain of convergence.
\end{proof}

\begin{Theorem}{simultaneous-diagonalization}
  Two diagonalizable matrices $\mata, \matb\in \C^{n\times n}$ can be
  diagonalized by the same set of eigenvectors if and only if they
  commute. namely $\mata\matb = \matb\mata$.
\end{Theorem}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
