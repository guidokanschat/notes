\begin{Definition}{similar-matrix}
  Two matrices $\mata,\matb\in\C^{n\times n}$ are called \define{similar}, if there is a nonsingular matrix $\matv\in\C^{n\times n}$, such that
  \begin{gather}
      \mata = \matv \matb \matv^{-1}.
  \end{gather}
  We call the mapping
  \begin{gather}
      \matb \mapsto \matv \matb \matv^{-1}
  \end{gather}
  a \define{similarity transformation}.
\end{Definition}

\begin{Lemma}{similarity-equivalence}
  Similarity is an equivalence relation, that is, it is reflexive,
  symmetric, and transitive.
\end{Lemma}

\begin{Lemma}{matrix-basis-change}
  Let $\phi\colon \C^n\to \C^n$ be a linear mapping represented with
  respect to the canonical basis by the matrix $\mata$, such that
  \begin{gather}
    \phi(\vx) = \mata \vx.
  \end{gather}
  Then, the matrix $\matb = \matv^{-1}\mata\matv$ represents $\phi$
  with respect to the basis $\matv$, namely, if $\vu = \matv \vy$ and
  $\phi(\vu) = \matv \vz$, then
  \begin{gather}
    \vz = \matb \vy.
  \end{gather}
\end{Lemma}

\begin{Lemma}{similarity-eigenvalues}
  The eigenvalues of a matrix are invariant under similarity transformations.
\end{Lemma}

\begin{proof}
  We use the definition of eigenvalues as roots of the characteristic
  polynomial and show that the characteristic polynomial does not
  change under similarity transformation. Let
  $\matb = \matv^{-1}\mata\matv$. Note that
  \begin{gather}
    \matb-\lambda\id = \matv^{-1}\mata\matv - \lambda \matv^{-1}\matv
    = \matv^{-1}(\mata-\lambda\id)\matv.
  \end{gather}
  Hence,
  \begin{gather}
    \chi_{\matb}(\lambda) = \det(\matv^{-1}{(\mata-\lambda\id)}\matv)
    = \det(\matv^{-1})\det(\mata-\lambda\id)\det(\matv)
    = \chi_{\mata}(\lambda).
  \end{gather}
\end{proof}

\begin{Lemma}{invariant-blockmatrix}
  Let $\mata\in\Cnn$ have invariant subspaces $V_i$ of dimension
  $n_i$, $i=1,\dots,k$, such that $\C^n= \bigoplus V_i$. Then, there
  is a basis $\matv$ such that
  \begin{gather}
    \matb = \matv^{-1}\mata\matv =
    \begin{pmatrix}
      \matb_1\\&\ddots\\&&\matb_k,
    \end{pmatrix}
  \end{gather}
  where each block $\matb_i\in\C^{n_i\times n_i}$.
\end{Lemma}

\begin{proof}
  For each $i=1,\dots,k$ choose a basis $\vv_{i,1},\dots,\vv_{i,n_i}$
  of $V_i$. Concatenate these bases to obtain
  \begin{gather}
    \matv = \bigl(\vv_{1,1},\dots,\vv_{1,n_1},\vv_{2,1},\dots,\vv_{2,n_2},\dots,\vv_{k,1},\dots,\vv_{k,n_k}\bigr).
  \end{gather}
  Let now $\vu\in V_i$, then $\vu = \matv\vx$, where the coefficient
  vector has nonzero entries only betwen the positions of $\vv_{i,1}$
  and $\vv_{i,n_i}$. Since $\matb V_i\subset V_i$, the coefficient
  vector of $\matb\vu = \matv\vy$ is also nonzero only in this
  range. In particular, all entries of rows in this index range are
  zero for columns not in this index range.
\begin{todo}
    Improve this text!
\end{todo}
\end{proof}

\begin{Theorem*}{Jordan-canonical-form}{Jordan canonical form}
  Every matrix $A\in\C^{n\times n}$ is similar to a matrix with block structure
  \begin{gather}
    \begin{pmatrix}
      \matj_1\\&\matj_2\\&&\ddots\\&&&\matj_k
    \end{pmatrix},
  \end{gather}
  where each block has the form
  \begin{gather}
    \matj_i = \begin{pmatrix}
      \lambda_i&1\\&\ddots&\ddots\\
      &&\lambda_i&1\\
      &&&\lambda_i
    \end{pmatrix}.
  \end{gather}
\end{Theorem*}


\begin{Definition}{diagonalizable}
  A matrix is called \define{diagonalizable}, if it is similar to a diagonal matrix, that is,
  \begin{gather}
    \matlambda = \matv^{-1} \mata \matv
  \end{gather}
  with a diagonal matrix $\matlambda$ of eigenvalues.
\end{Definition}

\begin{remark}
  Let us apply \slideref{Lemma}{matrix-basis-change} to this
  definition such that $\matlambda$ is the matrix representing $\mata$
  in the basis $\matv$.

  By our method of writing bases as matrices, the transformation
  matrix $\matv$ corresponds to a basis $\vv_i = \matv\ve_i$.

  Since $\matlambda$ is diagonal, we have
  \begin{gather}
    \matlambda\ve_i = \lambda_e\ve_i.
  \end{gather}
  \slideref{Lemma}{matrix-basis-change} now tells us that
  \begin{gather}
    \mata\vv_i = \lambda_i\matv\ve_i=\lambda_i\vv_i.    
  \end{gather}
  Hence, the columns of $\matv$ are indeed eigenvectors of $\mata$.
\end{remark}

\begin{Theorem}{matrix-functions}
  Let $\mata\in \C^{n\times n}$ be diagonalizable such that
  \begin{gather}
    \mata = \matv \matlambda \matv^{-1},
    \qquad \matlambda = \diag(\lambda_1,\dots,\lambda_n).
  \end{gather}
  Then, for any analytic function $f\colon \C \to \C$, the matrix
  $f(\mata)$ is well defined by
  \begin{gather}
    f(\mata) = \matv f(\matlambda) \matv^{-1},
    \qquad f(\matlambda) = \diag\bigl(f(\lambda_1),\dots,f(\lambda_n)\bigr),
  \end{gather}
  if all eigenvalues are in the domain of convergence of the Tayor
  series of $f$.
\end{Theorem}

\begin{proof}
  First, we observe that
  \begin{gather}
    \mata^2 = \matv \matlambda \matv^{-1} \matv \matlambda \matv^{-1}
    = \matv \matlambda^2 \matv^{-1}.
  \end{gather}
  The square of the diagonal matrix $\matlambda$ is easily computed.
  By induction $\mata^k =  \matv \matlambda^k \matv^{-1}$.

  Let now $f(x) = \sum a_k x^k$ be the Taylor series of $f$. Then,
  \begin{gather}
    \begin{split}
      f(\mata)
      &= \sum_{k=0}^\infty a_k  \matv \matlambda^k \matv^{-1}\\
      &= \matv \left(\sum_{k=0}^\infty a_k \matlambda^k\right) \matv^{-1}\\
      &= \matv \diag\left(
        \sum_{k=0}^\infty a_k \lambda_1^k,\dots,
        \sum_{k=0}^\infty a_k\lambda_n^k
      \right)  \matv^{-1}.
    \end{split}
  \end{gather}
  All limits are well defined since we assume that all eigenvalues are
  in the domain of convergence.
\end{proof}

\begin{Theorem}{simultaneous-diagonalization}
  Two diagonalizable matrices $\mata, \matb\in \C^{n\times n}$ can be
  diagonalized by the same set of eigenvectors if and only if they
  commute. namely $\mata\matb = \matb\mata$.
\end{Theorem}

\begin{proof}
  First, we show that commutativity implies joint diagonalizability. To this end, 
  let $\vv$ be an eigenvector of $\mata$ such that $\mata\vv = \lambda\vv$. Then,
  \begin{gather}
    \label{eq:similarity:1}
    \mata\matb\vv = \matb\mata\matv = \lambda\matb\matv.
  \end{gather}
  Hence, $\matb\vv$ is also an eigenvector of $\mata$ for eigenvalue
  $\lambda$. Now, we distinguish two cases: if $\lambda$ is a simple
  eigenvalue of $\mata$, we immediately see that $\matb\vv$ must be a
  multiple of $\vv$ and hence it is an eigenvector of $\matb$ to some
  eigenvalue.

  If $\lambda$ is a multiple eigenvalue of $\mata$, we investigate its
  eigenspace $V_\lambda$. From~\eqref{eq:similarity:1}, we obtain that
  this space is an invariant subspace of $\matb$. Hence, there is a
  basis $\matv$ of eigenvectors of $\mata$ with those in $V_\lambda$
  first, such that
  \begin{gather}
    \matv^{-1}\matb\matv =
    \begin{pmatrix}
      \matb_1&\\&\matb_2
    \end{pmatrix}
  \end{gather}
  Since $\matb$ is diagonalizable, the basis for $V_\lambda$ can be
  chosen such that $\matb_1$ is diagonal. Doing this for any
  eigenspace of $\mata$ yields the joint diagonalizability.

  Assume now that there is a basis $\matv$ such that
  $\matlambda_{\mata} = \matv^{-1}\mata\matv$ and
  $\matlambda_{\matb} = \matv^{-1}\matb\matv$. Then,
  \begin{gather}
    \mata\matb = \matv\matlambda_{\mata}\matv^{-1}\matv\matlambda_{\matb}\matv^{-1}
    = \matv\matlambda_{\mata}\matlambda_{\matb}\matv^{-1}
    = \matv\matlambda_{\matb}\matlambda_{\mata}\matv^{-1}
    = \matb\mata.
  \end{gather}
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
