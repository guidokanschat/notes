\subsection{Conditioning of the eigenvalue problem}

In this section, we study the conditioning of finding eigenvalues and
eigenvectors. While we will not cover the full theory, we will provide
examples for ill-posed problems as well as exemplary proofs for
well-posedness.

In all cases, we will investigate the change of eigenvalues or
eigenvectors when the matrix $\mata$ is perturbed by a small matrix
$\mate$ of norm $\epsilon$.

\begin{Lemma}{bound-by-norm}
  Let $\norm{\cdot}$ be a vector norm and denote by the same symbol
  a consistent norm for matrices. Then, for any matrix $\mata\in\Cnn$
  and for any eigenvalue $\lambda\in\sigma(\mata)$ there holds
  \begin{gather}
    \abs{\lambda} \le \norm{\mata}.
  \end{gather}
\end{Lemma}

\begin{Lemma}{pre-gershgorin}
  Let $\mata,\matb\in\Cnn$ and let $\norm{\cdot}$ be an operator norm
  on the space of matrices corresponding to a vector norm denoted by
  $\norm{\cdot}$ as well. Then, for any eigenvalue
  $\lambda\in\sigma(\mata)$ such that $\lambda\not\in\sigma(\matb)$
  there holds
  \begin{gather}
    \norm*{(\lambda\id-\matb)^{-1}(\mata-\matb)} \ge 1.
  \end{gather}
\end{Lemma}

\begin{proof}
  Let $(\lambda,\vv)$ be an eigenpair of $\mata$. Then,
  \begin{gather}
    (\mata-\matb)\vv = (\lambda\id-\matb) \vv.
  \end{gather}
  If $\lambda$ is not an eigenvalue of $\matb$, the second matrix is invertible and there holds
  \begin{gather}
    (\lambda\id-\matb)^{-1}(\mata-\matb)\vv = \vv.
  \end{gather}
  Hence, we use the definition of an operator norm to obtain conclude
  that the norm of the matrix on the left must be not less than one.
\end{proof}

\begin{Definition}{conditioning-eigenvalue}
  Let $\mata\in\Cnn$ and let $\mata+\mate$ be a perturbation of $\mata$. Then, we define the \define{absolute conditioning}\index{conditioning!absolute} of the eigenvalue problem as follows: let $\mu$ be an eigenvalue of $\mata+\mate$, then, there is an eigenvalue $\lambda$ of $\mata$ and a constant $C_{\text{abs}}$ called \define{condition number}, such that
  \begin{gather*}
    \abs{\lambda-\mu} \le C_{\text{abs}} \norm{E},
  \end{gather*}
  for a suitable matrix norm.
  %The \define{relative conditioning}\index{conditioning!relative} is defined by
  %the estimate
  %\begin{gather*}
  %  \frac{\abs{\lambda-\mu}}{\lambda}
  %  \le C_{\text{rel}} \frac{\norm{E}}{\norm{A}}.
  %\end{gather*}
\end{Definition}

\begin{Example}{characteristic-polynomial}
  Take a matrix of dimension 20 with eigenvalues $1,2,\ldots,20$. Its
  characteristic polynomial is
  \begin{gather}
    \chi(\lambda) = (\lambda-1)\dots(\lambda-20).
  \end{gather}
  The coefficient in front of $\lambda^{20}$ is one, the constant term is $20! > 10^{19}$.
  We perturbe it in the form
  \begin{gather}
    \tilde \chi(\lambda) = \chi(\lambda) - 10^{-23}\lambda^{19}.
  \end{gather}
  Their greatest roots are
  \begin{gather}
    \begin{array}{l@{\qquad}l@{\,}c@{\,}l}
      \multicolumn{1}{c}{\chi}&
      \multicolumn{3}{c}{\tilde \chi}\\
      20&20.847\\
      19,18&19.502&\pm&1.940i\\
      17,16&16.731&\pm&2.813i\\
      15,14&13.992&\pm&2.519i\\
    \end{array}
  \end{gather}
  {\tiny Source: \cite{DeuflhardHohmann08}}
\end{Example}

\begin{Example}{conditioning-Jordan-block}
  Consider the matrix
  \begin{gather}
  \mata_\epsilon =
      \begin{pmatrix}
        0&1\\
%        &0&1\\
        &\ddots&\ddots\\
        &&0&1\\
        \epsilon &&&0
      \end{pmatrix}
      \in\C^{n\times n},
  \end{gather}
  For $\epsilon=0$, it has a single eigenvalue of geometric multiplicity one and algebraic multiplicity $n$.

  For $\epsilon>0$, it has $n$ simple eigenvalues
  \begin{gather}
      \lambda_j = \sqrt[n]{\epsilon} \,e^{2\frac jni\pi}.
  \end{gather}
\end{Example}

\begin{proof}
  For $\epsilon=0$, the matrix is the generic Jordan-block of an eigenvalue which is not semi-simple, thus the ill-posedness of this example implies the ill-posedness for not semi-simple eigenvalues in the general case. Note that this statement holds notwithstanding that special perturbations may be benign.

  The characteristic polynomial of this matrix is
  \begin{gather}
      \chi(\lambda) = \det(\mata-\lambda\id)
      = \det\begin{pmatrix}
      -\lambda&1\\
        &\ddots&\ddots\\
        &&-\lambda&1\\
        \epsilon &&&-\lambda
      \end{pmatrix}.
  \end{gather}
  Applying Laplace expansion to the first column yields
  \begin{gather}
      \chi(\lambda)
      = -\lambda \det\begin{pmatrix}
        -\lambda&1\\
        &\ddots&\ddots\\
        &&-\lambda&1\\
        &&&-\lambda
      \end{pmatrix}
      + (-1)^{n+1} \epsilon\det\begin{pmatrix}
        1 \\
        -\lambda &1\\
        &\ddots&\ddots\\
        &&-\lambda&1
      \end{pmatrix},
  \end{gather}
  where both matrices are of dimension $n-1$. Since they are triangular, recursion of Laplace expansion is particularly simple and yields the product of the diagonal elements. Thus
  \begin{gather}
      \chi(\lambda) = (-1)^n \lambda^n
      + (-1)^{n+1} \epsilon.
  \end{gather}
  Its roots are determined by the condition
  \begin{gather}
      \lambda^n = \epsilon.
  \end{gather}
  Thus, $\lambda$ can be computed as an $n$th root of unity times the (real) $n$th root of $\epsilon$.
\end{proof}

\begin{Theorem}{Jordan-block-ill-conditioned}
  The eigenvalue problem for eigenvalues which are not semi-simple is
  in general ill-posed.
\end{Theorem}

\begin{proof}
  The analysis in \slideref{Example}{conditioning-Jordan-block} is
  generic in the sense that it applies to nonzero eigenvalues and also
  to matrices which are similar to such a block. Thus, we can conclude
  that for every matrix $\mata$ which is similar to a matrix with a
  nontrivial Jordan block for eigenvalue $\lambda$, there is a
  perturbation $\mate$ such that the derivative of the function
  $\lambda(\epsilon) = \lambda(A+\epsilon\mate)$ at zero is unbounded.
\end{proof}

\begin{Theorem*}{bauer-fike}{Bauer-Fike}
  Let $\mata\in \Cnn$ be diagonalizable with matrix of eigenvectors
  $\matv \in \Cnn$ and diagonal matrix
  $\matlambda = \diag(\lambda_1\dots,\lambda_n)$. Let $\mata+\mate$ be
  a perturbation of $\mata$. Then, for any eigenvalue $\mu$ of
  $\mata+\mate$, there is an eigenvalue $\lambda_i$ of $\mata$ such
  that
  \begin{gather}
    \abs{\mu-\lambda_i} \le \cond_2(\matv) \norm{\mate}_2.
  \end{gather}
\end{Theorem*}

\begin{proof}
  We rewrite the eigenvalue equations $\mata\vv_i = \lambda_i \vv_i$ in matrix form
  \begin{gather}
    \mata\matv = \matv \matlambda.
  \end{gather}
  Without loss of generality, we assume $\mu \not\in\sigma(\mata)$. Therefore,
  \begin{align}
    \norm{(\mu\id-\mata)^{-1}}_2
    &= \norm{\matv(\mu\id-\mata)^{-1}\matv^{-1})}_2\\
    & \le \norm{\matv}_2\norm{(\lambda\id-\matlambda)^{-1}}\norm{\matv^{-1}}_2\\
    &\le \cond_2(\matv) \max_i{\abs{\mu-\lambda_i}^{-1}}.
  \end{align}
  Using \slideref{Lemma}{pre-gershgorin}, we conclude
  \begin{align}
    \min\abs{\mu-\lambda_i}
    &\le \cond_2(\matv) \frac1{\norm{(\mu\id-\mata)^{-1}}_2} \norm{(\mu\id-\mata)^{-1}\mate}\\
    & \le \cond_2(\matv)\norm{\mate}_2.
  \end{align}
\end{proof}

\begin{Corollary}{conditioning-eigenvalues-normal}
  The eigenvalue problem of a normal matrix $\mata\in\Cnn$ is
  well-conditioned in the sense that for every eigenvalue $\mu$ of the
  perturbed matrix $\mata+\mate$, there is an eigenvalue $\lambda$ of
  $\mata$ such that
  \begin{gather}
    \abs{\mu-\lambda} \le \norm{E}_2.
  \end{gather}
\end{Corollary}

The Bauer-Fike theorem provides a general estimate for diagonalizable
matrices in terms of the condition number of the matrix of
eigenvectors. The following theorem is less general, since it only
applies to simple eigenvalues, but it provides geometric intuition of
the issue.

\begin{Theorem}{conditioning-eigenvalue-single}
  Let $\mata_\epsilon = \mata+\epsilon\mate\in\Cnn$ be a perturbation
  of $\mata\in\Cnn$. Let $\lambda(0)$ be a simple
  eigenvalue of $\mata$. Then, there exists a uniquely defined
  differentiable continuation $\lambda(\epsilon)$ for small $\epsilon$
  such that $\lambda(\epsilon) \in \sigma(\mata_\epsilon)$ and with
  its left and right eigenvectors $\vu$, and $\vv$, respectively, there
  holds
  \begin{gather}
    \abs*{\tfrac{d}{d\epsilon} \lambda(0)}
    \le \norm{E}_2\frac{\norm{\vu}_2\norm{\vv}_2}{\abs{\scal(\vu,\vv)}}
    = \norm{E}_2 \frac1{\cos\angle(\vu,\vv)}.
  \end{gather}
\end{Theorem}

\begin{Theorem}{eigenvalues-continuous}
  Let $\mata,\matb\in\Cnn$. Let $\matc(t) = \mata + t \matb$. Then,
  the spectrum of $\matc(t)$ depends continuously on $t$.
\end{Theorem}

\begin{proof}
  The proof relies on another theorem that the set of roots of a polynomial $p_t(x)$ of the form
  \begin{gather}
    p_t(x) = a_0(t) + a_1(t) x + \dots + a_n(t) x^n,
  \end{gather}
  with continuous coefficient functions $a_i(t)$ depends continuously on $t$.

  If we are willing to make the assumption that the matrices
  $\matc(t)$ are diagonalizable for any $t$ with a uniform bound on
  the condition number of the transformation matrix $\matv(t)$, this
  result is a consequence of the Bauer-Fike theorem.
\end{proof}

\subsection{Conditioning of eigenvectors and eigenspaces}

\begin{intro}
  Positive results on the conditioning of eigenvectors require
  additional tools which go beyond the exposition planned for this
  class. We will thus only discuss this question at hand of an example
  and conclude a rule of thumb.
\end{intro}

\begin{Example}{conditioning-eigenvectors}
  Consider the two matrices
  \begin{gather}
    A =
    \begin{pmatrix}
      1-\epsilon & 0\\ 0 & 1+\epsilon
    \end{pmatrix},
    \qquad
    B =
    \begin{pmatrix}
      1&\epsilon\\\epsilon&1
    \end{pmatrix}.
  \end{gather}
  Their eigenvalues are $1-\epsilon$ and $1+\epsilon$, but their
  eigenvectors differ by an angle of $\pi/4$ independent of
  $\epsilon$.
\end{Example}

\begin{Remark}{conditioning-eigenvectors}
  The problem of finding eigenvectors for tight clusters of
  eigenvalues is ill-posed. Nevertheless, finding the invariant
  subspace associated to all eigenvalues in such a cluster is
  well-posed.

  Conditioning of the eigenvector problem depends on the separation of
  eigenvalues.
\end{Remark}

\begin{Problem}{almost-parallel}
  Consider the matrix 
  \begin{gather} 
    M = 
    \begin{pmatrix}
      \eta & 1\\  \eta &\eta
    \end{pmatrix}
   \end{gather}
   with $|\eta| << 1$.\\
  Explain why the problem of finding eigenvectors is \textit{not} well-posed in this example.
\end{Problem}


\subsection{Bounds on eigenvalues}


\begin{Theorem*}{gershgorin}{Gershgorin circle theorem}
  All eigenvalues of a matrix $\mata\in\Cnn$ are contained in the
  union of the \define{Gershgorin Circle}s
  \begin{gather}
    G_j = \left\{ z\in \C \middle| \abs{z-a_{jj}} \le \sum_{k\neq j} \abs{a_{jk}}\right\}.
  \end{gather}
  Furthermore, if there is a subset of $m$ circles disjoint from the
  other circles, then this subset contains $m$ eigenvalues.
\end{Theorem*}

\subsection{The Rayleigh quotient}

\begin{Definition}{rayleigh-quotient}
  For a matrix $\mata\in\Cnn$ and a vector $\vx\in\C^n$, the
  \define{Rayleigh quotient} is defined as
  \begin{gather}
    R_\mata(\vx) = \frac{\scal(\mata\vx,\vx)}{\scal(\vx,\vx)}.
  \end{gather}
\end{Definition}

\begin{Theorem*}{minmax}{Courant-Fischer min-max theorem}
  Let $\mata\in\Cnn$ be Hermitian with eigenvalues
  $\lambda_1 \ge \lambda_2\ge\dots\ge \lambda_n$. Then, for $k=1,\dots,n$
  \begin{align}
    \lambda_k
    &= \max_{\substack{V \subset \C^n\\\dim V = k}} \min_{\vx\in V} R_\mata(\vx),\\
    &= \min_{\substack{V \subset \C^n\\\dim V = n-k+1}} \max_{\vx\in V} R_\mata(\vx).
  \end{align}
  In particular,
  \begin{gather}
    \lambda_{\min}(\mata) = \min_{\vx\in\C^n} R_\mata(\vx),
    \qquad
    \lambda_{\max}(\mata) = \max_{\vx\in\C^n} R_\mata(\vx).
  \end{gather}
\end{Theorem*}

\begin{proof}
  Since $\mata$ is Hermitian, we make use of the fact that there is an
  orthonormal basis $\matq$ of eigenvectors such that
  $\mata\vq_k = \lambda_k\vq_k$. Let
  \begin{gather}
    W_k = \spann{\vq_{k},\dots,\vq_{n}}
  \end{gather}
  be the space spanned by the last eiqenvectors and let
  $V_k\subset\C^n$ an arbitrary subspace of dimension $k$. By
  comparing dimensions, we see that for the intersection there holds
  $W_k\cap V_k \neq \{0\}$. Any vector $\vv$ in this intersection can
  be expressed as a linear combination
  \begin{gather}
    \vv = \alpha_k \vq_k + \dots + \alpha_n \vq_n.
  \end{gather}
  Hence,
  \begin{gather}
    \scal(\mata\vv,\vv)
    = \lambda_k \abs{\alpha_k}^2 + \dots + \lambda_n \abs{\alpha_n}^2
    \le \lambda_k \left(
      \abs{\alpha_k}^2 + \dots +  \abs{\alpha_n}^2
    \right)
    = \lambda_k\norm{\vv}^2,
  \end{gather}
  where all norms are Euclidean. Thus, the minimum of the Rayleigh
  quotient on the space $V_k$ is bounded from above by
  $\lambda_k$. Since $V_k$ was chosen arbitrarily of dimension $k$,
  we have
  \begin{gather}
    \max_{\substack{V \subset \C^n\\\dim V = k}} \min_{\vx\in V} R_\mata(\vx)
    \le \lambda_k.
  \end{gather}
  
  It remains to show that the maximum is gerater or equal. To this
  end, define a new subspace
  \begin{gather}
    X_k = \spann{\vq_1,\dots,\vq_k},
  \end{gather}
  which is a subspace of dimension $k$. For any
  $\vx = \beta_1\vq_1+\dots+\beta_k\vq_k\in X_k$, there holds
  \begin{gather}
    \scal(\mata\vx,\vx)
    = \lambda_1\abs{\beta_1}^2 + \dots + \lambda_k\abs{\beta_k}^2
    \ge \lambda_k \norm{\vx}^2.
  \end{gather}
  Thus, we have proven the first characterization of $\lambda_k$. The
  second is proven the same way by vonsidering the matrix $-\mata$.
\end{proof}

\begin{remark}
   The eigenvalues in the Courant-Fischer theorem are not ordered by
   their absolute values! This is possible, since they are real. The
   theorem is wrong if formulated with absolute values.

   In many sources, the eigenvalues in the Courant-Fischer theorem are
   ordered the other way round.
\end{remark}

\begin{Lemma}{Rayleigh-approximation}
  Let $\vx = \vv + \epsilon \vw$ be an approximation to an eigenvector
  $\vv$ with eigenvalue $\lambda$, where $\vw\perp\vv$. Then, there holds
  \begin{gather}
    \abs{\lambda - R_\mata(\vx)} = \mathcal O(\epsilon^2).
  \end{gather}
\end{Lemma}

\begin{proof}
  Define $\vy = \mata \vw$, such that $\mata \vx = \lambda \vv + \epsilon \vy$. There holds $\vy\perp\vv$ and
  \begin{gather}
    R_\mata(\vx) = \frac{\scal(\lambda\vv+\epsilon\vy,\vv+\epsilon \vw)}{\norm{\vv+\epsilon \vw}_2^2}
    = \frac{\lambda\norm{\vv}_2^2 + \epsilon^2 \norm{\vy}_2^2}{\norm{\vv}_2^2 + \epsilon^2\norm{\vw}_2^2}.
  \end{gather}
  Subtracting from $\lambda$, the result becomes obvious.
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
