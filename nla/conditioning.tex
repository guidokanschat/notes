\subsection{Bounds on eigenvalues}

\begin{Lemma}{bound-by-norm}
  Let $\norm{\cdot}$ be a vector norm and denote by the same symbol
  a consistent norm for matrices. Then, for any matrix $\mata\in\Cnn$
  and for any eigenvalue $\lambda\in\sigma(\mata)$ there holds
  \begin{gather}
    \abs{\lambda} \le \norm{\mata}.
  \end{gather}
\end{Lemma}

\begin{Lemma}{pre-gershgorin}
  Let $\mata,\matb\in\Cnn$ and let $\norm{\cdot}$ be an operator norm
  on the space of matrices corresponding to a vector norm denoted by
  $\norm{\cdot}$ as well. Then, for any eigenvalue
  $\lambda\in\sigma(\mata)$ such that $\lambda\not\in\sigma(\matb)$
  there holds
  \begin{gather}
    \norm*{(\lambda\id-\matb)^{-1}(A-B)} \ge 1.
  \end{gather}
\end{Lemma}


\begin{Theorem*}{gershgorin}{Gershgorin circle theorem}
  All eigenvalues of a matrix $\mata\in\Cnn$ are contained in the
  union of the \define{Gershgorin Circle}s
  \begin{gather}
    G_j = \left\{ z\in \C \middle| \abs{z-a_{jj}} \le \sum_{k\neq j} \abs{a_{jk}}\right\}. 
  \end{gather}
  Furthermore, if there is a subset of $m$ circles disjoint from the
  other circles, then this subset contains $m$ eigenvalues.
\end{Theorem*}

\subsection{The Rayleigh quotient}

\begin{Definition}{rayleigh-quotient}
  For a matrix $\mata\in\Cnn$ and a vector $\vx\in\C^n$, the
  \define{Rayleigh quotient} is defined as
  \begin{gather}
    R_\mata(\vx) = \frac{\scal(\mata\vx,\vx)}{\scal(\vx,\vx)}.
  \end{gather}
\end{Definition}

\begin{Theorem*}{minmax}{Courant-Fischer min-max theorem}
  Let $\mata\in\Cnn$ be Hermitian with eigenvalues
  $\lambda_1 \le \lambda_2\le\dots\le \lambda_n$. Then, for $k=1,\dots,n$
  \begin{align}
    \lambda_k
    &= \min_{\substack{V \subset \C^n\\\dim V = k}} \max_{\vx\in V} R_\mata(\vx),\\
    &= \max_{\substack{V \subset \C^n\\\dim V = n-k+1}} \min_{\vx\in V} R_\mata(\vx).
  \end{align}
  In particular,
  \begin{gather}
    \lambda_{\min}(\mata) = \min_{\vx\in\C^n} R_\mata(\vx),
    \qquad
    \lambda_{\max}(\mata) = \max_{\vx\in\C^n} R_\mata(\vx).
  \end{gather}
\end{Theorem*}

\subsection{Conditioning of the eigenvalue problem}
In this section, we study the conditioning of finding eigenvalues and
eigenvectors. While we will not cover the full theory, we will provide
examples for ill-posed problems as well as exemplary proofs for
well-posedness.

In all cases, we will investigate the change of eigenvalues or
eigenvectors when the matrix $\mata$ is perturbed by a small matrix
$\mate$ of norm $\epsilon$.

\begin{Example}{characteristic-polynomial}
  Take a matrix of dimension 20 with eigenvalues $1,2,\ldots,20$. Its
  characteristic polynomial is
  \begin{gather}
    \chi(\lambda) = (\lambda-1)\dots(\lambda-20).
  \end{gather}
  The coefficient in front of $\lambda^{20}$ is one, the constant term is $20! > 10^{19}$.
  We perturbe it in the form
  \begin{gather}
    \tilde \chi(\lambda) = \chi(\lambda) - 10^{-23}\lambda^{19}.
  \end{gather}
  Their greatest roots are
  \begin{gather}
    \begin{array}{l@{\qquad}l@{\,}c@{\,}l}
      \multicolumn{1}{c}{\chi}&
      \multicolumn{3}{c}{\tilde \chi}\\
      20&20.847\\
      19,18&19.502&\pm&1.940i\\
      17,16&16.731&\pm&2.813i\\
      15,14&13.992&\pm&2.519i\\
    \end{array}
  \end{gather}
  {\tiny Source: \cite{DeuflhardHohmann08}}
\end{Example}

\begin{Example}{conditioning-Jordan-block}
  Consider the matrix
  \begin{gather}
  \mata_\epsilon =
      \begin{pmatrix}
        0&1\\
        &0&1\\
        &&\ddots&\ddots\\
        &&&0&1\\
        \epsilon &&&&0
      \end{pmatrix}
      \in\C^{n\times n},
  \end{gather}
  For $\epsilon=0$, it has a single eigenvalue of geometric multiplicity one and algebraic multiplicity $n$.
  
  For $\epsilon>0$, it has $n$ simple eigenvalues
  \begin{gather}
      \lambda_j = \sqrt[n]{\epsilon} \,e^{2\frac jni\pi}
  \end{gather}
\end{Example}

\begin{proof}
  For $\epsilon=0$, the matrix is the generic Jordan-block of an eigenvalue which is not semi-simple, thus the ill-posedness of this example implies the ill-posedness for not semi-simple eigenvalues in the general case. Note that this statement holds notwithstanding that special perturbations may be benign.
  
  The characteristic polynomial of this matrix is
  \begin{gather}
      \chi(\lambda) = \det(\mata-\lambda\id)
      = \det\begin{pmatrix}
      -\lambda&1\\  
        &\ddots&\ddots\\
        &&-\lambda&1\\
        \epsilon &&&-\lambda
      \end{pmatrix}.
  \end{gather}
  Applying Laplace expansion to the first column yields
  \begin{gather}
      \chi(\lambda)
      = -\lambda \det\begin{pmatrix}
        -\lambda&1\\
        &\ddots&\ddots\\
        &&-\lambda&1\\
        &&&-\lambda
      \end{pmatrix}
      + (-1)^{n+1} \epsilon\det\begin{pmatrix}
        1 \\
        -\lambda &1\\
        &\ddots&\ddots\\
        &&-\lambda&1
      \end{pmatrix},
  \end{gather}
  where both matrices are of dimension $n-1$. Since they are triangular, recursion of Laplace expansion is particularly simple and yields the product of the diagonal elements. Thus
  \begin{gather}
      \chi(\lambda) = (-1)^n \lambda^n
      + (-1)^{n+1} \epsilon.
  \end{gather}
  Its roots are determined by the condition
  \begin{gather}
      \lambda^n = \epsilon.
  \end{gather}
  Thus, $\lambda$ can be computed as an $n$th root of unity times the (real) $n$th root of $\epsilon$.
\end{proof}

\begin{Theorem}{Jordan-block-ill-conditioned}
  The eigenvalue problem for eigenvalues which are not semi-simple is
  in general ill-posed.
\end{Theorem}

\begin{proof}
  The analysis in \slideref{Example}{conditioning-Jordan-block} is
  generic in the sense that it applies to nonzero eigenvalues and also
  to matrices which are similar to such a block. Thus, we can conclude
  that for every matrix $\mata$ which is similar to a matrix with a
  nontrivial Jordan block for eigenvalue $\lambda$, there is a
  perturbation $\mate$ such that the derivative of the function
  $\lambda(\epsilon) = \lambda(A+\epsilon\mate)$ at zero is unbounded.
\end{proof}

\begin{Theorem*}{bauer-fike}{Bauer-Fike}
  Let $\mata\in \Cnn$ be diagonalizable with matrix of eigenvectors
  $\matv \in \Cnn$ and diagonal matrix
  $\matlambda = \diag(\lambda_1\dots,\lambda_n)$. Let $\mata+\mate$ be
  a perturbation of $\mata$. Then, for any eigenvalue $\mu$ of
  $\mata+\mate$, there is an eigenvalue $\lambda_i$ of $\mata$ such
  that
  \begin{gather}
    \abs{\mu-\lambda_i} \le \cond_2(\matv) \norm{\mate}_2.
  \end{gather}
\end{Theorem*}

\begin{proof}
  Wikipedia
\end{proof}

\begin{Corollary}{conditioning-eigenvalues-normal}
  The eigenvalue problem of a normal matrix $\mata\in\Cnn$ is
  well-conditioned in the sense that for every eigenvalue $\mu$ of the
  perturbed matrix $\mata+\mate$, there is an eigenvalue $\lambda$ of
  $\mata$ such that
  \begin{gather}
    \abs{\mu-\lambda} \le \norm{E}_2.
  \end{gather}
\end{Corollary}

The Bauer-Fike theorem provides a general estimate for diagonalizable
matrices in terms of the condition number of the matrix of
eigenvectors. The following theorem is less general, since it only
applies to simple eigenvalues, but it provides geometric intuition of
the issue.

\begin{Theorem}{conditioning-eigenvalue-single}
  Let $\mata_\epsilon = \mata+\epsilon\mate\in\Cnn$ be a perturbation
  of $\mata\in\Cnn$. Let $\lambda(0)$ and let $\lambda(0)$ be a single
  eigenvalue of $\mata$. Then, there exists a uniquely defined
  differentiable continuation $\lambda(\epsilon)$ for small $\epsilon$
  such that $\lambda(\epsilon) \in \sigma(\mata_\epsilon)$ and with
  its left and right eigenvectors $u$, and $w$, respectively, there
  holds
  \begin{gather}
    \abs*{\tfrac{d}{d\epsilon} \lambda(0)}
    \le \norm{E}_2\frac{\norm{u}_2\norm{w}_2}{\abs{\scal(u,w)}}
    = \norm{E}_2 \frac1{\cos\angle(u,w)}.
  \end{gather}
\end{Theorem}

\subsection{Conditioning of eigenvectors and eigenspaces}

\begin{intro}
  Positive results on the conditioning of eigenvectors require
  additional tools which go beyond the exposition planned for this
  class. We will thus only discuss this question at hand of an example
  and conclude a rule of thumb.
\end{intro}

\begin{Example}{conditioning-eigenvectors}
  Consider the two matrices
  \begin{gather}
    A =
    \begin{pmatrix}
      1-\epsilon & 0\\ 0 & 1+\epsilon
    \end{pmatrix},
    \qquad
    B =
    \begin{pmatrix}
      1&\epsilon\\\epsilon&1
    \end{pmatrix}.
  \end{gather}
  Their eigenvalues are $1-\epsilon$ and $1+\epsilon$, but their
  eigenvectors differ by an angle of $\pi/4$ independent of
  $\epsilon$.
\end{Example}

\begin{Remark}{conditioning-eigenvectors}
  The problem of finding eigenvectors for tight clusters of
  eigenvalues is ill-posed. Nevertheless, finding the invariant
  subspace associated to all eigenvalues in such a cluster is
  well-posed.

  Conditioning of the eigenvector problem depends on the separation of
  eigenvalues.
\end{Remark}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
