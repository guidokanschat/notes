\begin{Definition}{galerkin-method}
  Let $\mata\in\Rnn$ and $\vb, \vx\in\R^n$ with $\mata\vx=\vb$. Then,
  the vector $\tilde\vx\in\R^n$ is called the \define{Galerkin
    approximation} of $\vx$ in a subspace $K$ orthogonal to a subspace
  $L$, if there holds
  \begin{align}
    \tilde\vx &\in K,\\
    \vb-\mata\tilde\vx &\perp L.
                         \label{eq:proj-it:1}
  \end{align}
  This type of approximation is called \define{Galerkin method}, more
  specifically \define{Ritz-Galerkin method} in the case $K=L$ and
  \define{Petrov-Galerkin method} in the case $K\neq L$.
\end{Definition}

\begin{remark}
  Projection methods are at first hand not iterative, but they produce
  an approximation $\tilde \vx$ in one step. Nevertheless, this
  approximation may not be sufficiently accurate. In this case, we
  have two options:
  \begin{enumerate}
  \item We can enrich the subspaces $K$ and $L$, typically by a
    one-dimensional subspace each, in order to obtain a better
    approximation $\tilde \vx$ in this larger space.
  \item We can rewrite the projection method in an incremental way,
    where the result of a previous step is improved by an update
    obtained from a projection step.
  \end{enumerate}
\end{remark}

\begin{Definition}{projection-step}
  Given a vector $\vx^{(k)}\in\R^n$ and its residual
  $\vr^{(k)}=\vb-\mata\vx^{(k)}$. Then, we say that the vector
  $\vx^{(k+m)}\in\R^n$ is obtained by a \define{projection step}, if
  \begin{gather}
    \vx^{(k+m)} = \vx^{(k)} + \vv,
  \end{gather}
  where after the choice of $m$-dimensional subspaces $K_k$ and $L_k$ the update $\vv$ is
  determined by the condition
  \begin{align}
    \vv &\in K_k\\
    \vr^{(k+m)} \equiv \vr^{(k)} - \mata\vv &\perp L_k.
  \end{align}
  A method of this type is called \define{successive subspace correction} method.
\end{Definition}

\begin{remark}
  The notation $\vx^{(k+m)}$ in the previous definition is quite
  arbitrary and we could have written $\vx^{(k+1)}$ as
  well. Nevertheless, algorithms we will study later have a structure
  where the $m$-dimensional projection step will resemble $m$
  one-dimensional steps, at least from the algorithmic point of
  view. Thus, we adopt the view of a single step in dimension $m$ as
  an $m$-fold step.

  Since most methods determine the update in each step by the same
  algorithm, we will typically only discuss the step from $\vx^{(0)}$
  to $\vx^{(m)}$, thus omitting the index $k$ of the initial vector of
  the step, or discuss a step from $\vx$ to $\tilde\vx$, hence
  omitting indices at all, while keeping in mind that there is an
  iterative procedure in the background.
\end{remark}

\begin{Notation}{incremental}
  We will mostly only consider a single step of the iterative
  projection method. Since the algorithm determining the update
  usually does not vary between steps, we will equivalently describe
  iteration steps as mappings
  \begin{align*}
    \vx^{(k)} &\mapsto \vx^{(k+m)}\\
    \vx^{(0)} &\mapsto \vx^{(m)}\\
    \vx &\mapsto \tilde\vx
  \end{align*}
\end{Notation}

\begin{Definition}{parallel-subspace-correction}
  Given an initial vector $\vx$, a step of the \define{parallel
    subspace correction} method is obtained by choosing $m$ subspaces
  $K_1\dots,K_m$ and $L_1,\dots,L_m$, and computing independently $m$
  update vectors by the projection conditions
  \begin{align}
    \vv_i &\in K_i\\
    \vr_i \equiv \vb-\mata\vx - \mata\vv_i &\perp L_i.
  \end{align}
  The result of the correction step is
  \begin{gather}
    \tilde \vx = \vx + \omega\sum\vv_i,
  \end{gather}
  where $\omega$ is a relaxation parameter chosen to achieve convergence.
\end{Definition}

\begin{Example}{projection-gauss-seidel}
  The Gauss-Seidel and the Jacobi substeps are projection steps with the choice
  \begin{gather}
    K=L=\spann{\ve_i}.
  \end{gather}
  Here, each step of the Jacobi iteration is a parallel subspace
  correction method, while the Gauss-Seidel iteration is a successive
  subspace correction method.
\end{Example}

\begin{proof}
  The condition for the update vector $\vv_i$ is
  \begin{gather}
    \scal(\vb-\mata\vx-\mata\vv_i,\ve_i) = 0,
  \end{gather}
  which, since $\vv_i$ is a multiple of $\ve_i$, translates to
  \begin{gather}
    \vb_i - (\mata\vx)_i - a_{ii}v_i = \vb_i-\sum_{j\neq i} a_{ij}x_j - a_{ii}x_i -a_{ii}v_i.
  \end{gather}
  Hence
  \begin{gather}
    v_i = -x_i + \frac1{a_{ii}} \left(\vb_i - \sum_{j\neq i} a_{ij}x_j\right).
  \end{gather}
  For the parallel subspace correction method, we have for each component independently
  \begin{gather}
    x_i = x_i+v_i = \frac1{a_{ii}} \left(\vb_i - \sum_{j\neq i} a_{ij}x_j\right),
  \end{gather}
  which is equal to the Jacobi step. For the successive subspace
  correction method, we obtain the Gauss-Seidel step by observing that
  the start vector $\vx$ changes in every step.
\end{proof}

\begin{Definition}{projection-method-matrix}
  Let $\matv=(\vv_1,\dots,\vv_m)$ and $\matw=(\vw_1,\dots,\vw_m)$ be bases for
  the subspaces $K$ and $L$, respectively. Then, the solution
  $\vx^{(m)}$ to the projection step is determined by $\vy\in\R^m$ and
  \begin{align}
    \vx^{(m)} &= \vx^{(0)} + \matv\vy\\
    \matw^*\mata\matv \vy &= \matw^* \vr^{(0)}.
  \end{align}
  It is thus obtained by solving an $m$-by-$m$ linear system, called
  the projected system or the \define{Galerkin
    equations}. $\matw^*\mata\matv\in\R^{m\times m}$ is the
  \define{projected matrix}.
\end{Definition}

\begin{proof}
  See \cite[Section 5.1.2]{Saad00}. We observe that the Galerkin condition translates to
  \begin{gather}
    \scal(\vw_i,\vb-\mata\vx^{(0)}-\mata\vv)\qquad i=1,\dots,m,
  \end{gather}
  which we can rewrite as a matrix equation
  \begin{gather}
    \matw^*(\vb-\mata\vx^{(0)}-\mata\vv) = \matw^*\vr^{(0)} -
    \matw^*\mata\matv.
  \end{gather}
  Since we can express $\vv$ in terms of the basis $\matv$, we obtain
  \begin{gather}
    \matw^*\mata\vv = \matw^*\mata\matv\vy,
  \end{gather}
  for some vector $\vy\in\R^m$.
\end{proof}

\begin{Theorem}{projected-invertible}
  Let one of the following conditions hold:
  \begin{enumerate}
  \item $\mata$ is symmetric, positive definite and $L=K$.
  \item $\mata$ is invertible and $L = \mata K$.
  \end{enumerate}
  Then, the projected matrix $\matw^*\mata\matv$ is invertible for any
  bases $\matv$ and $\matw$ of $K$ and $L$, respectively.
\end{Theorem}

\begin{proof}
  First, consider the symmetric, positive definite case. Since $K=L$,
  we can use the same basis $\matw=\matv$  and obtain
  \begin{gather}
    \mata_m = \matv^*\mata\matv,
  \end{gather}
  which inherits the symmetry from $\mata$. Hence, we can estimate its
  lowest eigenvalue by the Rayleigh quotient,
  \begin{multline}
    \lambda_{\text{min}}(\mata_m)
    = \min_{\vy\in\R^m} \frac{\scal(\mata_m\vy,\vy)}{\scal(\vy,\vy)}
    = \min_{\vy\in\R^m} \frac{\scal(\matv^*\mata\matv\vy,\vy)}{\scal(\vy,\vy)}
    \\
    = \min_{\vy\in\R^m} \frac{\scal(\mata\matv\vy,\matv\vy)}{\scal(\vy,\vy)}
    \ge\min_{\vv\in\R^n} \frac{\scal(\mata\vv,\vv)}{\scal(\vv,\vv)}
    \min_{\vy\in\R^m} \frac{\scal(\matv\vy,\matv\vy)}{\scal(\vy,\vy)}.
  \end{multline}
  Both factors are positive, the first by the positive definiteness of
  $\mata$, the second because $\matv$ is a basis. The results holds
  for a different basis $\matw$ on the left, since this corresponds to
  multiplying $\mata_m$ by an invertible matrix from the left.

  If $\mata$ is invertible and $L=\mata K$ we choose
  $\matw=\mata\matv$. Observing that $\mata^*\mata$ is symmetric and
  positive definite, we can apply the previous argument.
\end{proof}

\begin{Theorem}{projection-orthogonal-optimal}
  Let $\mata\in\Rnn$ be symmetric, positive definite. Then,
  $\tilde \vx$ is the result of the orthogonal ($L=K$) projection
  method with previous state $\vx^{(k)}$ if and only if it minimizes
  the \putindex{A-norm} of the error over the space $\vx^{(k)}+K$. Namely, for the
  solution $\vx$ of $\mata\vx=\vb$ there holds
  \begin{gather}
    \norm{\tilde\vx-\vx}_A
    = \min_{\vy\in \vx^{(k)}+K} \norm{\vy-\vx}_A
    = \min_{\vy\in K} \norm{\vy+\vx^{(k)}-\vx}_A.
  \end{gather}
  Here, $\norm\vv_A = \sqrt{\vv^*\mata\vv}$ is the so-called \define{energy norm} or \define{A-norm}.
\end{Theorem}

\begin{proof}
  We use the definition of the projection step to derive for
  $\tilde \vx = \vx^{(k+m)}$ and any $\vw \in L = K$:
  \begin{align}
    0
    &= \scal(\vr^{(k)} - \mata \vv,\vw)\\
    &= \scal(\vb-\mata\tilde\vx,\vw)\\
    &= \scal(\mata\vx - \mata\tilde\vx,\vw)\\
    &= \scal(\vx-\tilde\vx,\vw)_{\mata}.
  \end{align}
  That is, the error after the projection step is orthogonal to the
  test space $K$ in the $\mata$-inner product. Hence, we can continue
  with a standard proof for the optimality of orthogonal
  projection. We present here the \putindex{variational method} which
  is also used in functional analysis. To this end, we define for
  arbitrary nonzero $\vw\in K$ the function
  \begin{gather}
    F(t) = \tfrac12 \norm{\tilde\vx+t\vw-\vx}_{\mata}^2.
  \end{gather}
  We observe that since $\tilde\vx \in \vx^{(k)}+K$, also
  $\tilde\vx + t \vw \in \vx^{(k)}+K$. Hence, we can replace the
  minimum as one over $\vw$ and $t$.
  
  We observe that if $\tilde\vx$ is a minimizer of the norm, then this
  function has a minimum at $t=0$. Furthermore, if it has a minimimum
  at $t=0$ for all $\vw\in K$, then this implies that $\tilde\vx$ is a
  minimum over $K$. Hence, we have reduced the question to a set of
  one-dimensional minimization problems. We have
  \begin{align}
    F(t) &= \tfrac{t^2}2 \norm{\vw}_{\mata}^2 + t \scal(\tilde\vx-\vx,\vw)_{\mata}
    + \tfrac12 \norm{\tilde\vx-\vx}_{\mata}^2,\\
    F'(t) &= t\norm{\vw}_{\mata}^2 + \scal(\tilde\vx-\vx,\vw)_{\mata},\\
    F''(t) &= \norm{\vw}_{\mata}^2 = \vw^*\mata\vw.
  \end{align}
  These derivatives are also called \define{first variation} and
  \define{second variation} of the minimization problem.
  Necessary and sufficient for a minimum at $t=0$ is $F'(0)=0$ and
  $F''(0)>0$. The latter is due to the positive definiteness of
  $\mata$. The first is fulfilled if
  \begin{gather}
    F'(0) = \scal(\tilde\vx-\vx,\vw)_{\mata} = 0,
  \end{gather}
  which is exactly what we derived for the projection step.
\end{proof}


\begin{Theorem}{projection-oblique-optimal}
  Let $\mata\in\Rnn$ and $L=\mata K$. Then, $\tilde \vx$ is the result
  of the (oblique) projection method with previous state $\vx^{(k)}$
  if and only if it minimizes the Euclidean norm of the residual
  $\vb-\mata\tilde\vx$ over the space $\vx^{(k)}+K$. Namely, there
  holds
  \begin{gather}
    \norm{\vb-\mata\tilde\vx}_2
    = \min_{\vy\in\vx^{(k)}+K} \norm{\vb-\mata\vy}_2
  \end{gather}
\end{Theorem}

\begin{proof}
  We begin as in the previous proof and use the definition of the
  projection step to derive for $\tilde \vx = \vx^{(k+m)}$ and any
  $\mata\vw \in L = \mata K$:
  \begin{align}
    0
    &= \scal(\vr^{(k)} - \mata \vv,\mata\vw)\\
    &= \scal(\vb-\mata\tilde\vx,\mata\vw)\\
    &= \scal(\mata\vx - \mata\tilde\vx,\mata\vw)\\
    &= \scal(\vx-\tilde\vx,\vw)_{\mata^*\mata}.
  \end{align}
  Since $\mata^*\mata$ is positive definite if $\mata$ is nonsingular,
  we can proceed as in the previous theorem, observing that
  \begin{gather}
    \norm{\vb-\mata\tilde\vx}^2 = \scal(\vb-\mata\tilde\vx,\vb-\mata\tilde\vx)
    = \scal(\vx-\tilde\vx,\vx-\tilde\vx)_{\mata^*\mata}.
  \end{gather}
\end{proof}

\begin{Theorem}{projection-1d}
  Any one-dimensional projection method can be characterized by two
  vectors, the \define{search direction} $\vv$ and the test vector
  $\vw$. Then,
  \begin{gather}
    K = \spann{\vv^{(k)}},
    \qquad L = \spann{\vw^{(k)}}.
  \end{gather}
  The new solution is
  \begin{gather}
    \vx^{(k+1)} = \vx^{(k)}+\alpha_k \vv^{(k)},
  \end{gather}
  where
  \begin{gather}
    \label{eq:projection:1d-1}
    \alpha_k = \frac{\scal(\vr^{(k)},\vw^{(k)})}{\scal(\mata\vv^{(k)},\vw^{(k)})}.
  \end{gather}
\end{Theorem}

\begin{proof}
  After choosing the spaces $K$ and $L$, the method is uniquely defined. Since (omitting indices)
  \begin{gather}
    \scal(\vb-\mata\vx - \alpha\mata\vv,\vw) = 0,
  \end{gather}
  we have
  \begin{gather}
    \alpha\scal(\mata\vv,\vw) = \scal(\vr,\vw),
  \end{gather}
  which implies the formula for $\alpha$ if the denominator is nonzero.
\end{proof}

\begin{Definition}{breakdown}
  The computation of the step length in all projection methods
  requires a division as in~\eqref{eq:projection:1d-1}. If the
  denominator is zero, the next step of the method cannot be
  computed. We call this a \define{breakdown} of the method.
  
  Furthermore, this value might be almost zero, which is called a
  \define{near breakdown}. In this case, the next step might be
  computed, but it will suffer from round-off errors.
\end{Definition}

\begin{Algorithm*}{steepest-descent-algol}{The steepest descent method}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$ s.p.d.; $\quad\vx,\vb\in\R^n$
    \State $\vr\gets \vb-\mata\vx$
    \Repeat
    \State $\alpha\gets\frac{\scal(\vr,\vr)}{\scal(\mata\vr,\vr)}$
    \State $\vx \gets \vx + \alpha\vr$
    \State $\vr \gets \vr-\alpha\mata\vr$
    \Until convergence
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{steepest-descent}
  Each step of the steepest descent method computes the minimum of
  $F(\vy) = \frac12\norm{\vy-\vx}_A^2$ along the line from the current
  iterate $\vx^{(k)}$ in direction $\vw = -\nabla F(\vx^{(k)}) = -\vr^{(k)}$.
  Furthermore, there holds
  \begin{gather}
    \vr^{(k+1)} \perp \vr^{(k)}.
  \end{gather}
\end{Lemma}

\begin{proof}
  This result is an immediate consequence of
  \slideref{Theorem}{projection-orthogonal-optimal}. From the proof of
  this theorem, we deduce for any $\vw$
  \begin{gather}
    \nabla F(\vx^{(k)})(\vw) = \scal(\vx^{(k)}-\vx,\vw)_{\mata}
    = -\scal(\vr^{(k)},\vw).
  \end{gather}
  Furthermore, the orthogonality condition of the orthogonal
  projection method implies
  \begin{gather}
    0 = \scal(\vr^{(k+1)},\vw^{(k)}) = -\scal(\vr^{(k+1)},\vr^{(k)}).
  \end{gather}
\end{proof}

\begin{Remark}{line-search}
  The one-dimensional projection method is an application of the class
  of \define{line search} algorithms to the solution of a linear
  system formulated as a minimization problem.

  Such algorithms in general compute the next approximation to a
  minimization problem by finding a minimizer in a one-dimensional
  subspace determined by the xurrent iterate and a search direction.
\end{Remark}

\begin{Example}{steepest-descent-10}
  We run the steepest descent method for
  \begin{gather}
    \mata =
    \begin{pmatrix}
      .1 \\&1
    \end{pmatrix},
    \vb =
    \begin{pmatrix}
      0\\0
    \end{pmatrix},
    \vx^{(0)} =
    \begin{pmatrix}
      -10\\1
    \end{pmatrix}.
  \end{gather}
  The iteration history together with  the level sets of $\vx^T\mata\vx$ is
  \begin{center}
    \includegraphics[width=.8\textwidth]{./graph/steepest-descent-10.tikz}
  \end{center}
\end{Example}

\begin{remark}
  In this algorithm, the operation $\mata\vr$ is applied twice. Since
  in most implementations this will be the part with the most
  computational operations, we introduce an auxiliary vector
  $\vp = \mata\vr$, which can be used in lines 3 and 5. At the cost of
  one additional vector, the numerical effort per step is almost
  cut in half.
\end{remark}

\begin{Remark}{convergence-residual}
  The residual $\tilde \vr = \vb - \mata\tilde\vx$ of the current iterate
  $\tilde\vx$ measures the misfit of $\tilde\vx$ in the equation
  $\mata\vx = \vb$. Since the error $\tilde\vx-\vx$ is unknown, we can
  use it as criterion for the accuracy of the current
  solution. Indeed, there holds
  \begin{gather}
    \norm{\vx-\tilde\vx}
    = \norm*{\mata^{-1}\bigl(\mata\vx - \mata\tilde\vx\bigr)}
    = \norm*{\mata^{-1}\bigl(\vb - \mata\tilde\vx\bigr)}
    \le \norm{\mata^{-1}}\norm{\tilde \vr}.
  \end{gather}
  Therefore, the criterion for convergence of the algorithm is typically
  \begin{gather}
    \norm{\tilde \vr} \le \text{TOL},
  \end{gather}
  where TOL is a tolerance chosen by the user.

  Note, that the computation of $\tilde \vr$ is subject to roundoff
  errors. Thus, the tolerance should always be chosen
  \begin{gather}
    \text{TOL} > c\norm{\vb}\eps,
  \end{gather}
  where $\eps$ is the machine accuracy and $c$ should account for
  error accumulation in the matrix-vector product.
\end{Remark}

\begin{Algorithm*}{steepest-descent-python1}{Steepest descent in Python}
  \lstinputlisting{python/steepest-descent.py}
\end{Algorithm*}

\begin{Algorithm*}{steepest-descent-cpp1}{Steepest descent in C++ (armadillo)}
  \lstinputlisting{code/cpp/steepest_descent.cc}
\end{Algorithm*}

\begin{remark}
  Depending on the quality of the compiler/interpreter, the code line
  \begin{lstlisting}[language=Python,numbers=none]
    x += alpha*r
  \end{lstlisting}
  may involve creating an auxiliary vector $\vp = \alpha \vr$ and
  adding this vector to $\vx$.  Obviously, this could be avoided by
  directly implementing
  \begin{lstlisting}[language=Python,numbers=none]
    for i in range(0,n):
      x[i] += alpha*r[i]
  \end{lstlisting}
  Since operations like this are ubuquitous in scientific computing,
  they were standardized early on in the BLAS (basic linear algebra
  subroutines) library. It contains the FORTRAN function
  \begin{lstlisting}[language=Fortran,numbers=none]
    SUBROUTINE DAXPY( n, alpha, x, incx, y, incy)
  \end{lstlisting}
  which computes $\vy\gets\vy+\alpha\vx$ for double precision vectors
  of length $n$ (the increment arguments allow to skip elements). For
  usage in Python, there is a wrapper
  \begin{lstlisting}[language=Python,numbers=none]
    scipy.linalg.blas.daxpy(x, y[, n, a, offx, incx, offy, incy])
  \end{lstlisting}
\end{remark}

\begin{Algorithm*}{steepest-descent-python2}{Steepest descent with daxpy}
  \lstinputlisting{python/steepest-descent-axpy.py}
\end{Algorithm*}

\begin{Algorithm*}{minimal-residual-algol}{The minimal residual method}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vx,\vb\in\R^n$
    \State $\vr\gets \vb-\mata\vx$
    \Repeat
    \State $\alpha\gets\frac{\scal(\mata\vr,\vr)}{\scal(\mata\vr,\mata\vr)}$
    \State $\vx \gets \vx + \alpha\vr$
    \State $\vr \gets \vr-\alpha\mata\vr$
    \Until convergence
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{minimal-residual}
  Each step of the minimal method computes the minimum of
  $F(\vy) = \frac12\norm{\vb-\mata\vy}_2^2$ along the line from the current
  iterate $\vx^{(k)}$ in direction $-\vr^{(k)}$. Furthermore, there holds
  \begin{gather}
    \vr^{(k+1)} \perp \mata \vr^{(k)}
  \end{gather}
\end{Lemma}

\begin{proof}
  The proof is identical ot the one of
  \slideref{Lemma}{steepest-descent} after replacing
  $\vw^{(k)} = \mata\vr^{(k)}$ and using
  \slideref{Theorem}{projection-oblique-optimal}.
\end{proof}

\begin{Lemma}{lucky-breakdown}
  The iteration sequences $\{\vx^{(k)}\}$ of the steepest descent and
minimal residual methods, respectively, are well defined except for
the case where $\vx^{(k)}$ is the exact solution $\vx$ and thus
$\vr^{(k)} = 0$.

We refer to this phenomenon as \define{lucky breakdown}, since the
method only fails after the exact solution has been found.
\end{Lemma}

\begin{Lemma*}{kantorovich-inequality}{Kantorovich inequality}
  Let $\mata\in\Rnn$ be symmetric, positive definite with minimal and
  maximal eigenvalues $\lambda_{\min}$ and $\lambda_{\max}$. Then, for
  $\vx\in\R^n$ there holds
  \begin{gather}
    \frac{\scal(\mata\vx,\vx)\scal(\mata^{-1}\vx,\vx)}{\scal(\vx,\vx)^2}
    \le \frac{(\lambda_{\min}+\lambda_{\max})^2}{4\lambda_{\min}\lambda_{\max}}
  \end{gather}
\end{Lemma*}

\begin{proof}
  See~\cite[Lemma 5.8]{Saad00}.
\end{proof}

\begin{Theorem}{steepest-descent-convergence}
  Let $\mata\in\Rnn$ be symmetric, positive definite with extremal
  eigenvalues $\lambda_{\min}$ and $\lambda_{\max}$. Then, the error
  $\ve^{(k)} = \vx-\vx^{(k)}$ of the steepest descent method admits
  the estimate
  \begin{gather}
    \norm{\ve^{(k+1)}}_A \le \rho \norm{\ve^{(k)}}_A,
  \end{gather}
  where the contraction number is
  \begin{gather}
    \rho
    = \frac{\lambda_{\max}-\lambda_{\min}}{\lambda_{\max}+\lambda_{\min}}
    = \frac{\cond_2\mata - 1}{\cond_2\mata+1}
    = 1-\frac2{\cond_2\mata+1}
  \end{gather}
\end{Theorem}

\begin{proof}
  See~\cite[Theorem 5.9]{Saad00}.
\end{proof}

\begin{Theorem}{minimal-residual-convergence}
  Let $\mata\in\Rnn$ such that its symmetric part $(\mata+\mata^T)/2$
  is positive definite. Then, the residuals $\vr^{(k)}$ of the minimal
  residual method admit the estimate
  \begin{gather}
    \norm{\vr^{(k+1)}}_2 \le \rho \norm{\vr^{(k)}}_2,
  \end{gather}
  where the contraction number is
  \begin{gather}
    \rho = \left(1-\frac{\mu^2}{\norm{\mata}^2}\right)^{\nicefrac12}
  \end{gather}
  and $\mu$ is the smallest eigenvalue of $(\mata+\mata^T)/2$.
\end{Theorem}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
