\begin{Definition}{gradient-method}
  The \define{gradient method} for finding minimizers of a nonlinear
  functional $F(x)$ reads:  given an initial value $x^{(0)}$, compute
  iterates $x^{(k)}$, $k=1,2,\ldots$ by the rule
  \begin{gather}
    \label{eq:gradient-method:1}
    \begin{split}
      d^{(k)} &= -\nabla F(x^{(k)}),
      \\
      \alpha_k &=
      \operatorname*{argmin}_{\gamma>0} F\left(x^{(k)} + \gamma d^{(k)}\right)
      \\
      x^{(k+1)} &= x^{(k)} + \alpha_k d^{(k)}.
    \end{split}
  \end{gather}
  The minimization process used to compute $\alpha_k$, also called
  \define{line search}, is one-dimensional
  and therefore simple. It may be replaced by a heuristic choice of
  $\alpha_k$.
\end{Definition}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../notes"
%%% End:
